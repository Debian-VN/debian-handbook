#
# AUTHOR <EMAIL@ADDRESS>, YEAR.
# Ryuunosuke Ayanokouzi <i38w7i3@yahoo.co.jp>, 2015-2017.
# Kenshi Muto <kmuto@kmuto.jp>, 2015.
# Yoichi Chonan <cyoichi@maple.ocn.ne.jp>, 2015.
# YABUKI Youichi <yabuki@sraoss.co.jp>, 2015.
# Osamu Aoki <osamu@debian.org>, 2016.
#
msgid ""
msgstr ""
"Project-Id-Version: 0\n"
"POT-Creation-Date: 2017-02-05 09:00+0900\n"
"PO-Revision-Date: 2017-02-05 09:00+0900\n"
"Last-Translator: AYANOKOUZI, Ryuunosuke <i38w7i3@yahoo.co.jp>\n"
"Language-Team: Japanese <https://github.com/l/debian-handbook/tree/master/translation/ja_JP/push>\n"
"Language: ja\n"
"MIME-Version: 1.0\n"
"Content-Type: application/x-publican; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "RAID"
msgstr "RAID"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "LVM"
msgstr "LVM"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "FAI"
msgstr "FAI"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Preseeding"
msgstr "Preseed"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Monitoring"
msgstr "監視"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Virtualization"
msgstr "仮想化"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Xen"
msgstr "Xen"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "LXC"
msgstr "LXC"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Advanced Administration"
msgstr "高度な管理"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "This chapter revisits some aspects we already described, with a different perspective: instead of installing one single computer, we will study mass-deployment systems; instead of creating RAID or LVM volumes at install time, we'll learn to do it by hand so we can later revise our initial choices. Finally, we will discuss monitoring tools and virtualization techniques. As a consequence, this chapter is more particularly targeting professional administrators, and focuses somewhat less on individuals responsible for their home network."
msgstr "この章では、前章までに説明した一部の側面を異なる視点からもう一度取り上げます。すなわち、1 台のコンピュータにインストールするのではなく、大規模な配備システムについて学びます。さらに、初回インストール時に RAID や LVM ボリュームを作成するのではなく、手作業でこれを行う方法について学びます。こうすることで初回インストール時の選択を訂正することが可能です。最後に、監視ツールと仮想化技術について議論します。その結果として、この章はより熟練した管理者を対象にしており、ホームネットワークに責任を負う個人を対象にしていません。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "RAID and LVM"
msgstr "RAID と LVM"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<xref linkend=\"installation\" /> presented these technologies from the point of view of the installer, and how it integrated them to make their deployment easy from the start. After the initial installation, an administrator must be able to handle evolving storage space needs without having to resort to an expensive reinstallation. They must therefore understand the required tools for manipulating RAID and LVM volumes."
msgstr "<xref linkend=\"installation\" />ではインストーラの視点から RAID と LVM の技術を説明し、インストーラを使って初回インストール時に RAID や LVM を簡単に配備する方法について説明しました。初回インストールが終わったら、管理者は再インストールという手間のかかる最終手段を行使することなく、より大きなストレージ領域の要求に対処しなければいけません。すなわち、管理者は RAID と LVM ボリュームを操作するために必要なツールを理解しなければいけません。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "RAID and LVM are both techniques to abstract the mounted volumes from their physical counterparts (actual hard-disk drives or partitions thereof); the former secures the data against hardware failure by introducing redundancy, the latter makes volume management more flexible and independent of the actual size of the underlying disks. In both cases, the system ends up with new block devices, which can be used to create filesystems or swap space, without necessarily having them mapped to one physical disk. RAID and LVM come from quite different backgrounds, but their functionality can overlap somewhat, which is why they are often mentioned together."
msgstr "RAID と LVM は両方ともマウントされたボリュームを物理的に対応する物 (実際のハードディスクドライブまたはそのパーティション) から抽象化する技術です。さらに、RAID は冗長性を導入することでデータをハードディスク障害から守り、LVM はボリューム管理をより柔軟にしてディスクの実サイズに依存しないようにします。どちらの場合であっても、最終的にシステムには新しいブロックデバイスが追加されます。追加されたブロックデバイスはファイルシステムやスワップ領域を作成するために使われますが、必ずしも単独の物理ディスクに対応付けられるものではありません。RAID と LVM は全く異なる生い立ちを持っていますが、両者の機能は多少重複しています。このため、両者は一緒に言及されることが多いです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>PERSPECTIVE</emphasis> Btrfs combines LVM and RAID"
msgstr "<emphasis>PERSPECTIVE</emphasis> Btrfs が LVM と RAID を結び付ける"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "While LVM and RAID are two distinct kernel subsystems that come between the disk block devices and their filesystems, <emphasis>btrfs</emphasis> is a new filesystem, initially developed at Oracle, that purports to combine the featuresets of LVM and RAID and much more. It is mostly functional, and although it is still tagged “experimental” because its development is incomplete (some features aren't implemented yet), it has already seen some use in production environments. <ulink type=\"block\" url=\"http://btrfs.wiki.kernel.org/\" />"
msgstr "LVM と RAID は 2 種類の全く別のカーネルサブシステムで、どちらもディスクブロックデバイスとそのファイルシステムの間のやり取りを担当します。<emphasis>btrfs</emphasis> は最初 Oracle で開発された新しいファイルシステムで、LVM と RAID の機能を結び付けると主張しています。<emphasis>btrfs</emphasis> は開発がまだ完了していない (一部の機能がまだ実装されていない) ため「実験中」とタグ付けされていますが、ほとんどうまく機能し、既に本番環境で使われています。<ulink type=\"block\" url=\"http://btrfs.wiki.kernel.org/\" />"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Among the noteworthy features are the ability to take a snapshot of a filesystem tree at any point in time. This snapshot copy doesn't initially use any disk space, the data only being duplicated when one of the copies is modified. The filesystem also handles transparent compression of files, and checksums ensure the integrity of all stored data."
msgstr "<emphasis>btrfs</emphasis> の特筆すべき機能に、任意の時点におけるファイルシステムツリーのスナップショットを取る機能があります。このスナップショットコピーは初期状態ではいかなるディスク領域も使いません、コピー内容の 1 つが修正された際にデータが複製されます。また、このファイルシステムはファイルを透過的に圧縮することが可能で、さらにチェックサムを用いて保存されているデータの完全性を保証します。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: PTAL;
msgid "In both the RAID and LVM cases, the kernel provides a block device file, similar to the ones corresponding to a hard disk drive or a partition. When an application, or another part of the kernel, requires access to a block of such a device, the appropriate subsystem routes the block to the relevant physical layer. Depending on the configuration, this block can be stored on one or several physical disks, and its physical location may not be directly correlated to the location of the block in the logical device."
msgstr "RAID と LVM のどちらの場合も、カーネルはハードディスクドライブやパーティションに対応するブロックデバイスファイルとよく似たブロックデバイスファイルを提供します。アプリケーションやカーネルの別の部分がそのようなデバイスのあるブロックにアクセスを要求する場合、適切なサブシステムが要求されたブロックを物理層のブロックに対応付けます。設定に依存して、アプリケーション側から見たブロックは単独か複数の物理ディスクに保存されます。このブロックの物理的場所は論理デバイス内のブロックの位置と直接的に対応するものではないかもしれません。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Software RAID"
msgstr "ソフトウェア RAID"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary>RAID</primary>"
msgstr "<primary>RAID</primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "RAID means <emphasis>Redundant Array of Independent Disks</emphasis>. The goal of this system is to prevent data loss in case of hard disk failure. The general principle is quite simple: data are stored on several physical disks instead of only one, with a configurable level of redundancy. Depending on this amount of redundancy, and even in the event of an unexpected disk failure, data can be losslessly reconstructed from the remaining disks."
msgstr "RAID は <emphasis>Redundant Array of Independent Disks</emphasis> を意味します。RAID システムの目標はハードディスク障害の際にデータ損失を防ぐことです。一般原則は極めて単純です。すなわち、データは設定できる冗長性のレベルに基づいて単独ではなく複数のディスクに保存されます。冗長性の度合いに依存して、たとえ予想外のディスク障害が起きた場合でも、データを残りのディスクから損失なく再構成することが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>CULTURE</emphasis> <foreignphrase>Independent</foreignphrase> or <foreignphrase>inexpensive</foreignphrase>?"
msgstr "<emphasis>CULTURE</emphasis> <foreignphrase>independent</foreignphrase> それとも <foreignphrase>inexpensive</foreignphrase>?"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The I in RAID initially stood for <emphasis>inexpensive</emphasis>, because RAID allowed a drastic increase in data safety without requiring investing in expensive high-end disks. Probably due to image concerns, however, it is now more customarily considered to stand for <emphasis>independent</emphasis>, which doesn't have the unsavory flavour of cheapness."
msgstr "当初、RAID の I は <emphasis>inexpensive</emphasis> を意味していました。なぜなら、RAID は高価な高性能ディスクへ投資することなくデータの安全性を劇的に高めることが可能だったからです。しかしながらおそらく心証的な懸念から、現在 RAID の I は通例 <emphasis>independent</emphasis> を意味するものとされます。<emphasis>independent</emphasis> には安価であることに対する悪い印象がないからです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "RAID can be implemented either by dedicated hardware (RAID modules integrated into SCSI or SATA controller cards) or by software abstraction (the kernel). Whether hardware or software, a RAID system with enough redundancy can transparently stay operational when a disk fails; the upper layers of the stack (applications) can even keep accessing the data in spite of the failure. Of course, this “degraded mode” can have an impact on performance, and redundancy is reduced, so a further disk failure can lead to data loss. In practice, therefore, one will strive to only stay in this degraded mode for as long as it takes to replace the failed disk. Once the new disk is in place, the RAID system can reconstruct the required data so as to return to a safe mode. The applications won't notice anything, apart from potentially reduced access speed, while the array is in degraded mode or during the reconstruction phase."
msgstr "RAID は専用ハードウェア (SCSI や SATA コントローラカードに統合された RAID モジュール) またはソフトウェア抽象化 (カーネル) を使って実装することが可能です。ハードウェアかソフトウェアかに関わらず、十分な冗長性を備えた RAID システムはディスク障害があっても利用できる状態を透過的に継続することが可能です。従って、スタックの上層 (アプリケーション) はディスク障害にも関わらず、引き続きデータにアクセスできます。もちろん「信頼性低下状態」は性能に影響をおよぼし、冗長性を低下させます。このため、もう一つ別のディスク障害が起きるとデータを失うことになります。このため実践的には、管理者は信頼性低下状態を障害の起きたディスクが交換されるまでの間だけに留めるように努力します。新しいディスクが配備されると、RAID システムは要求されたデータを再構成することが可能です。こうすることで信頼性の高い状態に戻ります。信頼性低下状態か再構成状態にある RAID アレイのアクセス速度は低下する可能性がありますが、この点を除けばアプリケーションがディスク障害に気が付くことはないでしょう。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "When RAID is implemented by hardware, its configuration generally happens within the BIOS setup tool, and the kernel will consider a RAID array as a single disk, which will work as a standard physical disk, although the device name may be different (depending on the driver)."
msgstr "RAID がハードウェアで実装された場合、その設定は通常 BIOS セットアップツールによってなされます。カーネルは RAID アレイを標準的な物理ディスクとして機能する単一のディスクとみなします。RAID アレイのデバイス名は (ドライバに依存して) 違うかもしれません。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "We only focus on software RAID in this book."
msgstr "本書ではソフトウェア RAID だけに注目します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Different RAID Levels"
msgstr "さまざまな RAID レベル"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "RAID is actually not a single system, but a range of systems identified by their levels; the levels differ by their layout and the amount of redundancy they provide. The more redundant, the more failure-proof, since the system will be able to keep working with more failed disks. The counterpart is that the usable space shrinks for a given set of disks; seen the other way, more disks will be needed to store a given amount of data."
msgstr "実際のところ RAID の種類は 1 種類だけではなく、そのレベルによって識別される複数の種類があります。すなわち、設計と提供される冗長性の度合いが異なる複数の RAID レベルが存在します。より冗長性を高くすれば、より障害に強くなります。なぜなら、より多くのディスクで障害が起きても、システムを動かし続けることができるからです。これに応じて、与えられた一連のディスクに対して利用できる領域が小さくなります。すなわち、あるサイズのデータを保存するために必要なディスク領域のサイズが多くなります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Linear RAID"
msgstr "リニア RAID"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Even though the kernel's RAID subsystem allows creating “linear RAID”, this is not proper RAID, since this setup doesn't involve any redundancy. The kernel merely aggregates several disks end-to-end and provides the resulting aggregated volume as one virtual disk (one block device). That's about its only function. This setup is rarely used by itself (see later for the exceptions), especially since the lack of redundancy means that one disk failing makes the whole aggregate, and therefore all the data, unavailable."
msgstr "カーネルの RAID サブシステムを使えば「リニア RAID」を作ることも可能ですが、「リニア RAID」は適切な RAID ではありません。なぜなら、「リニア RAID」には冗長性が一切ないからです。カーネルはただ単純に複数のディスク端同士を統合し、統合されたボリュームを 1 つの仮想ディスク (1 つのブロックデバイス) として提供するだけです。これが「リニア RAID」のすべてです。「リニア RAID」を使うのは極めてまれな場合に限られます (後から使用例を説明します)。なぜなら、冗長性がないということは 1 つのディスクの障害が統合されたボリューム全体を駄目にすること、ひいてはすべてのデータを駄目にすることを意味するからです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "RAID-0"
msgstr "RAID-0"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "This level doesn't provide any redundancy either, but disks aren't simply stuck on end one after another: they are divided in <emphasis>stripes</emphasis>, and the blocks on the virtual device are stored on stripes on alternating physical disks. In a two-disk RAID-0 setup, for instance, even-numbered blocks of the virtual device will be stored on the first physical disk, while odd-numbered blocks will end up on the second physical disk."
msgstr "同様に RAID-0 にも冗長性がありません。しかしながら、RAID-0 は順番通り単純に物理ディスクを連結する構成ではありません。すなわち、物理ディスクは<emphasis>ストライプ状</emphasis>に分割され、仮想デバイスのブロックは互い違いになった物理ディスクのストライプに保存されます。たとえば 2 台のディスクから構成されている RAID-0 セットアップでは、偶数を付番されたブロックは最初の物理ディスクに保存され、奇数を付番されたブロックは 2 番目の物理ディスクに保存されます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "This system doesn't aim at increasing reliability, since (as in the linear case) the availability of all the data is jeopardized as soon as one disk fails, but at increasing performance: during sequential access to large amounts of contiguous data, the kernel will be able to read from both disks (or write to them) in parallel, which increases the data transfer rate. However, RAID-0 use is shrinking, its niche being filled by LVM (see later)."
msgstr "RAID-0 システムを使っても信頼性は向上しません。なぜなら、システムの信頼性すなわちデータの可用性は (リニア RAID と同様に) ディスク障害があればすぐに脅かされるからです。しかしながら、RAID-0 システムを使うことで性能は向上します。すなわち隣接した巨大なデータにシーケンシャルアクセスする場合、カーネルは両方のディスクから並行して読み込む (書き込む) ことが可能です。これによりデータの転送率が増加します。とは言うものの、RAID-0 が使われる機会は減り、代わりに LVM (後から説明します) が使われるようになっています。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "RAID-1"
msgstr "RAID-1"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "This level, also known as “RAID mirroring”, is both the simplest and the most widely used setup. In its standard form, it uses two physical disks of the same size, and provides a logical volume of the same size again. Data are stored identically on both disks, hence the “mirror” nickname. When one disk fails, the data is still available on the other. For really critical data, RAID-1 can of course be set up on more than two disks, with a direct impact on the ratio of hardware cost versus available payload space."
msgstr "RAID-1 は「RAID ミラーリング」としても知られ、最も簡単で最も広く使われています。RAID-1 の標準的な構成では、同じサイズの 2 台の物理ディスクを使い、物理ディスクと同じサイズの論理ボリュームが利用できるようになります。データを両方のディスクに保存するため、「ミラー」と呼ばれています。一方のディスクに障害があっても、他方のディスクからデータを利用することが可能です。もちろん、非常に重要なデータ用に RAID-1 を 2 台以上の構成にすることも可能ですが、これはハードウェア費用と利用できる保存領域の比率に直接的な影響をおよぼします。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>NOTE</emphasis> Disks and cluster sizes"
msgstr "<emphasis>NOTE</emphasis> ディスクとクラスタサイズ"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "If two disks of different sizes are set up in a mirror, the bigger one will not be fully used, since it will contain the same data as the smallest one and nothing more. The useful available space provided by a RAID-1 volume therefore matches the size of the smallest disk in the array. This still holds for RAID volumes with a higher RAID level, even though redundancy is stored differently."
msgstr "異なるサイズの 2 台のディスクをミラーでセットアップする場合、サイズの大きい側のディスクは完全に利用されません。なぜなら、大きい側のディスクに含まれるデータは最も小さいディスクに含まれるデータと同じデータだからです。このため RAID-1 ボリュームで提供される利用できる領域のサイズは RAID アレイの最も小さなディスクのサイズと同じになります。冗長性を異なる方法で確保しているより高い RAID レベルの RAID ボリュームに対しても同じことが言えます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "It is therefore important, when setting up RAID arrays (except for RAID-0 and “linear RAID”), to only assemble disks of identical, or very close, sizes, to avoid wasting resources."
msgstr "それ故、(RAID-0 と「リニア RAID」以外の) RAID アレイをセットアップする場合、資源の無駄を防ぐためにアレイを構成するディスクはそのサイズが完全に同じか近いものを使うことが重要です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>NOTE</emphasis> Spare disks"
msgstr "<emphasis>NOTE</emphasis> 予備ディスク"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "RAID levels that include redundancy allow assigning more disks than required to an array. The extra disks are used as spares when one of the main disks fails. For instance, in a mirror of two disks plus one spare, if one of the first two disks fails, the kernel will automatically (and immediately) reconstruct the mirror using the spare disk, so that redundancy stays assured after the reconstruction time. This can be used as another kind of safeguard for critical data."
msgstr "冗長性を持たせた RAID レベルでは、必要なディスク数よりも多くのディスクで RAID アレイを構成させることが可能です。追加的ディスクは主要ディスクに障害が起きた場合に予備として使われます。たとえば、2 台のディスクと 1 台の予備ディスクのミラー構成では、最初の 2 台のうちの 1 台に障害が起きた場合、カーネルは自動的に (そして素早く) 予備ディスクを使ってミラーを再構成し、再構成の完了後に冗長性が再確保されます。すなわち、重要なデータに対するもう一つの安全装置として予備ディスクを使うことが可能ということです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "One would be forgiven for wondering how this is better than simply mirroring on three disks to start with. The advantage of the “spare disk” configuration is that the spare disk can be shared across several RAID volumes. For instance, one can have three mirrored volumes, with redundancy assured even in the event of one disk failure, with only seven disks (three pairs, plus one shared spare), instead of the nine disks that would be required by three triplets."
msgstr "この方式が単純に 3 台のディスクに対して最初からミラーリングを行うよりも優れているとされることに疑問を持つかもしれません。「予備ディスク」を設定する利点は複数の RAID ボリュームで予備ディスクを共有することが可能という点です。たとえば、1 台のディスク障害に対する冗長性を確保した 3 つのミラーされたボリュームを構成するには、ディスクを 7 台 (3 つのペアと 1 台の共有された予備) 用意するだけですみます。これに対して各ボリュームに 3 台のディスクを用意する場合には 9 台のディスクが必要です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "This RAID level, although expensive (since only half of the physical storage space, at best, is useful), is widely used in practice. It is simple to understand, and it allows very simple backups: since both disks have identical contents, one of them can be temporarily extracted with no impact on the working system. Read performance is often increased since the kernel can read half of the data on each disk in parallel, while write performance isn't too severely degraded. In case of a RAID-1 array of N disks, the data stays available even with N-1 disk failures."
msgstr "RAID-1 は高価であるにも関わらず (良くても物理ストレージ領域のたった半分しか使えないにも関わらず)、広く実運用されています。RAID-1 は簡単に理解でき、簡単にバックアップできます。なぜなら両方のディスクが全く同じ内容を持っているため、片方を一時的に取り外しても運用システムに影響をおよぼさないからです。通常 RAID-1 を使うことで、読み込み性能は好転します。なぜなら、カーネルはデータの半分をそれぞれのディスクから並行して読むことができるからです。これに対して、書き込み性能はそれほど悪化しません。N 台のディスクからなる RAID-1 アレイの場合、データは N-1 台のディスク障害に対して保護されます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "RAID-4"
msgstr "RAID-4"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "This RAID level, not widely used, uses N disks to store useful data, and an extra disk to store redundancy information. If that disk fails, the system can reconstruct its contents from the other N. If one of the N data disks fails, the remaining N-1 combined with the “parity” disk contain enough information to reconstruct the required data."
msgstr "RAID-4 は広く使われていません。RAID-4 は実データを保存するために N 台のディスクを使い、冗長性情報を保存するために 1 台の「パリティ」ディスクを使います。「パリティ」ディスクに障害が起きた場合、システムは他の N 台からデータを再構成することが可能です。N 台のデータディスクのうち、最大で 1 台に障害が起きた場合、残りの N-1 台と「パリティ」ディスクには、要求されたデータを再構成するために十分な情報が含まれます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "RAID-4 isn't too expensive since it only involves a one-in-N increase in costs and has no noticeable impact on read performance, but writes are slowed down. Furthermore, since a write to any of the N disks also involves a write to the parity disk, the latter sees many more writes than the former, and its lifespan can shorten dramatically as a consequence. Data on a RAID-4 array is safe only up to one failed disk (of the N+1)."
msgstr "RAID-4 は高価過ぎるというわけではありません。なぜならディスク 1 台につきたった N 分の 1 台分の追加費用で済むからです。また RAID-4 を使うと読み込み性能が大きく低下するというわけでもありません。しかしながら、RAID-4 は書き込み性能に深刻な影響をおよぼします。加えて、N 台の実データ用ディスクのどのディスクに書き込んでもパリティディスクに対する書き込みが発生するので、パリティディスクは実データ用ディスクに比べて書き込み回数が増えます。その結果、パリティディスクは極めて寿命が短くなります。RAID-4 アレイのデータは (N+1 台のディスクのうち) 1 台の障害に対して保護されます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "RAID-5"
msgstr "RAID-5"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "RAID-5 addresses the asymmetry issue of RAID-4: parity blocks are spread over all of the N+1 disks, with no single disk having a particular role."
msgstr "RAID-5 は RAID-4 の非対称性問題を対処したものです。すなわち、パリティブロックは N+1 台のディスクに分散して保存され、特定のディスクが特定の役割を果たすことはありません。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Read and write performance are identical to RAID-4. Here again, the system stays functional with up to one failed disk (of the N+1), but no more."
msgstr "読み込みと書き込み性能は RAID-4 と同様です。繰り返しになりますが、RAID-5 システムは (N+1 台のディスクのうち) 最大で 1 台までに障害が起きても動作します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "RAID-6"
msgstr "RAID-6"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "RAID-6 can be considered an extension of RAID-5, where each series of N blocks involves two redundancy blocks, and each such series of N+2 blocks is spread over N+2 disks."
msgstr "RAID-6 は RAID-5 の拡張と考えられます。RAID-6 では、N 個の連続するブロックに対して 2 個の冗長性ブロックを使います。この N+2 個のブロックは N+2 台のディスクに分散して保存されます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "This RAID level is slightly more expensive than the previous two, but it brings some extra safety since up to two drives (of the N+2) can fail without compromising data availability. The counterpart is that write operations now involve writing one data block and two redundancy blocks, which makes them even slower."
msgstr "RAID-6 は RAID-4 と RAID-5 に比べて少し高価ですが、RAID-6 を使うことで安全性はさらに高まります。なぜなら、(N+2 台中の) 最大で 2 台までの障害に対してデータを守ることが可能だからです。書き込み操作は 1 つのデータブロックと 2 つの冗長性ブロックを書き込むことに対応しますから、RAID-6 の書き込み性能は RAID-4 と RAID-5 に比べてさらに悪化します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "RAID-1+0"
msgstr "RAID-1+0"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "This isn't strictly speaking, a RAID level, but a stacking of two RAID groupings. Starting from 2×N disks, one first sets them up by pairs into N RAID-1 volumes; these N volumes are then aggregated into one, either by “linear RAID” or (increasingly) by LVM. This last case goes farther than pure RAID, but there's no problem with that."
msgstr "厳密に言えば RAID-1+0 は RAID レベルではなく、2 種類の RAID 分類を積み重ねたものです。RAID-1+0 を使うには 2×N 台のディスクが必要で、最初に 2 台ずつのペアから N 台の RAID-1 ボリュームを作ります。N 台の RAID-1 ボリュームは「リニア RAID」か LVM (次第にこちらを選ぶケースが増えています) のどちらか一方を使って 1 台に統合されます。LVM を使うと純粋な RAID ではなくなりますが、LVM を使っても問題はありません。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "RAID-1+0 can survive multiple disk failures: up to N in the 2×N array described above, provided that at least one disk keeps working in each of the RAID-1 pairs."
msgstr "RAID-1+0 は複数のディスク障害を乗り切ることが可能です。具体的に言えば、上に挙げた 2×N アレイの場合、最大で N 台までの障害に耐えます。ただし、各 RAID-1 ペアを構成するディスクの両方に障害が発生してはいけません。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>GOING FURTHER</emphasis> RAID-10"
msgstr "<emphasis>GOING FURTHER</emphasis> RAID-10"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "RAID-10 is generally considered a synonym of RAID-1+0, but a Linux specificity makes it actually a generalization. This setup allows a system where each block is stored on two different disks, even with an odd number of disks, the copies being spread out along a configurable model."
msgstr "通常 RAID-10 は RAID-1+0 の同意語と考えられますが、Linux では特別に RAID-10 をより一般的な構成を可能にするものとして定めています。RAID-10 では、システムが各ブロックを 2 種類の異なるディスクに保存することが可能です。奇数台のディスク構成の場合でも、ブロックのコピーは設定可能なモデルに従って分散して保存されます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Performances will vary depending on the chosen repartition model and redundancy level, and of the workload of the logical volume."
msgstr "RAID-10 の性能は選択した再分割モデルと冗長性の度合い、そして論理ボリュームの作業負荷に依存して変化します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Obviously, the RAID level will be chosen according to the constraints and requirements of each application. Note that a single computer can have several distinct RAID arrays with different configurations."
msgstr "RAID レベルを選ぶ際には、各用途からの制限および要求を考慮する必要があるのは明らかです。1 台のコンピュータに異なる設定を持つ複数の RAID アレイを配置することが可能である点に注意してください。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Setting up RAID"
msgstr "RAID の設定"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">mdadm</emphasis></primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Setting up RAID volumes requires the <emphasis role=\"pkg\">mdadm</emphasis> package; it provides the <command>mdadm</command> command, which allows creating and manipulating RAID arrays, as well as scripts and tools integrating it to the rest of the system, including the monitoring system."
msgstr "RAID ボリュームを設定するには <emphasis role=\"pkg\">mdadm</emphasis> パッケージが必要です。<emphasis role=\"pkg\">mdadm</emphasis> パッケージには RAID アレイを作成したり操作するための <command>mdadm</command> コマンド、システムの他の部分に RAID アレイを統合するためのスクリプトやツール、監視システムが含まれます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Our example will be a server with a number of disks, some of which are already used, the rest being available to setup RAID. We initially have the following disks and partitions:"
msgstr "以下の例では、多数のディスクを持つサーバをセットアップします。ディスクの一部は既に利用されており、残りは RAID をセットアップするために利用できるようになっています。最初の状態で、以下のディスクとパーティションが存在します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "the <filename>sdb</filename> disk, 4 GB, is entirely available;"
msgstr "<filename>sdb</filename> ディスク (4 GB) は全領域を利用できます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "the <filename>sdc</filename> disk, 4 GB, is also entirely available;"
msgstr "<filename>sdc</filename> ディスク (4 GB) は全領域を利用できます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "on the <filename>sdd</filename> disk, only partition <filename>sdd2</filename> (about 4 GB) is available;"
msgstr "<filename>sdd</filename> ディスクは <filename>sdd2</filename> パーティション (約 4 GB) だけを利用できます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "finally, a <filename>sde</filename> disk, still 4 GB, entirely available."
msgstr "<filename>sde</filename> ディスク (4 GB) は全領域を利用できます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>NOTE</emphasis> Identifying existing RAID volumes"
msgstr "<emphasis>NOTE</emphasis> 既存の RAID ボリュームの識別"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The <filename>/proc/mdstat</filename> file lists existing volumes and their states. When creating a new RAID volume, care should be taken not to name it the same as an existing volume."
msgstr "<filename>/proc/mdstat</filename> ファイルには既存のボリュームとその状態が書かれています。新しい RAID ボリュームを作成する場合、既存のボリュームと同じ名前を付けないように注意してください。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "We're going to use these physical elements to build two volumes, one RAID-0 and one mirror (RAID-1). Let's start with the RAID-0 volume:"
msgstr "RAID-0 とミラー (RAID-1) の 2 つのボリュームを作るために上記の物理ディスクを使います。それでは RAID-0 ボリュームから作っていきましょう。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</userinput>\n"
"<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md0 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md0</userinput>\n"
"<computeroutput>/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md0</userinput>\n"
"<computeroutput>/dev/md0:\n"
"        Version : 1.2\n"
"  Creation Time : Wed May  6 09:24:34 2015\n"
"     Raid Level : raid0\n"
"     Array Size : 8387584 (8.00 GiB 8.59 GB)\n"
"   Raid Devices : 2\n"
"  Total Devices : 2\n"
"    Persistence : Superblock is persistent\n"
"\n"
"    Update Time : Wed May  6 09:24:34 2015\n"
"          State : clean \n"
" Active Devices : 2\n"
"Working Devices : 2\n"
" Failed Devices : 0\n"
"  Spare Devices : 0\n"
"\n"
"     Chunk Size : 512K\n"
"\n"
"           Name : mirwiz:0  (local to host mirwiz)\n"
"           UUID : bb085b35:28e821bd:20d697c9:650152bb\n"
"         Events : 0\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       16        0      active sync   /dev/sdb\n"
"       1       8       32        1      active sync   /dev/sdc\n"
"# </computeroutput><userinput>mkfs.ext4 /dev/md0</userinput>\n"
"<computeroutput>mke2fs 1.42.12 (29-Aug-2014)\n"
"Creating filesystem with 2095104 4k blocks and 524288 inodes\n"
"Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6\n"
"Superblock backups stored on blocks: \n"
"        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n"
"\n"
"Allocating group tables: done                            \n"
"Writing inode tables: done                            \n"
"Creating journal (32768 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"# </computeroutput><userinput>mkdir /srv/raid-0</userinput>\n"
"<computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0</userinput>\n"
"<computeroutput># </computeroutput><userinput>df -h /srv/raid-0</userinput>\n"
"<computeroutput>Filesystem      Size  Used Avail Use% Mounted on\n"
"/dev/md0        7.9G   18M  7.4G   1% /srv/raid-0\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</userinput>\n"
"<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md0 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md0</userinput>\n"
"<computeroutput>/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md0</userinput>\n"
"<computeroutput>/dev/md0:\n"
"        Version : 1.2\n"
"  Creation Time : Wed May  6 09:24:34 2015\n"
"     Raid Level : raid0\n"
"     Array Size : 8387584 (8.00 GiB 8.59 GB)\n"
"   Raid Devices : 2\n"
"  Total Devices : 2\n"
"    Persistence : Superblock is persistent\n"
"\n"
"    Update Time : Wed May  6 09:24:34 2015\n"
"          State : clean \n"
" Active Devices : 2\n"
"Working Devices : 2\n"
" Failed Devices : 0\n"
"  Spare Devices : 0\n"
"\n"
"     Chunk Size : 512K\n"
"\n"
"           Name : mirwiz:0  (local to host mirwiz)\n"
"           UUID : bb085b35:28e821bd:20d697c9:650152bb\n"
"         Events : 0\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       16        0      active sync   /dev/sdb\n"
"       1       8       32        1      active sync   /dev/sdc\n"
"# </computeroutput><userinput>mkfs.ext4 /dev/md0</userinput>\n"
"<computeroutput>mke2fs 1.42.12 (29-Aug-2014)\n"
"Creating filesystem with 2095104 4k blocks and 524288 inodes\n"
"Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6\n"
"Superblock backups stored on blocks: \n"
"        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n"
"\n"
"Allocating group tables: done                            \n"
"Writing inode tables: done                            \n"
"Creating journal (32768 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"# </computeroutput><userinput>mkdir /srv/raid-0</userinput>\n"
"<computeroutput># </computeroutput><userinput>mount /dev/md0 /srv/raid-0</userinput>\n"
"<computeroutput># </computeroutput><userinput>df -h /srv/raid-0</userinput>\n"
"<computeroutput>ファイルシス   サイズ  使用  残り 使用% マウント位置\n"
"/dev/md0         7.9G   18M  7.4G    1% /srv/raid-0\n"
"</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The <command>mdadm --create</command> command requires several parameters: the name of the volume to create (<filename>/dev/md*</filename>, with MD standing for <foreignphrase>Multiple Device</foreignphrase>), the RAID level, the number of disks (which is compulsory despite being mostly meaningful only with RAID-1 and above), and the physical drives to use. Once the device is created, we can use it like we'd use a normal partition, create a filesystem on it, mount that filesystem, and so on. Note that our creation of a RAID-0 volume on <filename>md0</filename> is nothing but coincidence, and the numbering of the array doesn't need to be correlated to the chosen amount of redundancy. It's also possible to create named RAID arrays, by giving <command>mdadm</command> parameters such as <filename>/dev/md/linear</filename> instead of <filename>/dev/md0</filename>."
msgstr "<command>mdadm --create</command> コマンドには複数のパラメータが必要です。具体的に言えば、作成するボリュームの名前 (<filename>/dev/md*</filename>、MD は <foreignphrase>Multiple Device</foreignphrase> を意味します)、RAID レベル、ディスク数 (普通この値は RAID-1 とそれ以上のレベルでのみ意味があるにも関わらず、これは必須オプションです)、RAID を構成する物理デバイスを指定する必要があります。RAID デバイスを作成したら、RAID デバイスを通常のパーティションを取り扱うのと同様のやり方で取り扱うことが可能です。すなわち、ファイルシステムを作成したり、ファイルシステムをマウントしたりすることが可能です。ここで作成する RAID-0 ボリュームに <filename>md0</filename> と名前を付けたのは偶然に過ぎない点に注意してください。アレイに付けられた番号と冗長性の度合いを関連付ける必要はありません。また、<filename>/dev/md0</filename> の代わりに <filename>/dev/md/linear</filename> のようなパラメータを <command>mdadm</command> に渡すことで、名前付き RAID アレイを作成することも可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Creation of a RAID-1 follows a similar fashion, the differences only being noticeable after the creation:"
msgstr "同様のやり方で RAID-1 を作成します。注意するべき違いは作成後に説明します。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</userinput>\n"
"<computeroutput>mdadm: Note: this array has metadata at the start and\n"
"    may not be suitable as a boot device.  If you plan to\n"
"    store '/boot' on this device please ensure that\n"
"    your boot-loader understands md/v1.x metadata, or use\n"
"    --metadata=0.90\n"
"mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%\n"
"Continue creating array? </computeroutput><userinput>y</userinput>\n"
"<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md1 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md1</userinput>\n"
"<computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"        Version : 1.2\n"
"  Creation Time : Wed May  6 09:30:19 2015\n"
"     Raid Level : raid1\n"
"     Array Size : 4192192 (4.00 GiB 4.29 GB)\n"
"  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)\n"
"   Raid Devices : 2\n"
"  Total Devices : 2\n"
"    Persistence : Superblock is persistent\n"
"\n"
"    Update Time : Wed May  6 09:30:40 2015\n"
"          State : clean, resyncing (PENDING) \n"
" Active Devices : 2\n"
"Working Devices : 2\n"
" Failed Devices : 0\n"
"  Spare Devices : 0\n"
"\n"
"           Name : mirwiz:1  (local to host mirwiz)\n"
"           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
"         Events : 0\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       1       8       64        1      active sync   /dev/sde\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"          State : clean\n"
"[...]\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</userinput>\n"
"<computeroutput>mdadm: Note: this array has metadata at the start and\n"
"    may not be suitable as a boot device.  If you plan to\n"
"    store '/boot' on this device please ensure that\n"
"    your boot-loader understands md/v1.x metadata, or use\n"
"    --metadata=0.90\n"
"mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%\n"
"Continue creating array? </computeroutput><userinput>y</userinput>\n"
"<computeroutput>mdadm: Defaulting to version 1.2 metadata\n"
"mdadm: array /dev/md1 started.\n"
"# </computeroutput><userinput>mdadm --query /dev/md1</userinput>\n"
"<computeroutput>/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"        Version : 1.2\n"
"  Creation Time : Wed May  6 09:30:19 2015\n"
"     Raid Level : raid1\n"
"     Array Size : 4192192 (4.00 GiB 4.29 GB)\n"
"  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)\n"
"   Raid Devices : 2\n"
"  Total Devices : 2\n"
"    Persistence : Superblock is persistent\n"
"\n"
"    Update Time : Wed May  6 09:30:40 2015\n"
"          State : clean, resyncing (PENDING) \n"
" Active Devices : 2\n"
"Working Devices : 2\n"
" Failed Devices : 0\n"
"  Spare Devices : 0\n"
"\n"
"           Name : mirwiz:1  (local to host mirwiz)\n"
"           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
"         Events : 0\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       1       8       64        1      active sync   /dev/sde\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"          State : clean\n"
"[...]\n"
"</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>TIP</emphasis> RAID, disks and partitions"
msgstr "<emphasis>TIP</emphasis> RAID、ディスク、パーティション"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "As illustrated by our example, RAID devices can be constructed out of disk partitions, and do not require full disks."
msgstr "上の例で示した通り、RAID デバイスはディスクパーティションに作成することが可能です。必ずディスク全体を使わなければいけないというわけではありません。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "A few remarks are in order. First, <command>mdadm</command> notices that the physical elements have different sizes; since this implies that some space will be lost on the bigger element, a confirmation is required."
msgstr "いくつかの注意点があります。最初に、<command>mdadm</command> は物理デバイス同士のサイズが異なる点を指摘しています。さらに、このことによりサイズが大きい側のデバイスの一部の領域が使えなくなるため、確認が求められています。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "More importantly, note the state of the mirror. The normal state of a RAID mirror is that both disks have exactly the same contents. However, nothing guarantees this is the case when the volume is first created. The RAID subsystem will therefore provide that guarantee itself, and there will be a synchronization phase as soon as the RAID device is created. After some time (the exact amount will depend on the actual size of the disks…), the RAID array switches to the “active” or “clean” state. Note that during this reconstruction phase, the mirror is in a degraded mode, and redundancy isn't assured. A disk failing during that risk window could lead to losing all the data. Large amounts of critical data, however, are rarely stored on a freshly created RAID array before its initial synchronization. Note that even in degraded mode, the <filename>/dev/md1</filename> is usable, and a filesystem can be created on it, as well as some data copied on it."
msgstr "さらに重要なことは、ミラーの状態に注意することです。RAID ミラーの正常な状態とは、両方のディスクが全く同じ内容を持っている状態です。しかしながら、ボリュームを最初に作成した直後の RAID ミラーは正常な状態であることを保証されません。このため、RAID サブシステムは RAID ミラーの正常な状態を保証するために、RAID デバイスが作成されたらすぐに同期化作業を始めます。しばらくの後 (必要な時間はディスクの実サイズに依存します)、RAID アレイは「active」または「clean」状態に移行します。同期化作業中、ミラーは信頼性低下状態で、冗長性は保証されない点に注意してください。同期化作業中にディスク障害が起きると、すべてのデータを失うことにつながる恐れがあります。しかしながら、最近作成された RAID アレイの最初の同期化作業の前に大量の重要なデータがこの RAID アレイに保存されていることはほとんどないでしょう。信頼性低下状態であっても <filename>/dev/md1</filename> を利用することが可能で、ファイルシステムを作成したり、データのコピーを取ったりすることが可能という点に注意してください。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>TIP</emphasis> Starting a mirror in degraded mode"
msgstr "<emphasis>TIP</emphasis> 信頼性低下状態でミラーを開始する"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Sometimes two disks are not immediately available when one wants to start a RAID-1 mirror, for instance because one of the disks one plans to include is already used to store the data one wants to move to the array. In such circumstances, it is possible to deliberately create a degraded RAID-1 array by passing <filename>missing</filename> instead of a device file as one of the arguments to <command>mdadm</command>. Once the data have been copied to the “mirror”, the old disk can be added to the array. A synchronization will then take place, giving us the redundancy that was wanted in the first place."
msgstr "RAID-1 ミラーを構成する 2 台のディスクの両方をすぐに使えないことが時々あります。たとえば、ミラーを構成するディスクの片方にミラーに移動したいデータが既に保存されている場合です。このような場合、<command>mdadm</command> に渡すデバイスファイル引数の片方をデバイスファイルの代わりに <filename>missing</filename> にすることで、意図的に信頼性低下状態の RAID-1 アレイを作成することも可能です。ミラーに移動したいデータを含むディスクからデータを「ミラー」にコピーした後、そのディスクをアレイに追加することが可能です。追加作業が終われば、同期化作業が行われ、ミラーに移動したかったデータの冗長性が確保されます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>TIP</emphasis> Setting up a mirror without synchronization"
msgstr "<emphasis>TIP</emphasis> 同期化作業を行わずにミラーをセットアップする"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: PTAL;
msgid "RAID-1 volumes are often created to be used as a new disk, often considered blank. The actual initial contents of the disk is therefore not very relevant, since one only needs to know that the data written after the creation of the volume, in particular the filesystem, can be accessed later."
msgstr "通常 RAID-1 ボリュームは新しいディスクとして使うために作成され、RAID-1 ボリュームの作成直後にはデータが保存されていないと考えられます。すなわち、RAID-1 ボリュームの初期内容に価値はなく、RAID-1 で保護したい重要なデータは RAID-1 ボリュームの作成後に書き込まれるデータというわけです。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: PTAL;
msgid "One might therefore wonder about the point of synchronizing both disks at creation time. Why care whether the contents are identical on zones of the volume that we know will only be read after we have written to them?"
msgstr "そう考えると、RAID-1 ボリュームにデータが書き込まれる前に RAID-1 ボリュームを構成するディスクの内容が同期されるという点について疑問に思うかもしれません。RAID-1 ボリュームに書き込んでいないデータは後から読み込まれることもないのにも関わらず、なぜ RAID-1 ボリュームの作成時に RAID-1 ボリュームを構成するディスクの内容の同期化作業が必要なのでしょうか?"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: PTAL;
msgid "Fortunately, this synchronization phase can be avoided by passing the <literal>--assume-clean</literal> option to <command>mdadm</command>. However, this option can lead to surprises in cases where the initial data will be read (for instance if a filesystem is already present on the physical disks), which is why it isn't enabled by default."
msgstr "幸いなことに、RAID-1 を構成するディスクの内容の同期化作業は <literal>--assume-clean</literal> オプションを <command>mdadm</command> に渡せば避けることが可能です。しかしながら、初期データが読まれる場合、<literal>--assume-clean</literal> オプションを使うと問題があります (たとえば、物理ディスク上にファイルシステムが既に存在している場合、問題があります)。このため、デフォルトでこのオプションは有効化されません。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Now let's see what happens when one of the elements of the RAID-1 array fails. <command>mdadm</command>, in particular its <literal>--fail</literal> option, allows simulating such a disk failure:"
msgstr "RAID-1 アレイを構成するディスクの 1 台に障害が発生した場合、何が起きるかを見て行きましょう。<command>mdadm</command> に <literal>--fail</literal> オプションを付けることで、ディスク障害を模倣することが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde</userinput>\n"
"<computeroutput>mdadm: set /dev/sde faulty in /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"    Update Time : Wed May  6 09:39:39 2015\n"
"          State : clean, degraded \n"
" Active Devices : 1\n"
"Working Devices : 1\n"
" Failed Devices : 1\n"
"  Spare Devices : 0\n"
"\n"
"           Name : mirwiz:1  (local to host mirwiz)\n"
"           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
"         Events : 19\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       2       0        0        2      removed\n"
"\n"
"       1       8       64        -      faulty   /dev/sde</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --fail /dev/sde</userinput>\n"
"<computeroutput>mdadm: set /dev/sde faulty in /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"    Update Time : Wed May  6 09:39:39 2015\n"
"          State : clean, degraded \n"
" Active Devices : 1\n"
"Working Devices : 1\n"
" Failed Devices : 1\n"
"  Spare Devices : 0\n"
"\n"
"           Name : mirwiz:1  (local to host mirwiz)\n"
"           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
"         Events : 19\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       2       0        0        2      removed\n"
"\n"
"       1       8       64        -      faulty   /dev/sde</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The contents of the volume are still accessible (and, if it is mounted, the applications don't notice a thing), but the data safety isn't assured anymore: should the <filename>sdd</filename> disk fail in turn, the data would be lost. We want to avoid that risk, so we'll replace the failed disk with a new one, <filename>sdf</filename>:"
msgstr "RAID-1 ボリュームの内容はまだアクセスすることが可能ですが (そして、RAID-1 ボリュームがマウントされていた場合、アプリケーションはディスク障害に気が付きませんが)、データの安全性はもはや保証されません。つまり <filename>sdd</filename> ディスクにも障害が発生した場合、データは失われます。この危険性を避けるために、障害の発生したディスクを新しいディスク <filename>sdf</filename> に交換します。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>\n"
"<computeroutput>mdadm: added /dev/sdf\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"   Raid Devices : 2\n"
"  Total Devices : 3\n"
"    Persistence : Superblock is persistent\n"
"\n"
"    Update Time : Wed May  6 09:48:49 2015\n"
"          State : clean, degraded, recovering \n"
" Active Devices : 1\n"
"Working Devices : 2\n"
" Failed Devices : 1\n"
"  Spare Devices : 1\n"
"\n"
" Rebuild Status : 28% complete\n"
"\n"
"           Name : mirwiz:1  (local to host mirwiz)\n"
"           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
"         Events : 26\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       2       8       80        1      spare rebuilding   /dev/sdf\n"
"\n"
"       1       8       64        -      faulty   /dev/sde\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"    Update Time : Wed May  6 09:49:08 2015\n"
"          State : clean \n"
" Active Devices : 2\n"
"Working Devices : 2\n"
" Failed Devices : 1\n"
"  Spare Devices : 0\n"
"\n"
"           Name : mirwiz:1  (local to host mirwiz)\n"
"           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
"         Events : 41\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       2       8       80        1      active sync   /dev/sdf\n"
"\n"
"       1       8       64        -      faulty   /dev/sde</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --add /dev/sdf</userinput>\n"
"<computeroutput>mdadm: added /dev/sdf\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"   Raid Devices : 2\n"
"  Total Devices : 3\n"
"    Persistence : Superblock is persistent\n"
"\n"
"    Update Time : Wed May  6 09:48:49 2015\n"
"          State : clean, degraded, recovering \n"
" Active Devices : 1\n"
"Working Devices : 2\n"
" Failed Devices : 1\n"
"  Spare Devices : 1\n"
"\n"
" Rebuild Status : 28% complete\n"
"\n"
"           Name : mirwiz:1  (local to host mirwiz)\n"
"           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
"         Events : 26\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       2       8       80        1      spare rebuilding   /dev/sdf\n"
"\n"
"       1       8       64        -      faulty   /dev/sde\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"    Update Time : Wed May  6 09:49:08 2015\n"
"          State : clean \n"
" Active Devices : 2\n"
"Working Devices : 2\n"
" Failed Devices : 1\n"
"  Spare Devices : 0\n"
"\n"
"           Name : mirwiz:1  (local to host mirwiz)\n"
"           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464\n"
"         Events : 41\n"
"\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       2       8       80        1      active sync   /dev/sdf\n"
"\n"
"       1       8       64        -      faulty   /dev/sde</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Here again, the kernel automatically triggers a reconstruction phase during which the volume, although still accessible, is in a degraded mode. Once the reconstruction is over, the RAID array is back to a normal state. One can then tell the system that the <filename>sde</filename> disk is about to be removed from the array, so as to end up with a classical RAID mirror on two disks:"
msgstr "繰り返しになりますが、ボリュームはまだアクセスすることが可能とは言うもののボリュームが信頼性低下状態ならば、カーネルは自動的に再構成作業を実行します。再構成作業が終了したら、RAID アレイは正常状態に戻ります。ここで、システムに <filename>sde</filename> ディスクをアレイから削除することを伝えることが可能です。削除することで、2 台のディスクからなる古典的な RAID ミラーになります。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde</userinput>\n"
"<computeroutput>mdadm: hot removed /dev/sde from /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       2       8       80        1      active sync   /dev/sdf</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm /dev/md1 --remove /dev/sde</userinput>\n"
"<computeroutput>mdadm: hot removed /dev/sde from /dev/md1\n"
"# </computeroutput><userinput>mdadm --detail /dev/md1</userinput>\n"
"<computeroutput>/dev/md1:\n"
"[...]\n"
"    Number   Major   Minor   RaidDevice State\n"
"       0       8       50        0      active sync   /dev/sdd2\n"
"       2       8       80        1      active sync   /dev/sdf</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "From then on, the drive can be physically removed when the server is next switched off, or even hot-removed when the hardware configuration allows hot-swap. Such configurations include some SCSI controllers, most SATA disks, and external drives operating on USB or Firewire."
msgstr "この後、今後サーバの電源を切った際にドライブを物理的に取り外したり、ハードウェア設定がホットスワップに対応しているならばドライブをホットリムーブすることが可能です。一部の SCSI コントローラ、多くの SATA ディスク、USB や Firewire で接続された外部ドライブなどはホットスワップに対応しています。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Backing up the Configuration"
msgstr "設定のバックアップ"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Most of the meta-data concerning RAID volumes are saved directly on the disks that make up these arrays, so that the kernel can detect the arrays and their components and assemble them automatically when the system starts up. However, backing up this configuration is encouraged, because this detection isn't fail-proof, and it is only expected that it will fail precisely in sensitive circumstances. In our example, if the <filename>sde</filename> disk failure had been real (instead of simulated) and the system had been restarted without removing this <filename>sde</filename> disk, this disk could start working again due to having been probed during the reboot. The kernel would then have three physical elements, each claiming to contain half of the same RAID volume. Another source of confusion can come when RAID volumes from two servers are consolidated onto one server only. If these arrays were running normally before the disks were moved, the kernel would be able to detect and reassemble the pairs properly; but if the moved disks had been aggregated into an <filename>md1</filename> on the old server, and the new server already has an <filename>md1</filename>, one of the mirrors would be renamed."
msgstr "RAID ボリュームに関連するメタデータのほとんどはアレイを構成するディスク上に直接保存されています。このため、カーネルはアレイとその構成要素を検出し、システムの起動時に自動的にアレイを組み立てることが可能です。しかしながら、この設定をバックアップすることを推奨します。なぜなら、この検出機構は不注意による間違いを防ぐものではないからです。そして、注意して取り扱うべき状況ではまさに検出機構がうまく働かないことが見込まれます。上の例で、<filename>sde</filename> ディスク障害が本物で (模倣でない)、<filename>sde</filename> ディスクを取り外す前にシステムを再起動した場合、<filename>sde</filename> ディスクは再起動中に検出され、システムに復帰します。カーネルは 3 つの物理ディスクを検出し、それぞれのディスクが同じ RAID ボリュームの片割れであると主張します。さらに別の混乱する状況が考えられます。2 台のサーバで使われていた RAID ボリュームを片方のサーバに集約することを考えてみましょう。ディスクが移動される前、各アレイは正常に実行されていました。カーネルはアレイを検出して、適切なペアを組み立てることが可能です。しかし、片方のサーバに移動されたディスクが前のサーバでは <filename>md1</filename> に組み込まれており、さらに新しいサーバが既に <filename>md1</filename> という名前のアレイを持っていた場合、どちらか一方の名前が変えられます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Backing up the configuration is therefore important, if only for reference. The standard way to do it is by editing the <filename>/etc/mdadm/mdadm.conf</filename> file, an example of which is listed here:"
msgstr "このため、参考情報に過ぎないとは言うものの、設定を保存することは重要です。設定を保存する標準的な方法は <filename>/etc/mdadm/mdadm.conf</filename> ファイルを編集することです。以下に例を示します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<command>mdadm</command> configuration file"
msgstr "<command>mdadm</command> 設定ファイル"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
# Ref: $ man 5 mdadm.conf;
# Ref: $ man 1 madam;
msgid ""
"# mdadm.conf\n"
"#\n"
"# Please refer to mdadm.conf(5) for information about this file.\n"
"#\n"
"\n"
"# by default (built-in), scan all partitions (/proc/partitions) and all\n"
"# containers for MD superblocks. alternatively, specify devices to scan, using\n"
"# wildcards if desired.\n"
"DEVICE /dev/sd*\n"
"\n"
"# auto-create devices with Debian standard permissions\n"
"CREATE owner=root group=disk mode=0660 auto=yes\n"
"\n"
"# automatically tag new arrays as belonging to the local system\n"
"HOMEHOST &lt;system&gt;\n"
"\n"
"# instruct the monitoring daemon where to send mail alerts\n"
"MAILADDR root\n"
"\n"
"# definitions of existing MD arrays\n"
"ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb\n"
"ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464\n"
"\n"
"# This configuration was auto-generated on Thu, 17 Jan 2013 16:21:01 +0100\n"
"# by mkconf 3.2.5-3"
msgstr ""
"# mdadm.conf\n"
"#\n"
"# このファイルに関する詳細は mdadm.conf(5) を参照してください。\n"
"#\n"
"\n"
"# デフォルト (組み込み) 状態ならば、MD スーパーブロックを持つパーティション\n"
"# (/proc/partitions) とコンテナをすべてスキャンします。以下のようにスキャンする\n"
"# デバイスを指定することも可能です。必要ならばワイルドカードを使ってください。\n"
"DEVICE /dev/sd*\n"
"\n"
"# デバイスの自動作成時に使う Debian の標準的なパーミッションを指定します\n"
"CREATE owner=root group=disk mode=0660 auto=yes\n"
"\n"
"# 新規アレイの所属先にローカルシステムを自動登録します\n"
"HOMEHOST &lt;system&gt;\n"
"\n"
"# 監視デーモンに root を警告メールの送信先として通知します\n"
"MAILADDR root\n"
"\n"
"# 既存の MD アレイの定義\n"
"ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb\n"
"ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464\n"
"\n"
"# この設定ファイルは mkconf 3.2.5-3 により\n"
"# Fri, 18 Jan 2013 00:21:01 +0900 に自動生成されました"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "One of the most useful details is the <literal>DEVICE</literal> option, which lists the devices where the system will automatically look for components of RAID volumes at start-up time. In our example, we replaced the default value, <literal>partitions containers</literal>, with an explicit list of device files, since we chose to use entire disks and not only partitions, for some volumes."
msgstr "最も役に立つ設定項目の 1 つに <literal>DEVICE</literal> オプションがあります。これは起動時にシステムが RAID ボリュームの構成情報を自動的に探すデバイスをリストします。上の例では、値をデフォルト値 <literal>partitions containers</literal> からデバイスファイルを明示したリストに置き換えました。なぜなら、パーティションだけでなくすべてのディスクをボリュームとして使うように決めたからです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The last two lines in our example are those allowing the kernel to safely pick which volume number to assign to which array. The metadata stored on the disks themselves are enough to re-assemble the volumes, but not to determine the volume number (and the matching <filename>/dev/md*</filename> device name)."
msgstr "上の例における最後の 2 行を使うことで、カーネルはアレイに割り当てるボリューム番号を安全に選ぶことが可能です。ディスク本体に保存されたメタ情報はボリュームをもう一度組み上げるのに十分ですが、ボリューム番号を定義する (そして <filename>/dev/md*</filename> デバイス名にマッチすることを確認する) には不十分です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Fortunately, these lines can be generated automatically:"
msgstr "幸いなことに、以下のコマンドを実行すればこの行を自動的に生成することが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid ""
"<computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?</userinput>\n"
"<computeroutput>ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb\n"
"ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mdadm --misc --detail --brief /dev/md?</userinput>\n"
"<computeroutput>ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb\n"
"ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The contents of these last two lines doesn't depend on the list of disks included in the volume. It is therefore not necessary to regenerate these lines when replacing a failed disk with a new one. On the other hand, care must be taken to update the file when creating or deleting a RAID array."
msgstr "最後の 2 行の内容はボリュームを構成するディスクのリストに依存しません。このため、障害の発生したディスクを新しいディスクに交換した際に、これをもう一度生成する必要はありません。逆に、RAID アレイを作成および削除した際に、必ずこの設定ファイルを注意深く更新する必要があります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary>LVM</primary>"
msgstr "<primary>LVM</primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary>Logical Volume Manager</primary>"
msgstr "<primary>論理ボリュームマネージャ</primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "LVM, the <emphasis>Logical Volume Manager</emphasis>, is another approach to abstracting logical volumes from their physical supports, which focuses on increasing flexibility rather than increasing reliability. LVM allows changing a logical volume transparently as far as the applications are concerned; for instance, it is possible to add new disks, migrate the data to them, and remove the old disks, without unmounting the volume."
msgstr "LVM (<emphasis>論理ボリュームマネージャ</emphasis>) は物理ディスクから論理ボリュームを抽象化するもう一つの方法で、信頼性を増加させるのではなく柔軟性を増加させることに注目しています。LVM を使うことで、アプリケーションから見る限り透過的に論理ボリュームを変更することが可能です。LVM を使うことで、たとえば新しいディスクを追加し、データを新しいディスクに移行し、古いディスクを削除することがボリュームをアンマウントせずに可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "LVM Concepts"
msgstr "LVM の概念"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "This flexibility is attained by a level of abstraction involving three concepts."
msgstr "LVM の柔軟性は 3 つの概念から構成された抽象化レベルによって達成されます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "First, the PV (<emphasis>Physical Volume</emphasis>) is the entity closest to the hardware: it can be partitions on a disk, or a full disk, or even any other block device (including, for instance, a RAID array). Note that when a physical element is set up to be a PV for LVM, it should only be accessed via LVM, otherwise the system will get confused."
msgstr "1 番目の概念は PV (<emphasis>物理ボリューム</emphasis>) です。PV はハードウェアに最も近い要素です。具体的に言えば、PV はディスクのパーティション、ディスク全体、その他の任意のブロックデバイス (たとえば、RAID アレイ) などの物理的要素を指します。物理的要素を LVM の PV に設定した場合、物理的要素へのアクセスは必ず LVM を介すべきという点に注意してください。そうでなければ、システムが混乱します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "A number of PVs can be clustered in a VG (<emphasis>Volume Group</emphasis>), which can be compared to disks both virtual and extensible. VGs are abstract, and don't appear in a device file in the <filename>/dev</filename> hierarchy, so there's no risk of using them directly."
msgstr "2 番目の概念は VG (<emphasis>ボリュームグループ</emphasis>) です。複数の PV は VG にクラスタ化することが可能です。VG は仮想的かつ拡張できるディスクに例えられます。VG は概念的な要素で、<filename>/dev</filename> 階層のデバイスファイルに現れません。そのため、VG を直接的に操作する危険はありません。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The third kind of object is the LV (<emphasis>Logical Volume</emphasis>), which is a chunk of a VG; if we keep the VG-as-disk analogy, the LV compares to a partition. The LV appears as a block device with an entry in <filename>/dev</filename>, and it can be used as any other physical partition can be (most commonly, to host a filesystem or swap space)."
msgstr "3 番目の概念は LV (<emphasis>論理ボリューム</emphasis>) です。LV は VG の中の 1 つの塊です。さらに VG をディスクに例えたのと同様の考え方を使うと、LV はパーティションに例えられます。LV はブロックデバイスとして <filename>/dev</filename> に現れ、他の物理パーティションと同様に取り扱うことが可能です (一般的に言えば、LV にファイルシステムやスワップ領域を作成することが可能です)。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The important thing is that the splitting of a VG into LVs is entirely independent of its physical components (the PVs). A VG with only a single physical component (a disk for instance) can be split into a dozen logical volumes; similarly, a VG can use several physical disks and appear as a single large logical volume. The only constraint, obviously, is that the total size allocated to LVs can't be bigger than the total capacity of the PVs in the volume group."
msgstr "ここで重要な事柄は VG を LV に分割する場合に物理的要素 (PV) はいかなる制約も要求しないという点です。1 つの PV (たとえばディスク) から構成される VG を複数の LV に分割できます。同様に、複数の PV から構成される VG を 1 つの大きな LV として提供することも可能です。制約事項がたった 1 つしかないのは明らかです。それはある VG から分割された LV のサイズの合計はその VG を構成する PV のサイズの合計を超えることができないという点です。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: PTAL;
msgid "It often makes sense, however, to have some kind of homogeneity among the physical components of a VG, and to split the VG into logical volumes that will have similar usage patterns. For instance, if the available hardware includes fast disks and slower disks, the fast ones could be clustered into one VG and the slower ones into another; chunks of the first one can then be assigned to applications requiring fast data access, while the second one will be kept for less demanding tasks."
msgstr "しかしながら、ある VG を構成する PV 同士の性能を同様のものにしたり、その VG から分割された LV 同士に求められる性能を同様のものにしたりすることは通常理に適った方針です。たとえば、利用できるハードウェアに高速な PV と低速な PV がある場合、高速な PV から構成される VG と低速な PV から構成される VG に分けると良いでしょう。こうすることで、高速な PV から構成される VG から分割された LV を高速なデータアクセスを必要とするアプリケーションに割り当て、低速な PV から構成される VG から分割された LV を負荷の少ない作業用に割り当てることが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: PTAL;
msgid "In any case, keep in mind that an LV isn't particularly attached to any one PV. It is possible to influence where the data from an LV are physically stored, but this possibility isn't required for day-to-day use. On the contrary: when the set of physical components of a VG evolves, the physical storage locations corresponding to a particular LV can be migrated across disks (while staying within the PVs assigned to the VG, of course)."
msgstr "いかなる場合でも、LV は特定の PV を使用するわけではないという点を覚えておいてください。ある LV に含まれるデータの物理的な保存場所を操作することも可能ですが、普通に使っている限りその必要はありません。逆に、VG を構成する PV 群の構成要素が変化した場合、ある LV に含まれるデータの物理的な保存場所は対象の LV の分割元である VG の中ひいてはその VG を構成する PV 群の構成要素の中を移動することがあります (もちろん、データの移動先は対象の LV の分割元の VG を構成する PV 群の構成要素の中に限られます)。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Setting up LVM"
msgstr "LVM の設定"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Let us now follow, step by step, the process of setting up LVM for a typical use case: we want to simplify a complex storage situation. Such a situation usually happens after some long and convoluted history of accumulated temporary measures. For the purposes of illustration, we'll consider a server where the storage needs have changed over time, ending up in a maze of available partitions split over several partially used disks. In more concrete terms, the following partitions are available:"
msgstr "典型的な用途に対する LVM の設定過程を、段階的に見て行きましょう。具体的に言えば、複雑なストレージの状況を単純化したい場合を見ていきます。通常、長く複雑な一時的措置を繰り返した挙句の果てに、この状況に陥ることがあります。説明目的で、徐々にストレージを変更する必要のあったサーバを考えます。このサーバでは、PV として利用できるパーティションが複数の一部使用済みディスクに分散しています。より具体的に言えば、以下のパーティションを PV として利用できます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "on the <filename>sdb</filename> disk, a <filename>sdb2</filename> partition, 4 GB;"
msgstr "<filename>sdb</filename> ディスク上の <filename>sdb2</filename> パーティション (4 GB)。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "on the <filename>sdc</filename> disk, a <filename>sdc3</filename> partition, 3 GB;"
msgstr "<filename>sdc</filename> ディスク上の <filename>sdc3</filename> パーティション (3 GB)。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "the <filename>sdd</filename> disk, 4 GB, is fully available;"
msgstr "<filename>sdd</filename> ディスク (4 GB) は全領域を利用できます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "on the <filename>sdf</filename> disk, a <filename>sdf1</filename> partition, 4 GB; and a <filename>sdf2</filename> partition, 5 GB."
msgstr "<filename>sdf</filename> ディスク上の <filename>sdf1</filename> パーティション (4 GB) および <filename>sdf2</filename> パーティション (5 GB)。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "In addition, let's assume that disks <filename>sdb</filename> and <filename>sdf</filename> are faster than the other two."
msgstr "加えて、<filename>sdb</filename> と <filename>sdf</filename> が他の 2 台に比べて高速であると仮定しましょう。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: PTAL;
msgid "Our goal is to set up three logical volumes for three different applications: a file server requiring 5 GB of storage space, a database (1 GB) and some space for back-ups (12 GB). The first two need good performance, but back-ups are less critical in terms of access speed. All these constraints prevent the use of partitions on their own; using LVM can abstract the physical size of the devices, so the only limit is the total available space."
msgstr "今回の目標は、3 種類の異なるアプリケーション用に 3 つの LV を設定することです。具体的に言えば、5 GB のストレージ領域が必要なファイルサーバ、データベース (1 GB)、バックアップ用の領域 (12 GB) 用の LV を設定することです。ファイルサーバとデータベースは高い性能を必要とします。しかし、バックアップはアクセス速度をそれほど重要視しません。これらの要件により、各アプリケーションに設定する LV の使用する PV が決定されます。さらに LVM を使いますので、PV の物理的サイズからくる制限はありません。このため、PV 群として利用できる領域のサイズの合計だけが制限となります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The required tools are in the <emphasis role=\"pkg\">lvm2</emphasis> package and its dependencies. When they're installed, setting up LVM takes three steps, matching the three levels of concepts."
msgstr "LVM の設定に必要なツールは <emphasis role=\"pkg\">lvm2</emphasis> パッケージとその依存パッケージに含まれています。これらのパッケージをインストールしたら、3 つの手順を踏んで LVM を設定します。各手順は LVM の概念の 3 つの抽象化レベルに対応します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "First, we prepare the physical volumes using <command>pvcreate</command>:"
msgstr "最初に、<command>pvcreate</command> を使って PV を作成します。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>pvdisplay</userinput>\n"
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdb2\" successfully created\n"
"# </computeroutput><userinput>pvdisplay</userinput>\n"
"<computeroutput>  \"/dev/sdb2\" is a new physical volume of \"4.00 GiB\"\n"
"  --- NEW Physical volume ---\n"
"  PV Name               /dev/sdb2\n"
"  VG Name               \n"
"  PV Size               4.00 GiB\n"
"  Allocatable           NO\n"
"  PE Size               0   \n"
"  Total PE              0\n"
"  Free PE               0\n"
"  Allocated PE          0\n"
"  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I\n"
"\n"
"# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdc3\" successfully created\n"
"  Physical volume \"/dev/sdd\" successfully created\n"
"  Physical volume \"/dev/sdf1\" successfully created\n"
"  Physical volume \"/dev/sdf2\" successfully created\n"
"# </computeroutput><userinput>pvdisplay -C</userinput>\n"
"<computeroutput>  PV         VG   Fmt  Attr PSize PFree\n"
"  /dev/sdb2       lvm2 ---  4.00g 4.00g\n"
"  /dev/sdc3       lvm2 ---  3.09g 3.09g\n"
"  /dev/sdd        lvm2 ---  4.00g 4.00g\n"
"  /dev/sdf1       lvm2 ---  4.10g 4.10g\n"
"  /dev/sdf2       lvm2 ---  5.22g 5.22g\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>pvdisplay</userinput>\n"
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb2</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdb2\" successfully created\n"
"# </computeroutput><userinput>pvdisplay</userinput>\n"
"<computeroutput>  \"/dev/sdb2\" is a new physical volume of \"4.00 GiB\"\n"
"  --- NEW Physical volume ---\n"
"  PV Name               /dev/sdb2\n"
"  VG Name               \n"
"  PV Size               4.00 GiB\n"
"  Allocatable           NO\n"
"  PE Size               0   \n"
"  Total PE              0\n"
"  Free PE               0\n"
"  Allocated PE          0\n"
"  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I\n"
"\n"
"# </computeroutput><userinput>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdc3\" successfully created\n"
"  Physical volume \"/dev/sdd\" successfully created\n"
"  Physical volume \"/dev/sdf1\" successfully created\n"
"  Physical volume \"/dev/sdf2\" successfully created\n"
"# </computeroutput><userinput>pvdisplay -C</userinput>\n"
"<computeroutput>  PV         VG   Fmt  Attr PSize PFree\n"
"  /dev/sdb2       lvm2 ---  4.00g 4.00g\n"
"  /dev/sdc3       lvm2 ---  3.09g 3.09g\n"
"  /dev/sdd        lvm2 ---  4.00g 4.00g\n"
"  /dev/sdf1       lvm2 ---  4.10g 4.10g\n"
"  /dev/sdf2       lvm2 ---  5.22g 5.22g\n"
"</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "So far, so good; note that a PV can be set up on a full disk as well as on individual partitions of it. As shown above, the <command>pvdisplay</command> command lists the existing PVs, with two possible output formats."
msgstr "ここまでは順調です。PV はディスク全体およびディスク上の各パーティションに対して設定することが可能という点に注意してください。上に示した通り、<command>pvdisplay</command> コマンドは既存の PV をリストします。出力フォーマットは 2 種類あります。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "Now let's assemble these physical elements into VGs using <command>vgcreate</command>. We'll gather only PVs from the fast disks into a <filename>vg_critical</filename> VG; the other VG, <filename>vg_normal</filename>, will also include slower elements."
msgstr "<command>vgcreate</command> を使って、これらの PV から VG を構成しましょう。高速なディスクの PV から <filename>vg_critical</filename> VG を構成します。さらに、これ以外の低速なディスクの PV から <filename>vg_normal</filename> VG を構成します。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>vgdisplay</userinput>\n"
"<computeroutput>  No volume groups found\n"
"# </computeroutput><userinput>vgcreate vg_critical /dev/sdb2 /dev/sdf1</userinput>\n"
"<computeroutput>  Volume group \"vg_critical\" successfully created\n"
"# </computeroutput><userinput>vgdisplay</userinput>\n"
"<computeroutput>  --- Volume group ---\n"
"  VG Name               vg_critical\n"
"  System ID             \n"
"  Format                lvm2\n"
"  Metadata Areas        2\n"
"  Metadata Sequence No  1\n"
"  VG Access             read/write\n"
"  VG Status             resizable\n"
"  MAX LV                0\n"
"  Cur LV                0\n"
"  Open LV               0\n"
"  Max PV                0\n"
"  Cur PV                2\n"
"  Act PV                2\n"
"  VG Size               8.09 GiB\n"
"  PE Size               4.00 MiB\n"
"  Total PE              2071\n"
"  Alloc PE / Size       0 / 0   \n"
"  Free  PE / Size       2071 / 8.09 GiB\n"
"  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp\n"
"\n"
"# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</userinput>\n"
"<computeroutput>  Volume group \"vg_normal\" successfully created\n"
"# </computeroutput><userinput>vgdisplay -C</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize  VFree \n"
"  vg_critical   2   0   0 wz--n-  8.09g  8.09g\n"
"  vg_normal     3   0   0 wz--n- 12.30g 12.30g\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>vgdisplay</userinput>\n"
"<computeroutput>  No volume groups found\n"
"# </computeroutput><userinput>vgcreate vg_critical /dev/sdb2 /dev/sdf1</userinput>\n"
"<computeroutput>  Volume group \"vg_critical\" successfully created\n"
"# </computeroutput><userinput>vgdisplay</userinput>\n"
"<computeroutput>  --- Volume group ---\n"
"  VG Name               vg_critical\n"
"  System ID             \n"
"  Format                lvm2\n"
"  Metadata Areas        2\n"
"  Metadata Sequence No  1\n"
"  VG Access             read/write\n"
"  VG Status             resizable\n"
"  MAX LV                0\n"
"  Cur LV                0\n"
"  Open LV               0\n"
"  Max PV                0\n"
"  Cur PV                2\n"
"  Act PV                2\n"
"  VG Size               8.09 GiB\n"
"  PE Size               4.00 MiB\n"
"  Total PE              2071\n"
"  Alloc PE / Size       0 / 0   \n"
"  Free  PE / Size       2071 / 8.09 GiB\n"
"  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp\n"
"\n"
"# </computeroutput><userinput>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</userinput>\n"
"<computeroutput>  Volume group \"vg_normal\" successfully created\n"
"# </computeroutput><userinput>vgdisplay -C</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize  VFree \n"
"  vg_critical   2   0   0 wz--n-  8.09g  8.09g\n"
"  vg_normal     3   0   0 wz--n- 12.30g 12.30g\n"
"</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "Here again, commands are rather straightforward (and <command>vgdisplay</command> proposes two output formats). Note that it is quite possible to use two partitions of the same physical disk into two different VGs. Note also that we used a <filename>vg_</filename> prefix to name our VGs, but it is nothing more than a convention."
msgstr "繰り返しになりますが、<command>vgdisplay</command> コマンドはかなり簡潔です (そして <command>vgdisplay</command> には 2 種類の出力フォーマットがあります)。同じ物理ディスク上にある 2 つの PV から 2 つの異なる VG を構成することが可能である点に注意してください。また、<filename>vg_</filename> 接頭辞を VG の名前に使っていますが、これは慣例に過ぎない点に注意してください。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "We now have two “virtual disks”, sized about 8 GB and 12 GB, respectively. Let's now carve them up into “virtual partitions” (LVs). This involves the <command>lvcreate</command> command, and a slightly more complex syntax:"
msgstr "これでサイズが約 8 GB と約 12 GB の 2 台の「仮想ディスク」(VG) を手に入れたことになります。それでは仮想ディスク (VG) を「仮想パーティション」(LV) に分割しましょう。これを行うには <command>lvcreate</command> コマンドを少し複雑な構文で実行します。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>lvdisplay</userinput>\n"
"<computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical</userinput>\n"
"<computeroutput>  Logical volume \"lv_files\" created\n"
"# </computeroutput><userinput>lvdisplay</userinput>\n"
"<computeroutput>  --- Logical volume ---\n"
"  LV Path                /dev/vg_critical/lv_files\n"
"  LV Name                lv_files\n"
"  VG Name                vg_critical\n"
"  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT\n"
"  LV Write Access        read/write\n"
"  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400\n"
"  LV Status              available\n"
"  # open                 0\n"
"  LV Size                5.00 GiB\n"
"  Current LE             1280\n"
"  Segments               2\n"
"  Allocation             inherit\n"
"  Read ahead sectors     auto\n"
"  - currently set to     256\n"
"  Block device           253:0\n"
"\n"
"# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical</userinput>\n"
"<computeroutput>  Logical volume \"lv_base\" created\n"
"# </computeroutput><userinput>lvcreate -n lv_backups -L 12G vg_normal</userinput>\n"
"<computeroutput>  Logical volume \"lv_backups\" created\n"
"# </computeroutput><userinput>lvdisplay -C</userinput>\n"
"<computeroutput>  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_base    vg_critical -wi-a---  1.00g                                           \n"
"  lv_files   vg_critical -wi-a---  5.00g                                           \n"
"  lv_backups vg_normal   -wi-a--- 12.00g</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>lvdisplay</userinput>\n"
"<computeroutput># </computeroutput><userinput>lvcreate -n lv_files -L 5G vg_critical</userinput>\n"
"<computeroutput>  Logical volume \"lv_files\" created\n"
"# </computeroutput><userinput>lvdisplay</userinput>\n"
"<computeroutput>  --- Logical volume ---\n"
"  LV Path                /dev/vg_critical/lv_files\n"
"  LV Name                lv_files\n"
"  VG Name                vg_critical\n"
"  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT\n"
"  LV Write Access        read/write\n"
"  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400\n"
"  LV Status              available\n"
"  # open                 0\n"
"  LV Size                5.00 GiB\n"
"  Current LE             1280\n"
"  Segments               2\n"
"  Allocation             inherit\n"
"  Read ahead sectors     auto\n"
"  - currently set to     256\n"
"  Block device           253:0\n"
"\n"
"# </computeroutput><userinput>lvcreate -n lv_base -L 1G vg_critical</userinput>\n"
"<computeroutput>  Logical volume \"lv_base\" created\n"
"# </computeroutput><userinput>lvcreate -n lv_backups -L 12G vg_normal</userinput>\n"
"<computeroutput>  Logical volume \"lv_backups\" created\n"
"# </computeroutput><userinput>lvdisplay -C</userinput>\n"
"<computeroutput>  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_base    vg_critical -wi-a---  1.00g                                           \n"
"  lv_files   vg_critical -wi-a---  5.00g                                           \n"
"  lv_backups vg_normal   -wi-a--- 12.00g</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "Two parameters are required when creating logical volumes; they must be passed to the <command>lvcreate</command> as options. The name of the LV to be created is specified with the <literal>-n</literal> option, and its size is generally given using the <literal>-L</literal> option. We also need to tell the command what VG to operate on, of course, hence the last parameter on the command line."
msgstr "LV を作成する場合、2 種類のパラメータが必要です。このため、必ず 2 種類のパラメータをオプションとして <command>lvcreate</command> に渡します。作成する LV の名前を <literal>-n</literal> オプションで指定し、サイズを <literal>-L</literal> オプションで指定します。また、操作対象の VG をコマンドに伝えることが必要です。これはもちろん最後のコマンドラインパラメータです。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "<emphasis>GOING FURTHER</emphasis> <command>lvcreate</command> options"
msgstr "<emphasis>GOING FURTHER</emphasis> <command>lvcreate</command> のオプション"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "The <command>lvcreate</command> command has several options to allow tweaking how the LV is created."
msgstr "<command>lvcreate</command> コマンドは複数のオプションを取り、作成する LV を微調整することが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "Let's first describe the <literal>-l</literal> option, with which the LV's size can be given as a number of blocks (as opposed to the “human” units we used above). These blocks (called PEs, <emphasis>physical extents</emphasis>, in LVM terms) are contiguous units of storage space in PVs, and they can't be split across LVs. When one wants to define storage space for an LV with some precision, for instance to use the full available space, the <literal>-l</literal> option will probably be preferred over <literal>-L</literal>."
msgstr "最初に <literal>-l</literal> オプションについて説明しましょう。<literal>-l</literal> オプションを使った場合 LV のサイズをブロック数 (上の例で用いた「人間にとって分かりやすい」単位ではありません) で指定することが可能です。ブロックとは (LVM の用語で PE すなわち<emphasis>物理エクステント</emphasis>と呼ばれています) PV 中のストレージ領域の連続した単位です。ブロックは LV 中に分散されています。ある LV 用のストレージ領域を正確に定義したい場合、たとえば利用できる領域のすべてを使いたい場合、<literal>-l</literal> オプションのほうが <literal>-L</literal> オプションよりも使いやすいでしょう。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "It's also possible to hint at the physical location of an LV, so that its extents are stored on a particular PV (while staying within the ones assigned to the VG, of course). Since we know that <filename>sdb</filename> is faster than <filename>sdf</filename>, we may want to store the <filename>lv_base</filename> there if we want to give an advantage to the database server compared to the file server. The command line becomes: <command>lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</command>. Note that this command can fail if the PV doesn't have enough free extents. In our example, we would probably have to create <filename>lv_base</filename> before <filename>lv_files</filename> to avoid this situation – or free up some space on <filename>sdb2</filename> with the <command>pvmove</command> command."
msgstr "LV の物理的な位置を示唆することも可能です。こうすることで、LV の PE は特定の PV 上 (もちろん、VG を構成する PV 上に限ります) に作成されます。<filename>sdb</filename> は <filename>sdf</filename> よりも高速なので、<filename>lv_base</filename> を <filename>sdb</filename> 上に作成することでファイルサーバよりもデータベースサーバが高速にアクセスできるようにするには、以下のコマンドラインを使います。すなわち <command>lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</command> です。指定した PV に十分な空き PE がない場合、このコマンドは失敗する可能性があります。今回の例でこのような失敗を防ぐには <filename>lv_files</filename> の前に <filename>lv_base</filename> を作成するか、<command>pvmove</command> コマンドを使って <filename>sdb2</filename> に多少の領域を空ける必要があるかもしれません。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "Logical volumes, once created, end up as block device files in <filename>/dev/mapper/</filename>:"
msgstr "LV が作成され、ブロックデバイスファイルとして <filename>/dev/mapper/</filename> に現れます。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/mapper</userinput>\n"
"<computeroutput>total 0\n"
"crw------- 1 root root 10, 236 Jun 10 16:52 control\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0\n"
"lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2\n"
"# </computeroutput><userinput>ls -l /dev/dm-*</userinput>\n"
"<computeroutput>brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0\n"
"brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1\n"
"brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/mapper</userinput>\n"
"<computeroutput>合計 0\n"
"crw------- 1 root root 10, 236  6月 10 16:52 control\n"
"lrwxrwxrwx 1 root root       7  6月 10 17:05 vg_critical-lv_base -&gt; ../dm-1\n"
"lrwxrwxrwx 1 root root       7  6月 10 17:05 vg_critical-lv_files -&gt; ../dm-0\n"
"lrwxrwxrwx 1 root root       7  6月 10 17:05 vg_normal-lv_backups -&gt; ../dm-2\n"
"# </computeroutput><userinput>ls -l /dev/dm-*</userinput>\n"
"<computeroutput>brw-rw---T 1 root disk 253, 0  6月 10 17:05 /dev/dm-0\n"
"brw-rw---- 1 root disk 253, 1  6月 10 17:05 /dev/dm-1\n"
"brw-rw---- 1 root disk 253, 2  6月 10 17:05 /dev/dm-2\n"
"</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>NOTE</emphasis> Autodetecting LVM volumes"
msgstr "<emphasis>NOTE</emphasis> LVM ボリュームの自動検出"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
# Ref: $ man 8 vgchange;
msgid "When the computer boots, the <filename>lvm2-activation</filename> systemd service unit executes <command>vgchange -aay</command> to “activate” the volume groups: it scans the available devices; those that have been initialized as physical volumes for LVM are registered into the LVM subsystem, those that belong to volume groups are assembled, and the relevant logical volumes are started and made available. There is therefore no need to edit configuration files when creating or modifying LVM volumes."
msgstr "コンピュータの起動時に、<filename>lvm2-activation</filename> systemd サービスユニットは <command>vgchange -aay</command> を実行して VG を「始動」します。具体的に言えば、<filename>lvm2-activation</filename> systemd サービスユニットは利用できるデバイスを探します。そして LVM サブシステムに LVM 用の PV として初期化されたデバイスが登録され、PV から構成される VG が開始され、VG から分割された LV が開始され、LV が利用できるようになります。このため、LVM ボリュームを作成したり変更する際に設定ファイルを編集する必要はありません。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "Note, however, that the layout of the LVM elements (physical and logical volumes, and volume groups) is backed up in <filename>/etc/lvm/backup</filename>, which can be useful in case of a problem (or just to sneak a peek under the hood)."
msgstr "しかしながら、LVM 要素 (PV、LV、GV) の配置図は <filename>/etc/lvm/backup</filename> にバックアップされ、問題が起きた時 (見えないところで何が行われているかを確認したい時) に有益です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "To make things easier, convenience symbolic links are also created in directories matching the VGs:"
msgstr "ブロックデバイスファイルを分かり易くするために、VG に対応するディレクトリの中に便利なシンボリックリンクが作成されます。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/vg_critical</userinput>\n"
"<computeroutput>total 0\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0\n"
"# </computeroutput><userinput>ls -l /dev/vg_normal</userinput>\n"
"<computeroutput>total 0\n"
"lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>ls -l /dev/vg_critical</userinput>\n"
"<computeroutput>合計 0\n"
"lrwxrwxrwx 1 root root 7  6月 10 17:05 lv_base -&gt; ../dm-1\n"
"lrwxrwxrwx 1 root root 7  6月 10 17:05 lv_files -&gt; ../dm-0\n"
"# </computeroutput><userinput>ls -l /dev/vg_normal</userinput>\n"
"<computeroutput>合計 0\n"
"lrwxrwxrwx 1 root root 7  6月 10 17:05 lv_backups -&gt; ../dm-2</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The LVs can then be used exactly like standard partitions:"
msgstr "LV は標準的なパーティションと全く同様に取り扱われます。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups</userinput>\n"
"<computeroutput>mke2fs 1.42.12 (29-Aug-2014)\n"
"Creating filesystem with 3145728 4k blocks and 786432 inodes\n"
"Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d\n"
"[...]\n"
"Creating journal (32768 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"# </computeroutput><userinput>mkdir /srv/backups</userinput>\n"
"<computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups</userinput>\n"
"<computeroutput># </computeroutput><userinput>df -h /srv/backups</userinput>\n"
"<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_normal-lv_backups   12G   30M   12G   1% /srv/backups\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>cat /etc/fstab</userinput>\n"
"<computeroutput>[...]\n"
"/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2\n"
"/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2\n"
"/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mkfs.ext4 /dev/vg_normal/lv_backups</userinput>\n"
"<computeroutput>mke2fs 1.42.12 (29-Aug-2014)\n"
"Creating filesystem with 3145728 4k blocks and 786432 inodes\n"
"Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d\n"
"[...]\n"
"Creating journal (32768 blocks): done\n"
"Writing superblocks and filesystem accounting information: done \n"
"# </computeroutput><userinput>mkdir /srv/backups</userinput>\n"
"<computeroutput># </computeroutput><userinput>mount /dev/vg_normal/lv_backups /srv/backups</userinput>\n"
"<computeroutput># </computeroutput><userinput>df -h /srv/backups</userinput>\n"
"<computeroutput>ファイルシス                     サイズ  使用  残り 使用% マウント位置\n"
"/dev/mapper/vg_normal-lv_backups    12G   30M   12G    1% /srv/backups\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>cat /etc/fstab</userinput>\n"
"<computeroutput>[...]\n"
"/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2\n"
"/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2\n"
"/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "From the applications' point of view, the myriad small partitions have now been abstracted into one large 12 GB volume, with a friendlier name."
msgstr "アプリケーションにしてみれば、無数の小さなパーティションがわかり易い名前を持つ 1 つの大きな 12 GB のボリュームにまとめられたことになります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "LVM Over Time"
msgstr "経時変化に伴う LVM の利便性"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Even though the ability to aggregate partitions or physical disks is convenient, this is not the main advantage brought by LVM. The flexibility it brings is especially noticed as time passes, when needs evolve. In our example, let's assume that new large files must be stored, and that the LV dedicated to the file server is too small to contain them. Since we haven't used the whole space available in <filename>vg_critical</filename>, we can grow <filename>lv_files</filename>. For that purpose, we'll use the <command>lvresize</command> command, then <command>resize2fs</command> to adapt the filesystem accordingly:"
msgstr "LVM のパーティションや物理ディスクを統合する機能は便利ですが、これは LVM のもたらす主たる利点ではありません。時間経過に伴い LVM のもたらす柔軟性が特に重要になる時とは LV のサイズを増加させる必要が生じた時でしょう。ここまでの例を使い、LV に新たに巨大なファイルを保存したいけれども、ファイルサーバ用の LV はこの巨大なファイルを保存するには狭すぎると仮定しましょう。<filename>vg_critical</filename> から分割できる全領域はまだ使い切られていないので、<filename>lv_files</filename> のサイズを増やすことが可能です。LV のサイズを増やすために <command>lvresize</command> コマンドを使い、LV のサイズの変化にファイルシステムを対応させるために <command>resize2fs</command> を使います。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>df -h /srv/files/</userinput>\n"
"<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_files  5.0G  4.6G  146M  97% /srv/files\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
"<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_files vg_critical -wi-ao-- 5.00g\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
"  vg_critical   2   2   0 wz--n- 8.09g 2.09g\n"
"# </computeroutput><userinput>lvresize -L 7G vg_critical/lv_files</userinput>\n"
"<computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).\n"
"  Logical volume lv_files successfully resized\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
"<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_files vg_critical -wi-ao-- 7.00g\n"
"# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files</userinput>\n"
"<computeroutput>resize2fs 1.42.12 (29-Aug-2014)\n"
"Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required\n"
"old_desc_blocks = 1, new_desc_blocks = 1\n"
"The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.\n"
"\n"
"# </computeroutput><userinput>df -h /srv/files/</userinput>\n"
"<computeroutput>Filesystem                        Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_files  6.9G  4.6G  2.1G  70% /srv/files</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>df -h /srv/files/</userinput>\n"
"<computeroutput>ファイルシス                     サイズ  使用  残り 使用% マウント位置\n"
"/dev/mapper/vg_critical-lv_files   5.0G  4.6G  146M   97% /srv/files\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
"<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_files vg_critical -wi-ao-- 5.00g\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
"  vg_critical   2   2   0 wz--n- 8.09g 2.09g\n"
"# </computeroutput><userinput>lvresize -L 7G vg_critical/lv_files</userinput>\n"
"<computeroutput>  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).\n"
"  Logical volume lv_files successfully resized\n"
"# </computeroutput><userinput>lvdisplay -C vg_critical/lv_files</userinput>\n"
"<computeroutput>  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert\n"
"  lv_files vg_critical -wi-ao-- 7.00g\n"
"# </computeroutput><userinput>resize2fs /dev/vg_critical/lv_files</userinput>\n"
"<computeroutput>resize2fs 1.42.12 (29-Aug-2014)\n"
"Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required\n"
"old_desc_blocks = 1, new_desc_blocks = 1\n"
"The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.\n"
"\n"
"# </computeroutput><userinput>df -h /srv/files/</userinput>\n"
"<computeroutput>ファイルシス                     サイズ  使用  残り 使用% マウント位置\n"
"/dev/mapper/vg_critical-lv_files   6.9G  4.6G  2.1G   70% /srv/files</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>CAUTION</emphasis> Resizing filesystems"
msgstr "<emphasis>CAUTION</emphasis> ファイルシステムのサイズ変更"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Not all filesystems can be resized online; resizing a volume can therefore require unmounting the filesystem first and remounting it afterwards. Of course, if one wants to shrink the space allocated to an LV, the filesystem must be shrunk first; the order is reversed when the resizing goes in the other direction: the logical volume must be grown before the filesystem on it. It's rather straightforward, since at no time must the filesystem size be larger than the block device where it resides (whether that device is a physical partition or a logical volume)."
msgstr "すべてのファイルシステムがオンラインでサイズを変更できるわけではありません。このため、ボリュームのサイズ変更前にファイルシステムをアンマウントし、ボリュームのサイズ変更後に再マウントしなければいけません。もちろん、ボリュームのサイズを小さくする場合、ボリューム上のファイルシステムのサイズを小さくした後にボリュームのサイズを小さくしなければいけません。ボリュームのサイズを大きくする場合、ボリュームのサイズを大きくした後にボリューム上のファイルシステムを大きくしなければいけません。これはかなりわかり易いです。なぜなら、ブロックデバイス上に存在するファイルシステムのサイズをブロックデバイスよりも大きくすることは絶対に不可能だからです (この原則はボリュームが物理パーティションか LV かに依存しません)。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The ext3, ext4 and xfs filesystems can be grown online, without unmounting; shrinking requires an unmount. The reiserfs filesystem allows online resizing in both directions. The venerable ext2 allows neither, and always requires unmounting."
msgstr "ext3、ext4、xfs ファイルシステムはオンラインでサイズを増加させることすなわちアンマウントすることなくサイズを増加させることが可能です。しかし、サイズを減少させる場合はアンマウントを必要とします。reiserfs はオンラインでサイズを増加および減少することが可能です。ext2 は増加も減少も可能ですが、アンマウントを必要とします。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "We could proceed in a similar fashion to extend the volume hosting the database, only we've reached the VG's available space limit:"
msgstr "同様の方法でデータベースをホストしている <filename>lv_base</filename> のサイズを増加させます。以下の通り <filename>lv_base</filename> の分割元である <filename>vg_critical</filename> から分割できる領域は既にほぼ使い切った状態になっています。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>df -h /srv/base/</userinput>\n"
"<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_base 1008M  854M  104M  90% /srv/base\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree \n"
"  vg_critical   2   2   0 wz--n- 8.09g 92.00m</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>df -h /srv/base/</userinput>\n"
"<computeroutput>ファイルシス                    サイズ  使用  残り 使用% マウント位置\n"
"/dev/mapper/vg_critical-lv_base  1008M  854M  104M   90% /srv/base\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree \n"
"  vg_critical   2   2   0 wz--n- 8.09g 92.00m</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "No matter, since LVM allows adding physical volumes to existing volume groups. For instance, maybe we've noticed that the <filename>sdb1</filename> partition, which was so far used outside of LVM, only contained archives that could be moved to <filename>lv_backups</filename>. We can now recycle it and integrate it to the volume group, and thereby reclaim some available space. This is the purpose of the <command>vgextend</command> command. Of course, the partition must be prepared as a physical volume beforehand. Once the VG has been extended, we can use similar commands as previously to grow the logical volume then the filesystem:"
msgstr "でもご安心ください。LVM を使っていれば新しい PV を既存の VG を構成する PV の 1 つとして追加することが可能です。たとえば、今までは LVM の外で管理されていた <filename>sdb1</filename> パーティションには、<filename>lv_backups</filename> に移動しても問題のないアーカイブだけが含まれていた点に気が付いたとしましょう。このため、<filename>sdb1</filename> パーティションを <filename>vg_critical</filename> を構成する PV の 1 つとして再利用することが可能です。こうすることで、<filename>vg_critical</filename> から <filename>lv_base</filename> に分割される領域のサイズを増やすことが可能です。これが <command>vgextend</command> コマンドの目的です。もちろん、事前に <filename>sdb1</filename> パーティションを PV として準備しなければいけません。<filename>vg_critical</filename> を拡張したら、先と同様のコマンドを使って先に <filename>lv_base</filename> のサイズを増加させ、その後に <filename>lv_base</filename> 上のファイルシステムのサイズを増加させます。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb1</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdb1\" successfully created\n"
"# </computeroutput><userinput>vgextend vg_critical /dev/sdb1</userinput>\n"
"<computeroutput>  Volume group \"vg_critical\" successfully extended\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
"  vg_critical   3   2   0 wz--n- 9.09g 1.09g\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>df -h /srv/base/</userinput>\n"
"<computeroutput>Filesystem                       Size  Used Avail Use% Mounted on\n"
"/dev/mapper/vg_critical-lv_base  2.0G  854M  1.1G  45% /srv/base</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>pvcreate /dev/sdb1</userinput>\n"
"<computeroutput>  Physical volume \"/dev/sdb1\" successfully created\n"
"# </computeroutput><userinput>vgextend vg_critical /dev/sdb1</userinput>\n"
"<computeroutput>  Volume group \"vg_critical\" successfully extended\n"
"# </computeroutput><userinput>vgdisplay -C vg_critical</userinput>\n"
"<computeroutput>  VG          #PV #LV #SN Attr   VSize VFree\n"
"  vg_critical   3   2   0 wz--n- 9.09g 1.09g\n"
"# </computeroutput><userinput>[...]</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>df -h /srv/base/</userinput>\n"
"<computeroutput>ファイルシス                    サイズ  使用  残り 使用% マウント位置\n"
"/dev/mapper/vg_critical-lv_base   2.0G  854M  1.1G   45% /srv/base</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>GOING FURTHER</emphasis> Advanced LVM"
msgstr "<emphasis>GOING FURTHER</emphasis> LVM の上級活用"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "LVM also caters for more advanced uses, where many details can be specified by hand. For instance, an administrator can tweak the size of the blocks that make up physical and logical volumes, as well as their physical layout. It is also possible to move blocks across PVs, for instance to fine-tune performance or, in a more mundane way, to free a PV when one needs to extract the corresponding physical disk from the VG (whether to affect it to another VG or to remove it from LVM altogether). The manual pages describing the commands are generally clear and detailed. A good entry point is the <citerefentry><refentrytitle>lvm</refentrytitle> <manvolnum>8</manvolnum></citerefentry> manual page."
msgstr "LVM にはさらに上級の使い方があり、多くの設定事項を手作業で指定することが可能です。たとえば、管理者は PV と LV のブロックサイズおよびボリュームの物理的な配置を微調整することが可能です。また、ブロックを PV 間で移動することも可能です。これは、たとえば性能を微調整したり、よりありふれたケースではある物理ディスクに対応する PV を VG の構成要素から外したりするため (PV を別の VG に移動したり、完全に LVM から取り外したりするため) に行われます。コマンドを説明しているマニュアルページは基本的に明快で詳細です。手始めに、<citerefentry><refentrytitle>lvm</refentrytitle> <manvolnum>8</manvolnum></citerefentry> マニュアルページを参照することをお勧めします。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "RAID or LVM?"
msgstr "RAID それとも LVM?"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "RAID and LVM both bring indisputable advantages as soon as one leaves the simple case of a desktop computer with a single hard disk where the usage pattern doesn't change over time. However, RAID and LVM go in two different directions, with diverging goals, and it is legitimate to wonder which one should be adopted. The most appropriate answer will of course depend on current and foreseeable requirements."
msgstr "1 番目の利用形態は用途が時間的に変化しない 1 台のハードディスクを備えたデスクトップコンピュータのような単純な利用形態です。この場合 RAID と LVM はどちらも疑う余地のない利点をもたらします。しかしながら、RAID と LVM は目標を分岐させて別々の道を歩んでいます。どちらを使うべきか悩むのは間違っていることではありません。最も適切な答えはもちろん現在の要求と将来に予測される要求に依存します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "There are a few simple cases where the question doesn't really arise. If the requirement is to safeguard data against hardware failures, then obviously RAID will be set up on a redundant array of disks, since LVM doesn't really address this problem. If, on the other hand, the need is for a flexible storage scheme where the volumes are made independent of the physical layout of the disks, RAID doesn't help much and LVM will be the natural choice."
msgstr "いくつかの状況では、疑問の余地がないくらい簡単に答えを出すことが可能です。2 番目の利用形態はハードウェア障害からデータを保護することが求められる利用形態です。この場合、ディスクの冗長性アレイ上に RAID をセットアップするのは明らかです。なぜなら LVM はこの種の問題への対応策を全く用意していないからです。逆に、柔軟なストレージ計画が必要でディスクの物理的な配置に依存せずにボリュームを構成したい場合、RAID はあまり役に立たず LVM を選ぶのが自然です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>NOTE</emphasis> If performance matters…"
msgstr "<emphasis>NOTE</emphasis> 性能が重要な場合"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "If input/output speed is of the essence, especially in terms of access times, using LVM and/or RAID in one of the many combinations may have some impact on performances, and this may influence decisions as to which to pick. However, these differences in performance are really minor, and will only be measurable in a few use cases. If performance matters, the best gain to be obtained would be to use non-rotating storage media (<indexterm><primary>SSD</primary></indexterm><emphasis>solid-state drives</emphasis> or SSDs); their cost per megabyte is higher than that of standard hard disk drives, and their capacity is usually smaller, but they provide excellent performance for random accesses. If the usage pattern includes many input/output operations scattered all around the filesystem, for instance for databases where complex queries are routinely being run, then the advantage of running them on an SSD far outweigh whatever could be gained by picking LVM over RAID or the reverse. In these situations, the choice should be determined by other considerations than pure speed, since the performance aspect is most easily handled by using SSDs."
msgstr "特にアクセス速度という意味の入出力速度が最重要な場合を考えてみましょう。LVM および RAID はどのような組み合わせで使っても性能にある程度の影響をおよぼします。このため、どの組み合わせを採用するかが議題に挙げられるかもしれません。しかしながら、どんな組み合わせを使っても性能差は極めて少なく、この程度の性能差が無視できない場合は極めて少ないと言えるでしょう。性能が重要な場合、実現できる最良の改善方針は非回転ストレージメディア (<indexterm><primary>SSD</primary></indexterm><emphasis>ソリッドステートドライブ</emphasis>すなわち SSD) を使うことです。SSD のメガバイト当たりの費用は標準的なハードディスクドライブよりも高価で、SSD の容量は通常小さいですが、SSD はランダムアクセスで素晴らしい性能を発揮します。ファイルシステムに広く分散された位置から数多くの入出力を行うような場合 (たとえば複雑な問い合わせが定期的に実行されるデータベースの場合)、SSD 上にデータベースを置くほうが RAID over LVM または LVM over RAID 上にデータベースを置くよりも良好な性能が手に入ります。このような場合、純粋な速度だけでなく他の要素も検討した上で採用の可否を決定するべきです。なぜなら、性能が必要な場合に SSD を採用することは最も安直な解決策だからです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The third notable use case is when one just wants to aggregate two disks into one volume, either for performance reasons or to have a single filesystem that is larger than any of the available disks. This case can be addressed both by a RAID-0 (or even linear-RAID) and by an LVM volume. When in this situation, and barring extra constraints (for instance, keeping in line with the rest of the computers if they only use RAID), the configuration of choice will often be LVM. The initial set up is barely more complex, and that slight increase in complexity more than makes up for the extra flexibility that LVM brings if the requirements change or if new disks need to be added."
msgstr "3 番目に注目すべき利用形態は単に 2 つのディスクを 1 つのボリュームにまとめるような利用形態です。性能が欲しかったり、利用できるディスクのどれよりも大きな単一のファイルシステムにしたい場合にこの利用形態が採用されます。この場合、RAID-0 (またはリニア RAID) か LVM ボリュームを使って対処できます。この状況では、追加的な制約事項 (たとえば、他のコンピュータが RAID だけを使っている場合に RAID を使わなければいけないなどの制約事項) がなければ、通常 LVM を選択すると良いでしょう。LVM の最初のセットアップは RAID に比べて複雑ですが、LVM は複雑度を少し増加させるだけで要求が変った場合や新しいディスクを追加する必要ができた場合に対処可能な追加的な柔軟性を大きく上昇させます。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Ref: https://wiki.archlinux.org/index.php/Software_RAID_and_LVM;
msgid "Then of course, there is the really interesting use case, where the storage system needs to be made both resistant to hardware failure and flexible when it comes to volume allocation. Neither RAID nor LVM can address both requirements on their own; no matter, this is where we use both at the same time — or rather, one on top of the other. The scheme that has all but become a standard since RAID and LVM have reached maturity is to ensure data redundancy first by grouping disks in a small number of large RAID arrays, and to use these RAID arrays as LVM physical volumes; logical partitions will then be carved from these LVs for filesystems. The selling point of this setup is that when a disk fails, only a small number of RAID arrays will need to be reconstructed, thereby limiting the time spent by the administrator for recovery."
msgstr "そしてもちろん、最後の本当に興味深い利用形態はストレージシステムにハードウェア障害に対する耐性を持たせさらにボリューム分割に対する柔軟性を持たせる必要がある場合の利用形態です。RAID と LVM のどちらも片方だけで両方の要求を満足させることは不可能です。しかし心配ありません。この要求を満足させるには RAID と LVM の両方を同時に使用する方針、正確に言えば一方の上に他方を構成する方針を採用すれば良いでのです。RAID と LVM の高い成熟度のおかげでほぼ標準になりつつある方針に従うならば、最初にディスクを少数の大きな RAID アレイにグループ分けすることでデータの冗長性を確保します。さらにそれらの RAID アレイを LVM の PV として使います。そして、ファイルシステム用の VG から分割された LV を論理パーティションとして使います。この標準的な方針の優れた点は、ディスク障害が起きた場合に再構築しなければいけない RAID アレイの数が少ない点です。このため、管理者は復旧に必要な時間を減らすことが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Let's take a concrete example: the public relations department at Falcot Corp needs a workstation for video editing, but the department's budget doesn't allow investing in high-end hardware from the bottom up. A decision is made to favor the hardware that is specific to the graphic nature of the work (monitor and video card), and to stay with generic hardware for storage. However, as is widely known, digital video does have some particular requirements for its storage: the amount of data to store is large, and the throughput rate for reading and writing this data is important for the overall system performance (more than typical access time, for instance). These constraints need to be fulfilled with generic hardware, in this case two 300 GB SATA hard disk drives; the system data must also be made resistant to hardware failure, as well as some of the user data. Edited videoclips must indeed be safe, but video rushes pending editing are less critical, since they're still on the videotapes."
msgstr "ここで具体例を見てみましょう。たとえば Falcot Corp の広報課は動画編集用にワークステーションを必要としていますが、広報課の予算の都合上、最初から高性能のハードウェアに投資することは不可能です。このため、グラフィック性能を担うハードウェア (モニタとビデオカード) に大きな予算を割き、ストレージ用には一般的なハードウェアを使うことが決定されました。しかしながら、広く知られている通りデジタルビデオ用のストレージはある種の条件を必要とします。すなわち、保存されるデータのサイズが大きく、このデータを読み込みおよび書き込みする際の処理速度がシステム全体の性能にとって重要 (たとえば、平均的なアクセス時間よりも重要) という条件です。この条件を一般的なハードウェアを使って満足させる必要があります。今回の場合 2 台の SATA ハードディスクドライブを使います。さらに、システムデータと一部のユーザデータはハードウェア障害に対する耐性を持たせる必要があります。編集済みのビデオクリップを保護する必要はありますが、編集前のビデオ素材をそれほど気にする必要はありません。なぜなら、編集前のビデオ素材はまだビデオテープに残されているからです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "RAID-1 and LVM are combined to satisfy these constraints. The disks are attached to two different SATA controllers to optimize parallel access and reduce the risk of a simultaneous failure, and they therefore appear as <filename>sda</filename> and <filename>sdc</filename>. They are partitioned identically along the following scheme:"
msgstr "前述の条件を満足させるために RAID-1 と LVM を組み合わせます。ディスクの並行アクセスを最適化し、そして障害が同時に発生する危険性を減らすために、各ディスクは 2 つの異なる SATA コントローラに接続されています。このため、各ディスクは <filename>sda</filename> と <filename>sdc</filename> として現れます。どちらのディスクも以下に示したパーティショニング方針に従ってパーティショニングされます。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>fdisk -l /dev/sda</userinput>\n"
"<computeroutput>\n"
"Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors\n"
"Units: sectors of 1 * 512 = 512 bytes\n"
"Sector size (logical/physical): 512 bytes / 512 bytes\n"
"I/O size (minimum/optimal): 512 bytes / 512 bytes\n"
"Disklabel type: dos\n"
"Disk identifier: 0x00039a9f\n"
"\n"
"Device    Boot     Start       End   Sectors Size Id Type\n"
"/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect\n"
"/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris\n"
"/dev/sda3        4000185 586099395 582099210 298G 5  Extended\n"
"/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect\n"
"/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect\n"
"/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>fdisk -l /dev/sda</userinput>\n"
"<computeroutput>\n"
"Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors\n"
"Units: sectors of 1 * 512 = 512 bytes\n"
"Sector size (logical/physical): 512 bytes / 512 bytes\n"
"I/O size (minimum/optimal): 512 bytes / 512 bytes\n"
"Disklabel type: dos\n"
"Disk identifier: 0x00039a9f\n"
"\n"
"Device    Boot     Start       End   Sectors Size Id Type\n"
"/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect\n"
"/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris\n"
"/dev/sda3        4000185 586099395 582099210 298G 5  Extended\n"
"/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect\n"
"/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect\n"
"/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The first partitions of both disks (about 1 GB) are assembled into a RAID-1 volume, <filename>md0</filename>. This mirror is directly used to store the root filesystem."
msgstr "<filename>sda1</filename> と <filename>sdc1</filename> パーティション (約 1 GB) から RAID-1 ボリューム <filename>md0</filename> を構成します。<filename>md0</filename> はルートファイルシステムを保存するために直接的に使われます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The <filename>sda2</filename> and <filename>sdc2</filename> partitions are used as swap partitions, providing a total 2 GB of swap space. With 1 GB of RAM, the workstation has a comfortable amount of available memory."
msgstr "<filename>sda2</filename> と <filename>sdc2</filename> パーティションから swap パーティションを作成します。スワップ領域のサイズは合計で 2 GB になります。RAM のサイズ 1 GB と合わせれば、ワークステーションで利用できるメモリサイズは十分な量と言えます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The <filename>sda5</filename> and <filename>sdc5</filename> partitions, as well as <filename>sda6</filename> and <filename>sdc6</filename>, are assembled into two new RAID-1 volumes of about 100 GB each, <filename>md1</filename> and <filename>md2</filename>. Both these mirrors are initialized as physical volumes for LVM, and assigned to the <filename>vg_raid</filename> volume group. This VG thus contains about 200 GB of safe space."
msgstr "<filename>sda5</filename> と <filename>sdc5</filename> パーティションおよび <filename>sda6</filename> と <filename>sdc6</filename> パーティションからそれぞれ約 100 GB の 2 つの新しい RAID-1 ボリューム <filename>md1</filename> と <filename>md2</filename> を構成します。<filename>md1</filename> と <filename>md2</filename> は LVM の PV として初期化され、これらの PV から <filename>vg_raid</filename> VG を構成します。<filename>vg_raid</filename> は約 200 GB の安全な領域になります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The remaining partitions, <filename>sda7</filename> and <filename>sdc7</filename>, are directly used as physical volumes, and assigned to another VG called <filename>vg_bulk</filename>, which therefore ends up with roughly 200 GB of space."
msgstr "残りのパーティションである <filename>sda7</filename> と <filename>sdc7</filename> はそのまま LVM の PV として初期化され、これらの PV から <filename>vg_bulk</filename> VG を構成します。<filename>vg_bulk</filename> はおよそ 200 GB の領域になります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Once the VGs are created, they can be partitioned in a very flexible way. One must keep in mind that LVs created in <filename>vg_raid</filename> will be preserved even if one of the disks fails, which will not be the case for LVs created in <filename>vg_bulk</filename>; on the other hand, the latter will be allocated in parallel on both disks, which allows higher read or write speeds for large files."
msgstr "VG を作成したら、VG をとても柔軟な方法で LV に分割することが可能です。<filename>vg_raid</filename> から分割された LV は 1 台のディスク障害に対して耐性を持ちますが、<filename>vg_bulk</filename> から分割された LV はディスク障害に対する耐性を持たない点を忘れないでください。逆に、<filename>vg_bulk</filename> は両方のディスクにわたって割り当てられるので、<filename>vg_bulk</filename> から分割された LV に保存された巨大なファイルの読み書き速度は高速化されるでしょう。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "We will therefore create the <filename>lv_usr</filename>, <filename>lv_var</filename> and <filename>lv_home</filename> LVs on <filename>vg_raid</filename>, to host the matching filesystems; another large LV, <filename>lv_movies</filename>, will be used to host the definitive versions of movies after editing. The other VG will be split into a large <filename>lv_rushes</filename>, for data straight out of the digital video cameras, and a <filename>lv_tmp</filename> for temporary files. The location of the work area is a less straightforward choice to make: while good performance is needed for that volume, is it worth risking losing work if a disk fails during an editing session? Depending on the answer to that question, the relevant LV will be created on one VG or the other."
msgstr "<filename>vg_raid</filename> から <filename>lv_usr</filename>、<filename>lv_var</filename>、<filename>lv_home</filename> を分割し、各 LV に応じたファイルシステムをホストさせます。さらに、<filename>vg_raid</filename> からもう一つの大きな LV である <filename>lv_movies</filename> を分割し、<filename>lv_movies</filename> に編集済みの最終版の映像をホストさせます。また、<filename>vg_bulk</filename> からデジタルビデオカメラから取り出したデータ用の大きな <filename>lv_rushes</filename> と一時ファイル用の <filename>lv_tmp</filename> を分割します。<filename>vg_raid</filename> と <filename>vg_bulk</filename> のどちらから作業領域用の LV を分割するかは簡単に決められるものではありません。つまり、作業領域用の LV は良い性能を必要としますが、編集作業中にディスク障害が起きた場合に作業内容を保護する必要があるでしょうか? この質問の回答次第で、作業領域用の LV を <filename>vg_raid</filename> か <filename>vg_bulk</filename> のどちらの VG に作成するかが決まります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "We now have both some redundancy for important data and much flexibility in how the available space is split across the applications. Should new software be installed later on (for editing audio clips, for instance), the LV hosting <filename>/usr/</filename> can be grown painlessly."
msgstr "これで、重要なデータ用に多少の冗長性と、利用できる領域が用途ごとにどのように分割されるかに関する大きな柔軟性が確保されました。後から (たとえば音声クリップの編集用に) 新しいソフトウェアをインストールする場合も、<filename>/usr/</filename> をホストしている LV のサイズを簡単に増加することが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>NOTE</emphasis> Why three RAID-1 volumes?"
msgstr "<emphasis>NOTE</emphasis> なぜ 3 種類の RAID-1 ボリュームが必要なのでしょうか?"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "We could have set up one RAID-1 volume only, to serve as a physical volume for <filename>vg_raid</filename>. Why create three of them, then?"
msgstr "RAID-1 ボリュームを 1 つだけ作成し、作成した PV から <filename>vg_raid</filename> を構成し、<filename>vg_raid</filename> から保護したい内容用の LV を分割することも可能でした。それにも関わらず、なぜ 3 種類の RAID-1 ボリュームを作成したのでしょうか?"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The rationale for the first split (<filename>md0</filename> vs. the others) is about data safety: data written to both elements of a RAID-1 mirror are exactly the same, and it is therefore possible to bypass the RAID layer and mount one of the disks directly. In case of a kernel bug, for instance, or if the LVM metadata become corrupted, it is still possible to boot a minimal system to access critical data such as the layout of disks in the RAID and LVM volumes; the metadata can then be reconstructed and the files can be accessed again, so that the system can be brought back to its nominal state."
msgstr "最初の分割 (<filename>md0</filename> とその他) の根本的理由はデータの安全性を考慮したためです。つまり RAID-1 ミラーを構成する要素に書き込まれるデータは要素同士で全く同じだからです。そのため RAID 層を迂回し、RAID-1 ミラーを構成する 1 台のディスクだけを直接マウントすることが可能です。すなわち、カーネルにバグがあったり LVM メタ情報が破壊されたりした場合でも、RAID と LVM ボリュームに含まれるディスクの配置などの重要なデータにアクセスするために最小限のシステムを起動することが可能ということです。そして、このメタ情報を再構成したりファイルにアクセスしたりすることが可能です。こうすることで、システムを正常状態に戻すことが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The rationale for the second split (<filename>md1</filename> vs. <filename>md2</filename>) is less clear-cut, and more related to acknowledging that the future is uncertain. When the workstation is first assembled, the exact storage requirements are not necessarily known with perfect precision; they can also evolve over time. In our case, we can't know in advance the actual storage space requirements for video rushes and complete video clips. If one particular clip needs a very large amount of rushes, and the VG dedicated to redundant data is less than halfway full, we can re-use some of its unneeded space. We can remove one of the physical volumes, say <filename>md2</filename>, from <filename>vg_raid</filename> and either assign it to <filename>vg_bulk</filename> directly (if the expected duration of the operation is short enough that we can live with the temporary drop in performance), or undo the RAID setup on <filename>md2</filename> and integrate its components <filename>sda6</filename> and <filename>sdc6</filename> into the bulk VG (which grows by 200 GB instead of 100 GB); the <filename>lv_rushes</filename> logical volume can then be grown according to requirements."
msgstr "2 番目の分割 (<filename>md1</filename> と <filename>md2</filename>) の根本的理由は明確というわけではありませんが、将来の不明確さを認めていることに関連します。動画編集用ワークステーションを最初に組み上げる時点で、要求される正確なストレージサイズを完全な精度で知ることは不可能です。それどころか、ストレージサイズは時間経過に従い増加するかもしれません。今回の場合、ビデオ素材用の <filename>lv_rushes</filename> と編集済みビデオクリップ用の <filename>lv_movies</filename> に対して実際に要求されるストレージサイズを事前に知ることはできません。とても大きな量の素材を必要とするクリップを <filename>lv_rushes</filename> に保存する必要があり、さらに冗長性データ用 VG である <filename>vg_raid</filename> のまだ半分以上が未使用状態ならば、<filename>vg_raid</filename> から未使用領域を再利用することが可能です。具体的には <filename>vg_raid</filename> の構成要素から片方の PV (たとえば <filename>md2</filename>) を削除し、<filename>md2</filename> を <filename>vg_bulk</filename> を構成する PV として初期化するか (予想される作業時間が一時的な性能の低下を許容できる程度に十分短い場合に限ります)、<filename>md2</filename> の RAID-1 セットアップを破棄してその構成要素である <filename>sda6</filename> と <filename>sdc6</filename> を <filename>vg_bulk</filename> を構成する PV として初期化する (この場合 100 GB ではなく 200 GB の増加になります) ことが可能です。そして <filename>lv_rushes</filename> を必要に応じて増加させることが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary>virtualization</primary>"
msgstr "<primary>仮想化</primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Virtualization is one of the most major advances in the recent years of computing. The term covers various abstractions and techniques simulating virtual computers with a variable degree of independence on the actual hardware. One physical server can then host several systems working at the same time and in isolation. Applications are many, and often derive from this isolation: test environments with varying configurations for instance, or separation of hosted services across different virtual machines for security."
msgstr "仮想化は最近のコンピューティングにおける最も大きな進歩の 1 つです。仮想化という用語は、実際のハードウェアに対するさまざまな独立性の度合いを持つ仮想コンピュータを模倣するさまざまな抽象化と技術を指します。1 台の物理的なサーバが同時かつ隔離された状態で動く複数のシステムをホストすることが可能です。仮想化アプリケーションは数多く存在し、隔離された仮想システムを使うことができます。たとえば、さまざまに設定されたテスト環境を作ったり、安全性を確保する目的で異なる仮想マシン間でホストされたサービスを分離したりすることが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "There are multiple virtualization solutions, each with its own pros and cons. This book will focus on Xen, LXC, and KVM, but other noteworthy implementations include the following:"
msgstr "複数の仮想化ソリューションが存在し、それぞれが利点と欠点を持っています。本書では Xen、LXC、KVM に注目しますが、他にも以下のような注目すべき実装が存在します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary><emphasis>VMWare</emphasis></primary>"
msgstr "<primary><emphasis>VMWare</emphasis></primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary><emphasis>Bochs</emphasis></primary>"
msgstr "<primary><emphasis>Bochs</emphasis></primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary><emphasis>QEMU</emphasis></primary>"
msgstr "<primary><emphasis>QEMU</emphasis></primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary><emphasis>VirtualBox</emphasis></primary>"
msgstr "<primary><emphasis>VirtualBox</emphasis></primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary><emphasis>KVM</emphasis></primary>"
msgstr "<primary><emphasis>KVM</emphasis></primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary><emphasis>LXC</emphasis></primary>"
msgstr "<primary><emphasis>LXC</emphasis></primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "QEMU is a software emulator for a full computer; performances are far from the speed one could achieve running natively, but this allows running unmodified or experimental operating systems on the emulated hardware. It also allows emulating a different hardware architecture: for instance, an <emphasis>amd64</emphasis> system can emulate an <emphasis>arm</emphasis> computer. QEMU is free software. <ulink type=\"block\" url=\"http://www.qemu.org/\" />"
msgstr "QEMU は完全なコンピュータを模倣するソフトウェアエミュレータです。このため QEMU の性能はネイティブに実行した場合の速度には遠くおよびませんが、QEMU を使うことで修正されていなかったり実験的なオペレーティングシステムをエミュレートされたハードウェア上で実行することが可能です。さらに QEMU は異なるハードウェアアーキテクチャをエミュレートすることが可能です。たとえば、<emphasis>amd64</emphasis> システムで <emphasis>arm</emphasis> コンピュータをエミュレートすることが可能です。QEMU はフリーソフトウェアです。<ulink type=\"block\" url=\"http://www.qemu.org/\" />"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Bochs is another free virtual machine, but it only emulates the x86 architectures (i386 and amd64)."
msgstr "Bochs は自由な仮想マシンですが、x86 アーキテクチャ (i386 と amd64) だけをエミュレートすることが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "VMWare is a proprietary virtual machine; being one of the oldest out there, it is also one of the most widely-known. It works on principles similar to QEMU. VMWare proposes advanced features such as snapshotting a running virtual machine. <ulink type=\"block\" url=\"http://www.vmware.com/\" />"
msgstr "VMWare はプロプライエタリの仮想マシンです。VMWare はこの分野で最も古く、最も広く使われているソフトウェアの 1 つです。VMWare は QEMU とよく似た原理で動いています。VMWare には実行中の仮想マシンのスナップショットなどの高度な機能が含まれています。<ulink type=\"block\" url=\"http://www.vmware.com/\" />"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "VirtualBox is a virtual machine that is mostly free software (some extra components are available under a proprietary license). Unfortunately it is in Debian's “contrib” section because it includes some precompiled files that cannot be rebuilt without a proprietary compiler. While younger than VMWare and restricted to the i386 and amd64 architectures, it still includes some snapshotting and other interesting features. <ulink type=\"block\" url=\"http://www.virtualbox.org/\" />"
msgstr "VirtualBox はほぼ自由なソフトウェアの仮想マシンです (一部の追加的な構成要素はプロプライエタリライセンスの下で利用できます)。残念なことに VirtualBox は Debian の「contrib」セクションにあります。なぜなら VirtualBox にはいくつかのコンパイル済みファイルが含まれ、このファイルを再ビルドするにはプロプライエタリコンパイラが必要だからです。VirtualBox は VMWare よりも歴史が浅く、i386 と amd64 アーキテクチャだけをサポートします。しかしながら、VirtualBox はスナップショットやその他の興味深い機能を備えています。<ulink type=\"block\" url=\"http://www.virtualbox.org/\" />"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Xen <indexterm><primary>Xen</primary></indexterm> is a “paravirtualization” solution. It introduces a thin abstraction layer, called a “hypervisor”, between the hardware and the upper systems; this acts as a referee that controls access to hardware from the virtual machines. However, it only handles a few of the instructions, the rest is directly executed by the hardware on behalf of the systems. The main advantage is that performances are not degraded, and systems run close to native speed; the drawback is that the kernels of the operating systems one wishes to use on a Xen hypervisor need to be adapted to run on Xen."
msgstr "Xen<indexterm><primary>Xen</primary></indexterm> は「準仮想化」ソリューションです。Xen には薄い抽象化層が含まれ、この抽象化層は「ハイパーバイザ」と呼ばれ、ハードウェアとその上にあるシステムの間に位置します。さらにハイパーバイザは審判員として振る舞い、仮想マシンからハードウェアへのアクセスを制御します。しかしながら、Xen ハイパーバイザは命令のほんの一部だけを取り扱い、残りは Xen ハイパーバイザではなくハードウェアによって直接的に実行されます。こうすることによる主な有効性は性能が低下せず、システムがネイティブ速度に迫る性能を発揮するという点です。一方で欠点は Xen ハイパーバイザ上でオペレーティングシステムを実行するには実行されるオペレーティングシステムのカーネルを修正しなければいけないという点です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Let's spend some time on terms. The hypervisor is the lowest layer, that runs directly on the hardware, even below the kernel. This hypervisor can split the rest of the software across several <emphasis>domains</emphasis>, which can be seen as so many virtual machines. One of these domains (the first one that gets started) is known as <emphasis>dom0</emphasis>, and has a special role, since only this domain can control the hypervisor and the execution of other domains. These other domains are known as <emphasis>domU</emphasis>. In other words, and from a user point of view, the <emphasis>dom0</emphasis> matches the “host” of other virtualization systems, while a <emphasis>domU</emphasis> can be seen as a “guest”."
msgstr "用語の解説に少し時間を割きましょう。Xen ハイパーバイザはカーネルよりも下層の最も低い層に位置し、ハードウェア上で直接動きます。Xen ハイパーバイザは残りのソフトウェアをいくつかの<emphasis>ドメイン</emphasis>に分割することが可能で、<emphasis>ドメイン</emphasis>は多数の仮想マシンと考えられます。これらのドメインの 1 つ (最初に起動されたもの) は <emphasis>dom0</emphasis> と呼ばれ、特別な役割を担います。なぜなら、<emphasis>dom0</emphasis> だけが Xen ハイパーバイザを制御することが可能だからです。他のドメインは <emphasis>domU</emphasis> として知られています。ユーザ視点で言い換えれば、<emphasis>dom0</emphasis> は他の仮想システムにおける「ホスト」、これに対して <emphasis>domU</emphasis> は「ゲスト」になります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>CULTURE</emphasis> Xen and the various versions of Linux"
msgstr "<emphasis>CULTURE</emphasis> Xen と Linux のさまざまなバージョン"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: PTAL;
# Ref: https://lwn.net/Articles/194543/
msgid "Xen was initially developed as a set of patches that lived out of the official tree, and not integrated to the Linux kernel. At the same time, several upcoming virtualization systems (including KVM) required some generic virtualization-related functions to facilitate their integration, and the Linux kernel gained this set of functions (known as the <emphasis>paravirt_ops</emphasis> or <emphasis>pv_ops</emphasis> interface). Since the Xen patches were duplicating some of the functionality of this interface, they couldn't be accepted officially."
msgstr "当初 Xen は Linux 公式ツリーの外部パッチとして開発され、Linux カーネルに組み込まれていませんでした。これと同時期に、複数の次世代仮想化システム (KVM など) は Linux カーネルへの組み込みを簡単にするためにいくつかの包括的な仮想化関連関数を必要としており、さらに Linux カーネルが (<emphasis>paravirt_ops</emphasis> または <emphasis>pv_ops</emphasis> インターフェースとして知られる) 一連の仮想化関連関数を獲得しました。Xen のパッチはこのインターフェースのいくつかの機能を複製していたため、Xen のパッチは公式に受け入れられませんでした。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Xensource, the company behind Xen, therefore had to port Xen to this new framework, so that the Xen patches could be merged into the official Linux kernel. That meant a lot of code rewrite, and although Xensource soon had a working version based on the paravirt_ops interface, the patches were only progressively merged into the official kernel. The merge was completed in Linux 3.0. <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/XenParavirtOps\" />"
msgstr "このため、Xen を影で支える会社の Xensource は新しく Linux カーネルに取り込まれた仮想化関連関数を使って Xen を移植しなければいけませんでした。この移植作業により、Xen のパッチを公式の Linux カーネルに取り込むことが可能になりました。この移植作業は多くのコードを書き換えることを意味していました。Xensource はすぐに paravirt_ops インターフェースを使って評価版を作ったにも関わらず、Xen のパッチを公式カーネルにマージする作業はゆっくりと進みました。マージか完了したのは Linux 3.0 です。<ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/XenParavirtOps\" />"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Since <emphasis role=\"distribution\">Jessie</emphasis> is based on version 3.16 of the Linux kernel, the standard <emphasis role=\"pkg\">linux-image-686-pae</emphasis> and <emphasis role=\"pkg\">linux-image-amd64</emphasis> packages include the necessary code, and the distribution-specific patching that was required for <emphasis role=\"distribution\">Squeeze</emphasis> and earlier versions of Debian is no more. <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix\" />"
msgstr "<emphasis role=\"distribution\">Jessie</emphasis> は Linux カーネルのバージョン 3.16 に基づくため、標準的な <emphasis role=\"pkg\">linux-image-686-pae</emphasis> と <emphasis role=\"pkg\">linux-image-amd64</emphasis> パッケージには Xen を動作させるために必要なコードが含まれます。Debian の <emphasis role=\"distribution\">Squeeze</emphasis> およびそれ以前のバージョンで必要とされていたディストリビューションに特有のパッチ作業はもはや必要ありません。<ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix\" />"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>NOTE</emphasis> Architectures compatible with Xen"
msgstr "<emphasis>NOTE</emphasis> Xen 互換のアーキテクチャ"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Xen is currently only available for the i386, amd64, arm64 and armhf architectures."
msgstr "現在のところ、Xen を利用できるのは i386、amd64、arm64、armhf アーキテクチャだけです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>CULTURE</emphasis> Xen and non-Linux kernels"
msgstr "<emphasis>CULTURE</emphasis> Xen と非 Linux カーネル"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Xen requires modifications to all the operating systems one wants to run on it; not all kernels have the same level of maturity in this regard. Many are fully-functional, both as dom0 and domU: Linux 3.0 and later, NetBSD 4.0 and later, and OpenSolaris. Others only work as a domU. You can check the status of each operating system in the Xen wiki: <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen\" /> <ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/DomU_Support_for_Xen\" />"
msgstr "Xen 上でオペレーティングシステムを動作させるには、いかなるオペレーティングシステムであってもそれを修正する必要があります。さらに、すべてのオペレーティングシステムのカーネルが修正点に関して同じ程度の成熟度を持っているとは限りません。多くのオペレーティングシステムは dom0 および domU として完全に動作します。具体的に言えば、Linux 3.0 とそれ以降、NetBSD 4.0 とそれ以降、OpenSolaris は dom0 および domU として完全に動作します。また、他のオペレーティングシステムは dom0 としては動作せず domU として動作します。dom0 および domU として動作するオペレーティングシステムの状況を確認するには Xen のウィキに含まれる該当ページをご覧ください。<ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen\" /><ulink type=\"block\" url=\"http://wiki.xenproject.org/wiki/DomU_Support_for_Xen\" />"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: PTAL;
msgid "However, if Xen can rely on the hardware functions dedicated to virtualization (which are only present in more recent processors), even non-modified operating systems can run as domU (including Windows)."
msgstr "しかしながら、Xen で仮想化専用のハードウェア機能 (最近のプロセッサだけが搭載した機能) に依存するオペレーティングシステムを動作させる場合や、修正されていないオペレーティングシステム (Windows など) を動作させる場合、そのオペレーティングシステムは domU としてのみ動作します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Using Xen under Debian requires three components:"
msgstr "Debian の下で Xen を使うには 3 つの要素が必要です。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "The hypervisor itself. According to the available hardware, the appropriate package will be either <emphasis role=\"pkg\">xen-hypervisor-4.4-amd64</emphasis>, <emphasis role=\"pkg\">xen-hypervisor-4.4-armhf</emphasis>, or <emphasis role=\"pkg\">xen-hypervisor-4.4-arm64</emphasis>."
msgstr "Xen ハイパーバイザ自身。適切なパッケージは利用できるハードウェアによって決まります。すなわち <emphasis role=\"pkg\">xen-hypervisor-4.4-amd64</emphasis>、<emphasis role=\"pkg\">xen-hypervisor-4.4-armhf</emphasis>、<emphasis role=\"pkg\">xen-hypervisor-4.4-arm64</emphasis> のうちどれか 1 つが必要です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "A kernel that runs on that hypervisor. Any kernel more recent than 3.0 will do, including the 3.16 version present in <emphasis role=\"distribution\">Jessie</emphasis>."
msgstr "ハイパーバイザ上で実行するカーネル。バージョン 3.0 より新しい Linux カーネルが動作します。<emphasis role=\"distribution\">Jessie</emphasis> に含まれる Linux カーネルのバージョンは 3.16 なのでこれも動作します。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "The i386 architecture also requires a standard library with the appropriate patches taking advantage of Xen; this is in the <emphasis role=\"pkg\">libc6-xen</emphasis> package."
msgstr "さらに i386 アーキテクチャでは、Xen を活用するための適切なパッチを組み込んだ標準的なライブラリが必要です。このライブラリは <emphasis role=\"pkg\">libc6-xen</emphasis> パッケージに含まれます。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "In order to avoid the hassle of selecting these components by hand, a few convenience packages (such as <emphasis role=\"pkg\">xen-linux-system-amd64</emphasis>) have been made available; they all pull in a known-good combination of the appropriate hypervisor and kernel packages. The hypervisor also brings <emphasis role=\"pkg\">xen-utils-4.4</emphasis>, which contains tools to control the hypervisor from the dom0. This in turn brings the appropriate standard library. During the installation of all that, configuration scripts also create a new entry in the Grub bootloader menu, so as to start the chosen kernel in a Xen dom0. Note however that this entry is not usually set to be the first one in the list, and will therefore not be selected by default. If that is not the desired behavior, the following commands will change it:"
msgstr "複数の構成要素を手作業で選択するという煩わしさを避けるために、いくつかの便利なパッケージ (<emphasis role=\"pkg\">xen-linux-system-amd64</emphasis> など) が用意されています。これらのパッケージをインストールすることで、適切な Xen ハイパーバイザとカーネルパッケージが既知の良い組み合わせで導入されます。ここで導入される Xen ハイパーバイザには <emphasis role=\"pkg\">xen-utils-4.4</emphasis> が含まれます。<emphasis role=\"pkg\">xen-utils-4.4</emphasis> パッケージには dom0 からハイパーバイザを操作するためのツールが含まれます。同様に、<emphasis role=\"pkg\">xen-utils-4.4</emphasis> パッケージには適切な標準的ライブラリが含まれます。すべてのインストール中に、設定スクリプトは Grub ブートローダメニューに新しいエントリを作成します。こうすることで Xen dom0 から選択されたカーネルを開始することが可能です。しかしながら、通常このエントリはリストの最初に置かれないため、デフォルトで選択されません。この点に注意してください。これを望まない場合、以下のコマンドを使って変更してください。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen\n"
"</userinput><computeroutput># </computeroutput><userinput>update-grub\n"
"</userinput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen\n"
"</userinput><computeroutput># </computeroutput><userinput>update-grub\n"
"</userinput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Once these prerequisites are installed, the next step is to test the behavior of the dom0 by itself; this involves a reboot to the hypervisor and the Xen kernel. The system should boot in its standard fashion, with a few extra messages on the console during the early initialization steps."
msgstr "これらの前提要件をインストールしたら、次に dom0 の挙動をテストします。テストを行うには、Xen ハイパーバイザと Xen カーネルの再起動が必要です。システムは標準的な方法で起動するべきです。初期化の早い段階でコンソールにいくつかの追加的メッセージが表示されます。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "Now is the time to actually install useful systems on the domU systems, using the tools from <emphasis role=\"pkg\">xen-tools</emphasis>. This package provides the <command>xen-create-image</command> command, which largely automates the task. The only mandatory parameter is <literal>--hostname</literal>, giving a name to the domU; other options are important, but they can be stored in the <filename>/etc/xen-tools/xen-tools.conf</filename> configuration file, and their absence from the command line doesn't trigger an error. It is therefore important to either check the contents of this file before creating images, or to use extra parameters in the <command>xen-create-image</command> invocation. Important parameters of note include the following:"
msgstr "これで、実用システムを domU システムに実際にインストールできるようになりました。これを行うには <emphasis role=\"pkg\">xen-tools</emphasis> に含まれるツールを使います。<emphasis role=\"pkg\">xen-tools</emphasis> パッケージには <command>xen-create-image</command> コマンドが含まれます。<command>xen-create-image</command> コマンドはインストール作業の大部分を自動化します。必須のパラメータは <literal>--hostname</literal> だけで、このパラメータは domU の名前を設定します。他のオプションは重要ですが、オプションを <filename>/etc/xen-tools/xen-tools.conf</filename> 設定ファイルに保存することが可能です。そして、コマンドラインでオプションを指定しなくてもエラーは起きません。このため、イメージを作る前にこのファイルの内容を確認するか、<command>xen-create-image</command> の実行時に追加的パラメータを使うことが重要です。以下に注目すべき重要なパラメータを示します。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "<literal>--memory</literal>, to specify the amount of RAM dedicated to the newly created system;"
msgstr "<literal>--memory</literal>。新たに作成する domU システム専用の RAM のサイズを指定します。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "<literal>--size</literal> and <literal>--swap</literal>, to define the size of the “virtual disks” available to the domU;"
msgstr "<literal>--size</literal> と <literal>--swap</literal>。domU で利用できる「仮想ディスク」のサイズを定義します。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "<literal>--debootstrap</literal>, to cause the new system to be installed with <command>debootstrap</command>; in that case, the <literal>--dist</literal> option will also most often be used (with a distribution name such as <emphasis role=\"distribution\">jessie</emphasis>)."
msgstr "<literal>--debootstrap</literal>。<command>debootstrap</command> を使って新しいシステムをインストールします。このオプションを使う場合、<literal>--dist</literal> オプション (ディストリビューションの名前、たとえば <emphasis role=\"distribution\">jessie</emphasis>) を一緒に使うことが多いです。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "<emphasis>GOING FURTHER</emphasis> Installing a non-Debian system in a domU"
msgstr "<emphasis>GOING FURTHER</emphasis> 非 Debian システムを domU にインストール"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "In case of a non-Linux system, care should be taken to define the kernel the domU must use, using the <literal>--kernel</literal> option."
msgstr "非 Linux システムを domU にインストールする場合、<literal>--kernel</literal> オプションを使って、domU で使うカーネルを定義しなければいけない点に注意してください。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "<literal>--dhcp</literal> states that the domU's network configuration should be obtained by DHCP while <literal>--ip</literal> allows defining a static IP address."
msgstr "<literal>--dhcp</literal>。domU のネットワーク設定を DHCP で取得することを宣言します。対して、<literal>--ip</literal> は静的 IP アドレスを定義します。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "Lastly, a storage method must be chosen for the images to be created (those that will be seen as hard disk drives from the domU). The simplest method, corresponding to the <literal>--dir</literal> option, is to create one file on the dom0 for each device the domU should be provided. For systems using LVM, the alternative is to use the <literal>--lvm</literal> option, followed by the name of a volume group; <command>xen-create-image</command> will then create a new logical volume inside that group, and this logical volume will be made available to the domU as a hard disk drive."
msgstr "最後に、作成されるイメージ (domU からはハードディスクドライブに見えるイメージ) の保存方法を選択します。最も簡単な方法は、<literal>--dir</literal> オプションを使い、各 domU を格納するデバイス用のファイルを dom0 上に作成する方法です。LVM を使っているシステムでは、<literal>--lvm</literal> オプションを使い、VG の名前を指定しても良いでしょう。この場合 <command>xen-create-image</command> は指定された VG から新しい LV を分割し、この LV をハードディスクドライブとして domU から利用できるようにします。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>NOTE</emphasis> Storage in the domU"
msgstr "<emphasis>NOTE</emphasis> domU 内のストレージ"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "Entire hard disks can also be exported to the domU, as well as partitions, RAID arrays or pre-existing LVM logical volumes. These operations are not automated by <command>xen-create-image</command>, however, so editing the Xen image's configuration file is in order after its initial creation with <command>xen-create-image</command>."
msgstr "ハードディスク全体、パーティション、RAID アレイ、既存の LVM の LV を domU に書き出すことも可能です。<command>xen-create-image</command> を使ってもこれらの操作を自動化することは不可能ですが、<command>xen-create-image</command> を使って Xen イメージの設定ファイルを作成した後にその設定ファイルを編集することで対応する操作が可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Once these choices are made, we can create the image for our future Xen domU:"
msgstr "これらを選んだ後、将来の Xen domU 用のイメージを作成することが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</userinput>\n"
"<computeroutput>\n"
"[...]\n"
"General Information\n"
"--------------------\n"
"Hostname       :  testxen\n"
"Distribution   :  jessie\n"
"Mirror         :  http://ftp.debian.org/debian/\n"
"Partitions     :  swap            128Mb (swap)\n"
"                  /               2G    (ext3)\n"
"Image type     :  sparse\n"
"Memory size    :  128Mb\n"
"Kernel path    :  /boot/vmlinuz-3.16.0-4-amd64\n"
"Initrd path    :  /boot/initrd.img-3.16.0-4-amd64\n"
"[...]\n"
"Logfile produced at:\n"
"         /var/log/xen-tools/testxen.log\n"
"\n"
"Installation Summary\n"
"---------------------\n"
"Hostname        :  testxen\n"
"Distribution    :  jessie\n"
"MAC Address     :  00:16:3E:8E:67:5C\n"
"IP-Address(es)  :  dynamic\n"
"RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b\n"
"Root Password   :  adaX2jyRHNuWm8BDJS7PcEJ\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</userinput>\n"
"<computeroutput>\n"
"[...]\n"
"General Information\n"
"--------------------\n"
"Hostname       :  testxen\n"
"Distribution   :  jessie\n"
"Mirror         :  http://ftp.debian.org/debian/\n"
"Partitions     :  swap            128Mb (swap)\n"
"                  /               2G    (ext3)\n"
"Image type     :  sparse\n"
"Memory size    :  128Mb\n"
"Kernel path    :  /boot/vmlinuz-3.16.0-4-amd64\n"
"Initrd path    :  /boot/initrd.img-3.16.0-4-amd64\n"
"[...]\n"
"Logfile produced at:\n"
"         /var/log/xen-tools/testxen.log\n"
"\n"
"Installation Summary\n"
"---------------------\n"
"Hostname        :  testxen\n"
"Distribution    :  jessie\n"
"MAC Address     :  00:16:3E:8E:67:5C\n"
"IP-Address(es)  :  dynamic\n"
"RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b\n"
"Root Password   :  adaX2jyRHNuWm8BDJS7PcEJ\n"
"</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "We now have a virtual machine, but it is currently not running (and therefore only using space on the dom0's hard disk). Of course, we can create more images, possibly with different parameters."
msgstr "これで仮想マシンが作成されましたが、仮想マシンはまだ実行されていません (このため dom0 のハードディスク上の領域が使われているだけです)。もちろん、異なるパラメータを使ってより多くのイメージを作成することが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Before turning these virtual machines on, we need to define how they'll be accessed. They can of course be considered as isolated machines, only accessed through their system console, but this rarely matches the usage pattern. Most of the time, a domU will be considered as a remote server, and accessed only through a network. However, it would be quite inconvenient to add a network card for each domU; which is why Xen allows creating virtual interfaces, that each domain can see and use in a standard way. Note that these cards, even though they're virtual, will only be useful once connected to a network, even a virtual one. Xen has several network models for that:"
msgstr "仮想マシンを起動する前に、仮想マシンにアクセスする方法を定義します。もちろん仮想マシンは隔離されたマシンですから、仮想マシンにアクセスする唯一の方法はシステムコンソールだけです。しかし、システムコンソールだけで要求を満足できることはほとんどないと言っても過言ではありません。ほとんどの時間、domU はリモートサーバとして機能し、ネットワークを通じてのみアクセスされます。しかしながら、各 domU 専用のネットワークカードを追加するのはかなり不便です。このため Xen は仮想インターフェースの作成機能を備えています。各ドメインは仮想インターフェースを参照し、標準的な方法で使うことが可能です。これらのネットワークカードは仮想的なものですが、ネットワークに接続されている状況下でのみ役に立つという点に注意してください。Xen は以下に挙げる複数のネットワークモデルを備えています。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The simplest model is the <emphasis>bridge</emphasis> model; all the eth0 network cards (both in the dom0 and the domU systems) behave as if they were directly plugged into an Ethernet switch."
msgstr "最も単純なモデルは <emphasis>bridge</emphasis> モデルです。この場合、すべての eth0 ネットワークカードが (dom0 と domU システムに含まれるものも含めて) 直接的にイーサネットスイッチに接続されているかのように振る舞います。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Then comes the <emphasis>routing</emphasis> model, where the dom0 behaves as a router that stands between the domU systems and the (physical) external network."
msgstr "2 番目に単純なモデルは <emphasis>routing</emphasis> モデルです。これは dom0 が domU システムと (物理) 外部ネットワークの間に位置するルータとして振る舞うモデルです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Finally, in the <emphasis>NAT</emphasis> model, the dom0 is again between the domU systems and the rest of the network, but the domU systems are not directly accessible from outside, and traffic goes through some network address translation on the dom0."
msgstr "最後が <emphasis>NAT</emphasis> モデルです。これは dom0 が domU システムとその他のネットワークの間に位置するモデルですが、domU システムに外部から直接アクセスすることは不可能です。dom0 の行ういくつかのネットワークアドレス変換がトラフィックを仲介します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "These three networking nodes involve a number of interfaces with unusual names, such as <filename>vif*</filename>, <filename>veth*</filename>, <filename>peth*</filename> and <filename>xenbr0</filename>. The Xen hypervisor arranges them in whichever layout has been defined, under the control of the user-space tools. Since the NAT and routing models are only adapted to particular cases, we will only address the bridging model."
msgstr "これら 3 種類のネットワークノードは <filename>vif*</filename>、<filename>veth*</filename>、<filename>peth*</filename>、<filename>xenbr0</filename> などの独特な名前を付けられた数多くのインターフェースと関係を持ちます。Xen ハイパーバイザは定義された配置に従いユーザ空間ツールの制御の下でインターフェースを準備します。NAT と routing モデルは特定の場合にのみ適合します。このためわれわれは bridge モデルを使います。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Ref: http://wiki.xenproject.org/wiki/Xen_Networking;
# Ref: http://wiki.xenproject.org/wiki/Network_Configuration_Examples_(Xen_4.1%2B);
msgid "The standard configuration of the Xen packages does not change the system-wide network configuration. However, the <command>xend</command> daemon is configured to integrate virtual network interfaces into any pre-existing network bridge (with <filename>xenbr0</filename> taking precedence if several such bridges exist). We must therefore set up a bridge in <filename>/etc/network/interfaces</filename> (which requires installing the <emphasis role=\"pkg\">bridge-utils</emphasis> package, which is why the <emphasis role=\"pkg\">xen-utils-4.4</emphasis> package recommends it) to replace the existing eth0 entry:"
msgstr "Xen パッケージの標準的な設定はシステム全体のネットワーク設定を変更しません。しかしながら、<command>xend</command> デーモンは既存のネットワークブリッジの中に仮想ネットワークインターフェースを組み込むように設定されています (複数のブリッジが存在する場合 <filename>xenbr0</filename> を優先します)。このためここでは <filename>/etc/network/interfaces</filename> の中にブリッジをセットアップして (<emphasis role=\"pkg\">bridge-utils</emphasis> パッケージをインストールする必要があります。このため <emphasis role=\"pkg\">bridge-utils</emphasis> パッケージは <emphasis role=\"pkg\">xen-utils-4.4</emphasis> パッケージの推奨パッケージになっています)、既存の eth0 エントリを置き替えます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid ""
"auto xenbr0\n"
"iface xenbr0 inet dhcp\n"
"    bridge_ports eth0\n"
"    bridge_maxwait 0\n"
"    "
msgstr ""
"auto xenbr0\n"
"iface xenbr0 inet dhcp\n"
"    bridge_ports eth0\n"
"    bridge_maxwait 0\n"
"    "

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "After rebooting to make sure the bridge is automatically created, we can now start the domU with the Xen control tools, in particular the <command>xl</command> command. This command allows different manipulations on the domains, including listing them and, starting/stopping them."
msgstr "再起動して、ブリッジが自動的に作成されることを確認します。この後 Xen 制御ツール、特に <command>xl</command> コマンドを使って domU を起動することが可能です。また、<command>xl</command> を使ってドメインを表示、起動、終了するなどの操作を行うことが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid ""
"<computeroutput># </computeroutput><userinput>xl list</userinput>\n"
"<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)\n"
"Domain-0                                     0   463     1     r-----      9.8\n"
"# </computeroutput><userinput>xl create /etc/xen/testxen.cfg</userinput>\n"
"<computeroutput>Parsing config from /etc/xen/testxen.cfg\n"
"# </computeroutput><userinput>xl list</userinput>\n"
"<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)\n"
"Domain-0                                     0   366     1     r-----     11.4\n"
"testxen                                      1   128     1     -b----      1.1</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>xl list</userinput>\n"
"<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)\n"
"Domain-0                                     0   463     1     r-----      9.8\n"
"# </computeroutput><userinput>xl create /etc/xen/testxen.cfg</userinput>\n"
"<computeroutput>Parsing config from /etc/xen/testxen.cfg\n"
"# </computeroutput><userinput>xl list</userinput>\n"
"<computeroutput>Name                                        ID   Mem VCPUs      State   Time(s)\n"
"Domain-0                                     0   366     1     r-----     11.4\n"
"testxen                                      1   128     1     -b----      1.1</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>TOOL</emphasis> Choice of toolstacks to manage Xen VM"
msgstr "<emphasis>TOOL</emphasis> Xen VM を管理するツールスタックの選択"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary><command>xm</command></primary>"
msgstr "<primary><command>xm</command></primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary><command>xe</command></primary>"
msgstr "<primary><command>xe</command></primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "In Debian 7 and older releases, <command>xm</command> was the reference command line tool to use to manage Xen virtual machines. It has now been replaced by <command>xl</command> which is mostly backwards compatible. But those are not the only available tools: <command>virsh</command> of libvirt and <command>xe</command> of XenServer's XAPI (commercial offering of Xen) are alternative tools."
msgstr "Debian 7 および Debian 7 よりも古いリリースでは、<command>xm</command> が Xen 仮想マシンの管理に使うための標準的なコマンドラインツールでした。現在 <command>xm</command> はほぼ後方互換性を持つ <command>xl</command> によって置き換えられました。しかし、利用できるツールは <command>xm</command> および <command>xl</command> だけではありません。代替ツールとしては、libvirt を操作する <command>virsh</command> と XenServer (Xen の商用版) の XAPI を操作する <command>xe</command> が存在します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>CAUTION</emphasis> Only one domU per image!"
msgstr "<emphasis>CAUTION</emphasis> 1 つのイメージに 1 台以上の domU を割り当てないでください!"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "While it is of course possible to have several domU systems running in parallel, they will all need to use their own image, since each domU is made to believe it runs on its own hardware (apart from the small slice of the kernel that talks to the hypervisor). In particular, it isn't possible for two domU systems running simultaneously to share storage space. If the domU systems are not run at the same time, it is however quite possible to reuse a single swap partition, or the partition hosting the <filename>/home</filename> filesystem."
msgstr "もちろん複数の domU システムを並列実行させることは可能ですが、各 domU システムは専用のイメージを必要とします。なぜなら、各 domU は自分に割り当てられたハードウェアを専有するという仮定に基づいて実行されるからです (ハイパーバイザとやり取りするカーネルの一部分は別です)。特に、ストレージ領域を共有する目的で 2 つの domU システムを同時に起動することは不可能です。2 つの domU システムが同時に起動していなければ、両者はストレージ領域を共有して、単独のスワップパーティションや <filename>/home</filename> ファイルシステムをホストしているパーティションを再利用することが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Note that the <filename>testxen</filename> domU uses real memory taken from the RAM that would otherwise be available to the dom0, not simulated memory. Care should therefore be taken, when building a server meant to host Xen instances, to provision the physical RAM accordingly."
msgstr "<filename>testxen</filename> domU は仮想メモリではなく RAM から取った物理メモリを使います。このメモリ領域は <filename>testxen</filename> domU が起動していなければ dom0 が使えるメモリ領域だったという点に注意してください。このため、サーバを作ることが Xen インスタンスをホストすることを意味する場合、それに応じて十分なサイズの物理 RAM が必要になるという点に注意が必要です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Voilà! Our virtual machine is starting up. We can access it in one of two modes. The usual way is to connect to it “remotely” through the network, as we would connect to a real machine; this will usually require setting up either a DHCP server or some DNS configuration. The other way, which may be the only way if the network configuration was incorrect, is to use the <filename>hvc0</filename> console, with the <command>xl console</command> command:"
msgstr "おめでとうございます! 仮想マシンが開始されました。仮想マシンにアクセスするには 2 種類の方法があります。通常の方法は、真のマシンに接続するのと同様に、ネットワークを介して「リモートで」仮想マシンに接続することです。そしてこれを行うには、通常別の DHCP サーバや DNS 設定をセットアップすることが必要です。別の方法は <command>xl console</command> コマンドから <filename>hvc0</filename> コンソールを使う方法です。ネットワーク設定が正しくない場合にはこれが唯一の方法です。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>xl console testxen</userinput>\n"
"<computeroutput>[...]\n"
"\n"
"Debian GNU/Linux 8 testxen hvc0\n"
"\n"
"testxen login: </computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>xl console testxen</userinput>\n"
"<computeroutput>[...]\n"
"\n"
"Debian GNU/Linux 8 testxen hvc0\n"
"\n"
"testxen login: </computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "One can then open a session, just like one would do if sitting at the virtual machine's keyboard. Detaching from this console is achieved through the <keycombo action=\"simul\"><keycap>Control</keycap> <keycap>]</keycap></keycombo> key combination."
msgstr "仮想マシンのキーボードの前に座っているかのごとくセッションを開くことが可能です。このコンソールからデタッチするには、<keycombo action=\"simul\"><keycap>Control</keycap> <keycap>]</keycap></keycombo> キーの組み合わせを使用します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>TIP</emphasis> Getting the console straight away"
msgstr "<emphasis>TIP</emphasis> すぐにコンソールを開始する"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "Sometimes one wishes to start a domU system and get to its console straight away; this is why the <command>xl create</command> command takes a <literal>-c</literal> switch. Starting a domU with this switch will display all the messages as the system boots."
msgstr "domU システムの開始直後にコンソールを始めたい場合があります。そしてこの希望に応えるために <command>xl create</command> コマンドは <literal>-c</literal> オプションを備えています。<literal>-c</literal> オプションを付けて domU を開始すれば、システム起動時に表示されるすべてのメッセージを見ることが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>TOOL</emphasis> OpenXenManager"
msgstr "<emphasis>TOOL</emphasis> OpenXenManager"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "OpenXenManager (in the <emphasis role=\"pkg\">openxenmanager</emphasis> package) is a graphical interface allowing remote management of Xen domains via Xen's API. It can thus control Xen domains remotely. It provides most of the features of the <command>xl</command> command."
msgstr "OpenXenManager (<emphasis role=\"pkg\">openxenmanager</emphasis> パッケージに含まれます) はグラフィカルインターフェースです。これ使うことで Xen API を介して Xen ドメインのリモート管理することが可能です。このため、Xen ドメインをリモートで制御することが可能です。OpenXenManager は <command>xl</command> コマンドの機能のほとんどを備えています。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "Once the domU is up, it can be used just like any other server (since it is a GNU/Linux system after all). However, its virtual machine status allows some extra features. For instance, a domU can be temporarily paused then resumed, with the <command>xl pause</command> and <command>xl unpause</command> commands. Note that even though a paused domU does not use any processor power, its allocated memory is still in use. It may be interesting to consider the <command>xl save</command> and <command>xl restore</command> commands: saving a domU frees the resources that were previously used by this domU, including RAM. When restored (or unpaused, for that matter), a domU doesn't even notice anything beyond the passage of time. If a domU was running when the dom0 is shut down, the packaged scripts automatically save the domU, and restore it on the next boot. This will of course involve the standard inconvenience incurred when hibernating a laptop computer, for instance; in particular, if the domU is suspended for too long, network connections may expire. Note also that Xen is so far incompatible with a large part of ACPI power management, which precludes suspending the host (dom0) system."
msgstr "domU の起動完了後、domU は他のサーバと同様に使うことが可能です (domU は結局 GNU/Linux システムに過ぎません)。しかしながら、domU の仮想マシンの状態はいくつかの追加的機能を備えています。たとえば、<command>xl pause</command> と <command>xl unpause</command> コマンドを使って domU を一時的に停止したり再開することが可能です。一時的に停止された domU は全くプロセッサを使いませんが、割り当てられたメモリを解放しません。<command>xl save</command> と <command>xl restore</command> コマンドを考慮することは興味深いかもしれません。なぜなら <command>xl save</command> で domU を保存すれば domU の使っていた RAM などの資源が解放されるからです。また、<command>xl restore</command> で domU を元に戻す時 (ついでに言えば <command>xl unpause</command> で再開する時)、domU は時間が経過したことに全く気が付きません。dom0 を停止した時に domU が動いていた場合、パッケージに含まれるスクリプトが自動的に <command>xl save</command> で domU を保存し、dom0 の次回起動時に自動的に <command>xl restore</command> で domU を再開します。もちろんこれにはラップトップコンピュータをハイバネートする場合と同様の標準的な不便さがあります。特に、domU が長い間一時停止されていた場合、ネットワーク接続が切断される可能性があります。今現在 Xen は ACPI 電源管理のほとんどに互換性がない点にも注意してください。このため、ホスト (dom0) システムを一時停止することは不可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "<emphasis>DOCUMENTATION</emphasis> <command>xl</command> options"
msgstr "<emphasis>DOCUMENTATION</emphasis> <command>xl</command> のオプション"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "Most of the <command>xl</command> subcommands expect one or more arguments, often a domU name. These arguments are well described in the <citerefentry><refentrytitle>xl</refentrytitle> <manvolnum>1</manvolnum></citerefentry> manual page."
msgstr "<command>xl</command> サブコマンドのほとんどは domU の名前などの 1 つか複数個の引数を取ります。これらの引数は <citerefentry><refentrytitle>xl</refentrytitle> <manvolnum>1</manvolnum></citerefentry> マニュアルページは詳しく説明されています。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "Halting or rebooting a domU can be done either from within the domU (with the <command>shutdown</command> command) or from the dom0, with <command>xl shutdown</command> or <command>xl reboot</command>."
msgstr "domU を停止したり再起動するには、domU の内部から (<command>shutdown</command> コマンドを使って) 行ったり、dom0 から <command>xl shutdown</command> または <command>xl reboot</command> を使って行うことも可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>GOING FURTHER</emphasis> Advanced Xen"
msgstr "<emphasis>GOING FURTHER</emphasis> Xen の上級活用"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Xen has many more features than we can describe in these few paragraphs. In particular, the system is very dynamic, and many parameters for one domain (such as the amount of allocated memory, the visible hard drives, the behavior of the task scheduler, and so on) can be adjusted even when that domain is running. A domU can even be migrated across servers without being shut down, and without losing its network connections! For all these advanced aspects, the primary source of information is the official Xen documentation. <ulink type=\"block\" url=\"http://www.xen.org/support/documentation.html\" />"
msgstr "Xen はここで示すことができた数項だけにとどまらない多くの機能を持っています。特に、Xen はシステムを動的に変更することが可能です。すなわちドメインに対する多くのパラメータ (割り当てメモリサイズ、見えるハードドライブ、タスクスケジューラの挙動など) をドメインの実行中に調整することが可能です。domU はシャットダウンすることもネットワーク接続を失うこともなくサーバ間を移動することさえ可能です! すべての上級活用法に関して、最良の情報源は公式の Xen 文書です。<ulink type=\"block\" url=\"http://www.xen.org/support/documentation.html\" />"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary>LXC</primary>"
msgstr "<primary>LXC</primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "Even though it is used to build “virtual machines”, LXC is not, strictly speaking, a virtualization system, but a system to isolate groups of processes from each other even though they all run on the same host. It takes advantage of a set of recent evolutions in the Linux kernel, collectively known as <emphasis>control groups</emphasis>, by which different sets of processes called “groups” have different views of certain aspects of the overall system. Most notable among these aspects are the process identifiers, the network configuration, and the mount points. Such a group of isolated processes will not have any access to the other processes in the system, and its accesses to the filesystem can be restricted to a specific subset. It can also have its own network interface and routing table, and it may be configured to only see a subset of the available devices present on the system."
msgstr "LXC は「仮想マシン」を作るために使われるにも関わらず、厳密に言うと仮想システムではなく、同じホスト上で実行されるプロセスのグループを隔離するためのシステムです。LXC は近年 Linux カーネルに対して行われた数々の機能の利点を活用しています。これらの機能はまとめて <emphasis>control groups</emphasis> として知られています。<emphasis>control groups</emphasis> を使うことにより、「グループ」と呼ばれるさまざまなプロセス群に対してシステム全体の特定の側面の状態を強制することが可能です。中でも最も注目すべき側面はプロセス ID、ネットワーク接続、マウントポイントです。隔離されたプロセスのグループはシステムの他のプロセスにアクセスできませんし、グループによるファイルシステムへのアクセスを特定の一部に限定することが可能です。さらにグループにネットワークインターフェースとルーティングテーブルを設定することにより、グループがシステム上の利用できるデバイスの一部だけを見えるように設定することが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "These features can be combined to isolate a whole process family starting from the <command>init</command> process, and the resulting set looks very much like a virtual machine. The official name for such a setup is a “container” (hence the LXC moniker: <emphasis>LinuX Containers</emphasis>), but a rather important difference with “real” virtual machines such as provided by Xen or KVM is that there's no second kernel; the container uses the very same kernel as the host system. This has both pros and cons: advantages include excellent performance due to the total lack of overhead, and the fact that the kernel has a global vision of all the processes running on the system, so the scheduling can be more efficient than it would be if two independent kernels were to schedule different task sets. Chief among the inconveniences is the impossibility to run a different kernel in a container (whether a different Linux version or a different operating system altogether)."
msgstr "これらの機能を組み合わせることで、<command>init</command> プロセスから起動されたすべてのプロセスファミリーを隔離することが可能です。その結果、仮想マシンにとてもよく似たものが作られます。このようなセットアップの正式名称が「コンテナ」です (LXC の名称 <emphasis>LinuX Containers</emphasis> はこれに由来しています)。Xen や KVM が提供する「真の」仮想マシンとのより重要な違いは仮想マシン用のカーネルがない点です。このため、コンテナはホストシステムと全く同じカーネルを使います。これには利点と欠点があります。すなわち、利点はオーバーヘッドが全くないことで素晴らしい性能を得ることが可能という点とカーネルはシステムで実行しているすべてのプロセスを見ることが可能という点です。このため 2 つの独立したカーネルが異なるタスクセットでスケジュールを行うよりも効果的なスケジューリングが可能です。欠点の最たるものはコンテナの中で異なるカーネルを動作させることが不可能という点です (異なる Linux バージョンや異なるオペレーティングシステムを同時に動かすことができません)。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>NOTE</emphasis> LXC isolation limits"
msgstr "<emphasis>NOTE</emphasis> LXC の隔離制限"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "LXC containers do not provide the level of isolation achieved by heavier emulators or virtualizers. In particular:"
msgstr "LXC コンテナは負荷の大きなエミュレータやバーチャライザが備える隔離機能を備えていません。たとえば以下のような機能を備えていません。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: PTAL;
msgid "since the kernel is shared among the host system and the containers, processes constrained to containers can still access the kernel messages, which can lead to information leaks if messages are emitted by a container;"
msgstr "カーネルはホストシステムとコンテナによって共有されているため、コンテナ内に隔離されているプロセスはカーネルメッセージにアクセスできます。このことにより、メッセージがコンテナによって発せられた場合、情報が漏洩する可能性があります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "for similar reasons, if a container is compromised and a kernel vulnerability is exploited, the other containers may be affected too;"
msgstr "同様の理由で、コンテナが不正アクセスされカーネルの脆弱性が悪用された場合、他のコンテナが影響を受ける可能性があります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "on the filesystem, the kernel checks permissions according to the numerical identifiers for users and groups; these identifiers may designate different users and groups depending on the container, which should be kept in mind if writable parts of the filesystem are shared among containers."
msgstr "ファイルシステムについて、カーネルはユーザとグループの数値的な識別子に従ってパーミッションを確認します。これらの識別子の意味するユーザとグループはコンテナごとに異なります。ファイルシステムの書き込み可能な部分がコンテナ同士で共有されている場合、この点を覚えておくべきです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Since we are dealing with isolation and not plain virtualization, setting up LXC containers is more complex than just running debian-installer on a virtual machine. We will describe a few prerequisites, then go on to the network configuration; we will then be able to actually create the system to be run in the container."
msgstr "LXC による隔離は単純な仮想化と異なるため、LXC コンテナを設定することは仮想マシン上で単純に debian-installer を実行するよりも複雑な作業です。このため、いくつかの必要条件を説明した後、ネットワーク設定を行います。こうすることで、コンテナの中で実行するシステムを実際に作成することが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Preliminary Steps"
msgstr "準備段階"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The <emphasis role=\"pkg\">lxc</emphasis> package contains the tools required to run LXC, and must therefore be installed."
msgstr "<emphasis role=\"pkg\">lxc</emphasis> パッケージには LXC を実行するために必要なツールが含まれるため、必ずこのパッケージをインストールしなければいけません。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "LXC also requires the <emphasis>control groups</emphasis> configuration system, which is a virtual filesystem to be mounted on <filename>/sys/fs/cgroup</filename>. Since Debian 8 switched to systemd, which also relies on control groups, this is now done automatically at boot time without further configuration."
msgstr "LXC を使うには <emphasis>control groups</emphasis> 設定システムが必要で、<filename>/sys/fs/cgroup</filename> に仮想ファイルシステムをマウントする必要があります。Debian 8 からは init システムとして systemd が採用されており、systemd は <emphasis>control groups</emphasis> に依存しているため、設定せずとも <filename>/sys/fs/cgroup</filename> は起動時に自動でマウントされます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Network Configuration"
msgstr "ネットワークの設定"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The goal of installing LXC is to set up virtual machines; while we could of course keep them isolated from the network, and only communicate with them via the filesystem, most use cases involve giving at least minimal network access to the containers. In the typical case, each container will get a virtual network interface, connected to the real network through a bridge. This virtual interface can be plugged either directly onto the host's physical network interface (in which case the container is directly on the network), or onto another virtual interface defined on the host (and the host can then filter or route traffic). In both cases, the <emphasis role=\"pkg\">bridge-utils</emphasis> package will be required."
msgstr "LXC をインストールする目的は仮想マシンをセットアップすることです。もちろん、仮想マシンをネットワークから隔離するように設定したり、ファイルシステムを介してのみ情報をやり取りするように設定することも可能ですが、コンテナに対して少なくとも最低限のネットワークアクセスを提供するように設定するのが一般的です。典型的な場合、各コンテナにはブリッジを介して実際のネットワークに接続された仮想ネットワークインターフェースが備えられています。この仮想インターフェースは、直接ホスト上の物理ネットワークインターフェースに接続されているか (この場合、コンテナは直接ネットワークに接続されています)、ホスト上に定義された他の仮想インターフェースに接続されています (ホストからトラフィックをフィルタしたり配送することが可能です)。どちらの場合も、<emphasis role=\"pkg\">bridge-utils</emphasis> パッケージが必要です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The simple case is just a matter of editing <filename>/etc/network/interfaces</filename>, moving the configuration for the physical interface (for instance <literal>eth0</literal>) to a bridge interface (usually <literal>br0</literal>), and configuring the link between them. For instance, if the network interface configuration file initially contains entries such as the following:"
msgstr "最も簡単なやり方は <filename>/etc/network/interfaces</filename> を編集することです。物理インターフェース (たとえば <literal>eth0</literal>) に関する設定をブリッジインターフェース (通常 <literal>br0</literal>) に変え、物理とブリッジインターフェース間のリンクを設定します。たとえば、最初にネットワークインターフェース設定ファイルが以下のようなエントリを持っていたとします。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid ""
"auto eth0\n"
"iface eth0 inet dhcp"
msgstr ""
"auto eth0\n"
"iface eth0 inet dhcp"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "They should be disabled and replaced with the following:"
msgstr "このエントリを無効化し、以下の通り書き換えます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid ""
"#auto eth0\n"
"#iface eth0 inet dhcp\n"
"\n"
"auto br0\n"
"iface br0 inet dhcp\n"
"  bridge-ports eth0"
msgstr ""
"#auto eth0\n"
"#iface eth0 inet dhcp\n"
"\n"
"auto br0\n"
"iface br0 inet dhcp\n"
"  bridge_ports eth0"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The effect of this configuration will be similar to what would be obtained if the containers were machines plugged into the same physical network as the host. The “bridge” configuration manages the transit of Ethernet frames between all the bridged interfaces, which includes the physical <literal>eth0</literal> as well as the interfaces defined for the containers."
msgstr "この設定により、コンテナをホストと同じ物理ネットワークに接続されたマシンとして考えた場合と、同様の効果が得られます。この「ブリッジ」設定はすべてのブリッジされたインターフェース間のイーサネットフレームの通過を管理します。これには物理的な <literal>eth0</literal> およびコンテナ用に定義されたインターフェースが含まれます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "In cases where this configuration cannot be used (for instance if no public IP addresses can be assigned to the containers), a virtual <emphasis>tap</emphasis> interface will be created and connected to the bridge. The equivalent network topology then becomes that of a host with a second network card plugged into a separate switch, with the containers also plugged into that switch. The host must then act as a gateway for the containers if they are meant to communicate with the outside world."
msgstr "この設定を使うことができない場合 (たとえば、公開 IP アドレスをコンテナに割り当てることができない場合)、仮想 <emphasis>tap</emphasis> インターフェースを作成し、これをブリッジに接続します。これと等価なネットワークトポロジーは、ホストの 2 番目のネットワークカードが分離されたスイッチに接続されている状態です。コンテナはこのスイッチに接続されています。コンテナが外部と通信するには、ホストがコンテナ用のゲートウェイとして振る舞わなければいけません。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "In addition to <emphasis role=\"pkg\">bridge-utils</emphasis>, this “rich” configuration requires the <emphasis role=\"pkg\">vde2</emphasis> package; the <filename>/etc/network/interfaces</filename> file then becomes:"
msgstr "この「ぜいたくな」設定を行うには <emphasis role=\"pkg\">bridge-utils</emphasis> と <emphasis role=\"pkg\">vde2</emphasis> パッケージが必要です。<filename>/etc/network/interfaces</filename> ファイルは以下のようになります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid ""
"# Interface eth0 is unchanged\n"
"auto eth0\n"
"iface eth0 inet dhcp\n"
"\n"
"# Virtual interface \n"
"auto tap0\n"
"iface tap0 inet manual\n"
"  vde2-switch -t tap0\n"
"\n"
"# Bridge for containers\n"
"auto br0\n"
"iface br0 inet static\n"
"  bridge-ports tap0\n"
"  address 10.0.0.1\n"
"  netmask 255.255.255.0"
msgstr ""
"# eth0 インターフェースは同じものを使います\n"
"auto eth0\n"
"iface eth0 inet dhcp\n"
"\n"
"# 仮想インターフェース\n"
"auto tap0\n"
"iface tap0 inet manual\n"
"  vde2-switch -t tap0\n"
"\n"
"# コンテナ用のブリッジ\n"
"auto br0\n"
"iface br0 inet static\n"
"  bridge_ports tap0\n"
"  address 10.0.0.1\n"
"  netmask 255.255.255.0"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The network can then be set up either statically in the containers, or dynamically with DHCP server running on the host. Such a DHCP server will need to be configured to answer queries on the <literal>br0</literal> interface."
msgstr "コンテナのネットワークは静的またはコンテナのホスト上で動く DHCP サーバを使って動的に設定されます。また、DHCP サーバを <literal>br0</literal> インターフェースを介した問い合わせに応答するように設定する必要があります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Setting Up the System"
msgstr "システムのセットアップ"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Let us now set up the filesystem to be used by the container. Since this “virtual machine” will not run directly on the hardware, some tweaks are required when compared to a standard filesystem, especially as far as the kernel, devices and consoles are concerned. Fortunately, the <emphasis role=\"pkg\">lxc</emphasis> includes scripts that mostly automate this configuration. For instance, the following commands (which require the <emphasis role=\"pkg\">debootstrap</emphasis> and <emphasis role=\"pkg\">rsync</emphasis> packages) will install a Debian container:"
msgstr "それではコンテナがファイルシステムを使うようにファイルシステムを設定しましょう。コンテナという「仮想マシン」はハードウェア上で直接的に実行されないため、標準的なファイルシステムに比べていくつかの微調整を必要とします。これは特にカーネル、デバイス、コンソールが該当します。幸いなことに、<emphasis role=\"pkg\">lxc</emphasis> にはこの設定をほぼ自動化するスクリプトが含まれます。たとえば、以下のコマンド (<emphasis role=\"pkg\">debootstrap</emphasis> と <emphasis role=\"pkg\">rsync</emphasis> パッケージが必要です) で Debian コンテナがインストールされます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-create -n testlxc -t debian\n"
"</userinput><computeroutput>debootstrap is /usr/sbin/debootstrap\n"
"Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 ... \n"
"Downloading debian minimal ...\n"
"I: Retrieving Release \n"
"I: Retrieving Release.gpg \n"
"[...]\n"
"Download complete.\n"
"Copying rootfs to /var/lib/lxc/testlxc/rootfs...\n"
"[...]\n"
"Root password is 'sSiKhMzI', please change !\n"
"root@mirwiz:~# </computeroutput>\n"
"        "
msgstr ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-create -n testlxc -t debian\n"
"</userinput><computeroutput>debootstrap は /usr/sbin/debootstrap です\n"
"Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 ... \n"
"Downloading debian minimal ...\n"
"I: Retrieving Release \n"
"I: Retrieving Release.gpg \n"
"[...]\n"
"Download complete.\n"
"Copying rootfs to /var/lib/lxc/testlxc/rootfs...\n"
"[...]\n"
"Root password is 'sSiKhMzI', please change !\n"
"root@mirwiz:~# </computeroutput>\n"
"        "

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Note that the filesystem is initially created in <filename>/var/cache/lxc</filename>, then moved to its destination directory. This allows creating identical containers much more quickly, since only copying is then required."
msgstr "ファイルシステムは最初に <filename>/var/cache/lxc</filename> の中に作成され、その後目的のディレクトリに移動されます。こうすることで、同一のコンテナが極めて素早く作成されます。なぜなら、単純にコピーするだけだからです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Note that the debian template creation script accepts an <option>--arch</option> option to specify the architecture of the system to be installed and a <option>--release</option> option if you want to install something else than the current stable release of Debian. You can also set the <literal>MIRROR</literal> environment variable to point to a local Debian mirror."
msgstr "この debian テンプレート作成スクリプトは、インストールされるシステムのアーキテクチャを指定する <option>--arch</option> オプションと、現在の Debian 安定版以外の物をインストールしたい場合に指定する <option>--release</option> オプションを取ります。また、<literal>MIRROR</literal> 環境変数を設定してローカル Debian アーカイブミラーを指定することも可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The newly-created filesystem now contains a minimal Debian system, and by default the container has no network interface (besides the loopback one). Since this is not really wanted, we will edit the container's configuration file (<filename>/var/lib/lxc/testlxc/config</filename>) and add a few <literal>lxc.network.*</literal> entries:"
msgstr "これで、新規に作成されたファイルシステムが最低限の Debian システムを含むようになりました。デフォルト状態だとこのコンテナにはネットワークインターフェースがありません (ループバックインターフェースすらありません)。これは全く望むべき状態ではないため、コンテナの設定ファイル (<filename>/var/lib/lxc/testlxc/config</filename>) を編集し、いくつかの <literal>lxc.network.*</literal> エントリを追加します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid ""
"lxc.network.type = veth\n"
"lxc.network.flags = up\n"
"lxc.network.link = br0\n"
"lxc.network.hwaddr = 4a:49:43:49:79:20"
msgstr ""
"lxc.network.type = veth\n"
"lxc.network.flags = up\n"
"lxc.network.link = br0\n"
"lxc.network.hwaddr = 4a:49:43:49:79:20"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "These entries mean, respectively, that a virtual interface will be created in the container; that it will automatically be brought up when said container is started; that it will automatically be connected to the <literal>br0</literal> bridge on the host; and that its MAC address will be as specified. Should this last entry be missing or disabled, a random MAC address will be generated."
msgstr "これらのエントリの意味するところはそれぞれ、仮想インターフェースはコンテナによって作られます。そして仮想インターフェースはコンテナが開始された時に自動的に利用できる状態にされます。そして仮想インターフェースはホストの <literal>br0</literal> ブリッジに自動的に接続されます。さらに仮想インターフェースの MAC アドレスは指定したものになります。最後のエントリを削除するか無効化した場合、ランダムな MAC アドレスが生成されます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Another useful entry in that file is the setting of the hostname:"
msgstr "以下のようにすることで、設定ファイル内でホスト名を設定することも可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "lxc.utsname = testlxc"
msgstr "lxc.utsname = testlxc"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Starting the Container"
msgstr "コンテナの開始"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Now that our virtual machine image is ready, let's start the container:"
msgstr "これで仮想マシンイメージの準備が整いました。それではコンテナを開始しましょう。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-start --daemon --name=testlxc\n"
"</userinput><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-console -n testlxc\n"
"</userinput><computeroutput>Debian GNU/Linux 8 testlxc tty1\n"
"\n"
"testlxc login: </computeroutput><userinput>root</userinput><computeroutput>\n"
"Password: \n"
"Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64\n"
"\n"
"The programs included with the Debian GNU/Linux system are free software;\n"
"the exact distribution terms for each program are described in the\n"
"individual files in /usr/share/doc/*/copyright.\n"
"\n"
"Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n"
"permitted by applicable law.\n"
"root@testlxc:~# </computeroutput><userinput>ps auxwf</userinput>\n"
"<computeroutput>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n"
"root         1  0.0  0.2  28164  4432 ?        Ss   17:33   0:00 /sbin/init\n"
"root        20  0.0  0.1  32960  3160 ?        Ss   17:33   0:00 /lib/systemd/systemd-journald\n"
"root        82  0.0  0.3  55164  5456 ?        Ss   17:34   0:00 /usr/sbin/sshd -D\n"
"root        87  0.0  0.1  12656  1924 tty2     Ss+  17:34   0:00 /sbin/agetty --noclear tty2 linux\n"
"root        88  0.0  0.1  12656  1764 tty3     Ss+  17:34   0:00 /sbin/agetty --noclear tty3 linux\n"
"root        89  0.0  0.1  12656  1908 tty4     Ss+  17:34   0:00 /sbin/agetty --noclear tty4 linux\n"
"root        90  0.0  0.1  63300  2944 tty1     Ss   17:34   0:00 /bin/login --     \n"
"root       117  0.0  0.2  21828  3668 tty1     S    17:35   0:00  \\_ -bash\n"
"root       268  0.0  0.1  19088  2572 tty1     R+   17:39   0:00      \\_ ps auxfw\n"
"root        91  0.0  0.1  14228  2356 console  Ss+  17:34   0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102\n"
"root       197  0.0  0.4  25384  7640 ?        Ss   17:38   0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e\n"
"root       266  0.0  0.1  12656  1840 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty5 linux\n"
"root       267  0.0  0.1  12656  1928 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty6 linux\n"
"root@testlxc:~# </computeroutput>"
msgstr ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-start --daemon --name=testlxc\n"
"</userinput><computeroutput>root@mirwiz:~# </computeroutput><userinput>lxc-console -n testlxc\n"
"</userinput><computeroutput>Debian GNU/Linux 8 testlxc tty1\n"
"\n"
"testlxc login: </computeroutput><userinput>root</userinput><computeroutput>\n"
"Password: \n"
"Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64\n"
"\n"
"The programs included with the Debian GNU/Linux system are free software;\n"
"the exact distribution terms for each program are described in the\n"
"individual files in /usr/share/doc/*/copyright.\n"
"\n"
"Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\n"
"permitted by applicable law.\n"
"root@testlxc:~# </computeroutput><userinput>ps auxwf</userinput>\n"
"<computeroutput>USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n"
"root         1  0.0  0.2  28164  4432 ?        Ss   17:33   0:00 /sbin/init\n"
"root        20  0.0  0.1  32960  3160 ?        Ss   17:33   0:00 /lib/systemd/systemd-journald\n"
"root        82  0.0  0.3  55164  5456 ?        Ss   17:34   0:00 /usr/sbin/sshd -D\n"
"root        87  0.0  0.1  12656  1924 tty2     Ss+  17:34   0:00 /sbin/agetty --noclear tty2 linux\n"
"root        88  0.0  0.1  12656  1764 tty3     Ss+  17:34   0:00 /sbin/agetty --noclear tty3 linux\n"
"root        89  0.0  0.1  12656  1908 tty4     Ss+  17:34   0:00 /sbin/agetty --noclear tty4 linux\n"
"root        90  0.0  0.1  63300  2944 tty1     Ss   17:34   0:00 /bin/login --     \n"
"root       117  0.0  0.2  21828  3668 tty1     S    17:35   0:00  \\_ -bash\n"
"root       268  0.0  0.1  19088  2572 tty1     R+   17:39   0:00      \\_ ps auxfw\n"
"root        91  0.0  0.1  14228  2356 console  Ss+  17:34   0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102\n"
"root       197  0.0  0.4  25384  7640 ?        Ss   17:38   0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e\n"
"root       266  0.0  0.1  12656  1840 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty5 linux\n"
"root       267  0.0  0.1  12656  1928 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty6 linux\n"
"root@testlxc:~# </computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "We are now in the container; our access to the processes is restricted to only those started from the container itself, and our access to the filesystem is similarly restricted to the dedicated subset of the full filesystem (<filename>/var/lib/lxc/testlxc/rootfs</filename>). We can exit the console with <keycombo action=\"simul\"><keycap>Control</keycap> <keycap>a</keycap></keycombo> <keycombo><keycap>q</keycap></keycombo>."
msgstr "これでコンテナの中に入りました。プロセスへのアクセスはコンテナ自身によって開始されたものだけに制限されていることがわかります。同様に、ファイルシステムへのアクセスも testlxc コンテナ専用に割り当てられた完全なファイルシステムの一部分 (<filename>/var/lib/lxc/testlxc/rootfs</filename>) に制限されています。コンソールを終了するには <keycombo action=\"simul\"><keycap>Control</keycap> <keycap>a</keycap></keycombo> <keycombo><keycap>q</keycap></keycombo> を使います。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Note that we ran the container as a background process, thanks to the <option>--daemon</option> option of <command>lxc-start</command>. We can interrupt the container with a command such as <command>lxc-stop --name=testlxc</command>."
msgstr "<command>lxc-start</command> に <option>--daemon</option> オプションを渡したおかげで、コンテナがバックグラウンドプロセスとして実行されていることに注意してください。コンテナを中止するには <command>lxc-stop --name=testlxc</command> などのコマンドを使います。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The <emphasis role=\"pkg\">lxc</emphasis> package contains an initialization script that can automatically start one or several containers when the host boots (it relies on <command>lxc-autostart</command> which starts containers whose <literal>lxc.start.auto</literal> option is set to 1). Finer-grained control of the startup order is possible with <literal>lxc.start.order</literal> and <literal>lxc.group</literal>: by default, the initialization script first starts containers which are part of the <literal>onboot</literal> group and then the containers which are not part of any group. In both cases, the order within a group is defined by the <literal>lxc.start.order</literal> option."
msgstr "<emphasis role=\"pkg\">lxc</emphasis> パッケージには、ホストの起動時に自動的に 1 つまたは複数のコンテナを開始するための初期化スクリプトが含まれます (この初期化スクリプトは <literal>lxc.start.auto</literal> オプションが 1 に設定されているコンテナを起動する <command>lxc-autostart</command> に依存しています)。起動順序を非常に細かく制御するには <literal>lxc.start.order</literal> と <literal>lxc.group</literal> を使います。デフォルトの場合、初期化スクリプトは <literal>onboot</literal> グループに所属するコンテナを起動し、その後いかなるグループにも所属しないコンテナを起動します。どちらの場合も、グループ内の起動順序を制御するには <literal>lxc.start.order</literal> オプションを使います。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>GOING FURTHER</emphasis> Mass virtualization"
msgstr "<emphasis>GOING FURTHER</emphasis> 大量の仮想化"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Since LXC is a very lightweight isolation system, it can be particularly adapted to massive hosting of virtual servers. The network configuration will probably be a bit more advanced than what we described above, but the “rich” configuration using <literal>tap</literal> and <literal>veth</literal> interfaces should be enough in many cases."
msgstr "LXC は非常に軽量の隔離システムですから、LXC を使って仮想サーバを大量にホストすることが可能です。この場合のネットワーク設定は上に述べた物よりも少し高度なものになるかもしれませんが、多くの場合 <literal>tap</literal> と <literal>veth</literal> インターフェースを用いた「ぜいたくな」設定を使えば事足ります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "It may also make sense to share part of the filesystem, such as the <filename>/usr</filename> and <filename>/lib</filename> subtrees, so as to avoid duplicating the software that may need to be common to several containers. This will usually be achieved with <literal>lxc.mount.entry</literal> entries in the containers configuration file. An interesting side-effect is that the processes will then use less physical memory, since the kernel is able to detect that the programs are shared. The marginal cost of one extra container can then be reduced to the disk space dedicated to its specific data, and a few extra processes that the kernel must schedule and manage."
msgstr "ファイルシステムの一部 (たとえば <filename>/usr</filename> と <filename>/lib</filename> などのサブツリー) を共有することは合理的です。こうすることで、複数のコンテナで共通に必要なソフトウェアを複製することを避けることが可能です。通常これを設定するには、コンテナ設定ファイルに含まれる <literal>lxc.mount.entry</literal> エントリを使います。さらに興味深い副作用として、より少ない物理メモリでプロセスを動かすことが可能になります。なぜなら、カーネルはプログラムが共有されていることを検出できるからです。このことにより 1 つのコンテナを追加するためのコストをコンテナに特有のデータに割り当てられたディスク領域と、カーネルがスケジュールと管理に使ういくつかの追加的プロセスだけに減らすことが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "We haven't described all the available options, of course; more comprehensive information can be obtained from the <citerefentry> <refentrytitle>lxc</refentrytitle> <manvolnum>7</manvolnum> </citerefentry> and <citerefentry> <refentrytitle>lxc.container.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> manual pages and the ones they reference."
msgstr "もちろん、ここではすべての利用できるオプションを説明していません。このため、より広範囲におよぶ情報を入手するには、<citerefentry> <refentrytitle>lxc</refentrytitle> <manvolnum>7</manvolnum> </citerefentry> と <citerefentry> <refentrytitle>lxc.container.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> マニュアルページおよびこれらのマニュアルページから参照されている文書を参照してください。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Virtualization with KVM"
msgstr "KVM を使った仮想化"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary>KVM</primary>"
msgstr "<primary>KVM</primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "KVM, which stands for <emphasis>Kernel-based Virtual Machine</emphasis>, is first and foremost a kernel module providing most of the infrastructure that can be used by a virtualizer, but it is not a virtualizer by itself. Actual control for the virtualization is handled by a QEMU-based application. Don't worry if this section mentions <command>qemu-*</command> commands: it is still about KVM."
msgstr "KVM は <emphasis>Kernel-based Virtual Machine</emphasis> を意味しており、仮想化システムの使うほとんどの基礎構造を提供する最初で最高のカーネルモジュールです。しかしながら、LVM 自身は仮想化システムではありません。仮想化の実際の制御を行うには QEMU に基づくアプリケーションを使います。この節で <command>qemu-*</command> コマンドがあっても心配しないでください。なぜならこのコマンドは KVM に関連するものだからです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Unlike other virtualization systems, KVM was merged into the Linux kernel right from the start. Its developers chose to take advantage of the processor instruction sets dedicated to virtualization (Intel-VT and AMD-V), which keeps KVM lightweight, elegant and not resource-hungry. The counterpart, of course, is that KVM doesn't work on any computer but only on those with appropriate processors. For x86-based computers, you can verify that you have such a processor by looking for “vmx” or “svm” in the CPU flags listed in <filename>/proc/cpuinfo</filename>."
msgstr "他の仮想化システムと異なり、KVM は最初から Linux カーネルにマージされていました。KVM の開発者はプロセッサが備える仮想化専用命令セット (Intel-VT と AMD-V) を有効活用することを選びました。仮想化専用命令セットを活用することで、KVM は軽量で簡潔でリソースを大量に消費しないものになっています。もちろんその代償として KVM にも欠点があります。それはすべてのコンピュータが KVM を動かせるわけではなく、適切なプロセッサを備えたコンピュータでなければ KVM を動かせないという点です。x86 ベースのコンピュータで <filename>/proc/cpuinfo</filename> 内の CPU フラグに「vmx」または「svm」が含まれている場合、そのプロセッサは KVM を動かすことができることを意味します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "With Red Hat actively supporting its development, KVM has more or less become the reference for Linux virtualization."
msgstr "Red Hat が KVM の開発を活発に支援したことで、KVM は事実上 Linux 仮想化の基準点になりました。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary><command>virt-install</command></primary>"
msgstr "<primary><command>virt-install</command></primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Unlike such tools as VirtualBox, KVM itself doesn't include any user-interface for creating and managing virtual machines. The <emphasis role=\"pkg\">qemu-kvm</emphasis> package only provides an executable able to start a virtual machine, as well as an initialization script that loads the appropriate kernel modules."
msgstr "VirtualBox などのツールと異なり、KVM は仮想マシンを作成管理するためのユーザインターフェースを含みません。仮想マシンを開始することが可能な実行ファイルおよび適切なカーネルモジュールを読み込むための初期化スクリプトを含むパッケージが <emphasis role=\"pkg\">qemu-kvm</emphasis> パッケージです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary>libvirt</primary>"
msgstr "<primary>libvirt</primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virt-manager</emphasis></primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Fortunately, Red Hat also provides another set of tools to address that problem, by developing the <emphasis>libvirt</emphasis> library and the associated <emphasis>virtual machine manager</emphasis> tools. libvirt allows managing virtual machines in a uniform way, independently of the virtualization system involved behind the scenes (it currently supports QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare and UML). <command>virtual-manager</command> is a graphical interface that uses libvirt to create and manage virtual machines."
msgstr "幸いなことに、Red Hat は <emphasis>libvirt</emphasis> ライブラリおよび関連する<emphasis>仮想マシンマネージャ</emphasis>ツールを開発することで、この問題に対処するためのツールを提供しています。libvirt により仮想マシンを管理する方法が統一され、仮想マシンの管理方法が裏で動く仮想システムに依存しなくなります (libvirt は現在 QEMU、KVM、Xen、LXC、OpenVZ、VirtualBox、VMWare、UML をサポートしています)。<command>virtual-manager</command> は仮想マシンを作成管理するために libvirt を使うグラフィルインターフェースです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"
msgstr "<primary><emphasis role=\"pkg\">virtinst</emphasis></primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "We first install the required packages, with <command>apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</command>. <emphasis role=\"pkg\">libvirt-bin</emphasis> provides the <command>libvirtd</command> daemon, which allows (potentially remote) management of the virtual machines running of the host, and starts the required VMs when the host boots. In addition, this package provides the <command>virsh</command> command-line tool, which allows controlling the <command>libvirtd</command>-managed machines."
msgstr "最初に <command>apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</command> を使って、必要なパッケージをインストールします。<emphasis role=\"pkg\">libvirt-bin</emphasis> には、<command>libvirtd</command> デーモンが含まれます。<command>libvirtd</command> デーモンを使うことでホストで実行されている仮想マシンを (潜在的にリモートで) 管理したり、ホスト起動時に要求された VM を開始したりすることが可能です。加えて、<emphasis role=\"pkg\">libvirt-bin</emphasis> パッケージは <command>virsh</command> コマンドラインツールを提供します。<command>virsh</command> を使うことで、<command>libvirtd</command> の管理するマシンを操作することが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The <emphasis role=\"pkg\">virtinst</emphasis> package provides <command>virt-install</command>, which allows creating virtual machines from the command line. Finally, <emphasis role=\"pkg\">virt-viewer</emphasis> allows accessing a VM's graphical console."
msgstr "<emphasis role=\"pkg\">virtinst</emphasis> パッケージには <command>virt-install</command> コマンドが含まれます。<command>virt-install</command> を使うことで、コマンドラインから仮想マシンを作成することが可能になります。最後に、<emphasis role=\"pkg\">virt-viewer</emphasis> を使うことで、仮想マシンのグラフィカルコンソールにアクセスすることが可能になります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Just as in Xen and LXC, the most frequent network configuration involves a bridge grouping the network interfaces of the virtual machines (see <xref linkend=\"sect.lxc.network\" />)."
msgstr "Xen や LXC と同様に、最もよく使われるネットワーク設定は仮想マシンのネットワークインターフェースをグループ化するブリッジです (<xref linkend=\"sect.lxc.network\" />を参照してください)。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Alternatively, and in the default configuration provided by KVM, the virtual machine is assigned a private address (in the 192.168.122.0/24 range), and NAT is set up so that the VM can access the outside network."
msgstr "ネットワーク設定には別の方法もあります。KVM の提供するデフォルト設定の中では、仮想マシンに (192.168.122.0/24 の範囲内に) プライベートアドレスが割り当てられており、さらに NAT が設定されています。この設定により仮想マシンは外部ネットワークにアクセスすることが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The rest of this section assumes that the host has an <literal>eth0</literal> physical interface and a <literal>br0</literal> bridge, and that the former is connected to the latter."
msgstr "この節の残りでは、ホストが <literal>eth0</literal> 物理インターフェースと <literal>br0</literal> ブリッジを備え、<literal>eth0</literal> が <literal>br0</literal> に接続されていることを仮定します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Installation with <command>virt-install</command>"
msgstr "<command>virt-install</command> を使ったインストール"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Creating a virtual machine is very similar to installing a normal system, except that the virtual machine's characteristics are described in a seemingly endless command line."
msgstr "仮想マシンの作成は普通のシステムをインストールするのとよく似ています。違いは、仮想マシンの性質をコマンドラインから非常に長々と指定する点です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Practically speaking, this means we will use the Debian installer, by booting the virtual machine on a virtual DVD-ROM drive that maps to a Debian DVD image stored on the host system. The VM will export its graphical console over the VNC protocol (see <xref linkend=\"sect.remote-desktops\" /> for details), which will allow us to control the installation process."
msgstr "具体的に言えば、これはホストシステムに保存された Debian DVD イメージを挿入された仮想 DVD-ROM ドライブから仮想マシンを起動することにより Debian インストーラを使うことを意味します。仮想マシンは VNC プロトコル (詳しくは<xref linkend=\"sect.remote-desktops\" />を参照してください) を介してグラフィカルコンソールに表示されます。これによりインストール作業を操作することが可能になります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "We first need to tell libvirtd where to store the disk images, unless the default location (<filename>/var/lib/libvirt/images/</filename>) is fine."
msgstr "最初にディスクイメージの保存先を libvirtd に伝える必要があります。デフォルトの保存先 (<filename>/var/lib/libvirt/images/</filename>) でも構わないならばこれは必要ありません。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>mkdir /srv/kvm</userinput>\n"
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm</userinput>\n"
"<computeroutput>Pool srv-kvm created\n"
"\n"
"root@mirwiz:~# </computeroutput>"
msgstr ""
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>mkdir /srv/kvm</userinput>\n"
"<computeroutput>root@mirwiz:~# </computeroutput><userinput>virsh pool-create-as srv-kvm dir --target /srv/kvm</userinput>\n"
"<computeroutput>Pool srv-kvm created\n"
"\n"
"root@mirwiz:~# </computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>TIP</emphasis> Add your user to the libvirt group"
msgstr "<emphasis>TIP</emphasis> libvirt グループに対するユーザの追加"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "All samples in this section assume that you are running commands as root. Effectively, if you want to control a local libvirt daemon, you need either to be root or to be a member of the <literal>libvirt</literal> group (which is not the case by default). Thus if you want to avoid using root rights too often, you can add yoursel to the <literal>libvirt</literal> group and run the various commands under your user identity."
msgstr "この節で挙げたすべての例では、root がコマンドを実行しています。実際、あるユーザがローカルの libvirt デーモンを操作するには、root になるか <literal>libvirt</literal> グループのメンバーになって (デフォルト状態では <literal>libvirt</literal> グループはユーザの初期参加グループに設定されていません) コマンドを実行する必要があります。このユーザに root 権限を頻繁に使わせることを避けたい場合、そのユーザを <literal>libvirt</literal> グループに追加して、本人の権限でさまざまなコマンドを実行するように設定することが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Let us now start the installation process for the virtual machine, and have a closer look at <command>virt-install</command>'s most important options. This command registers the virtual machine and its parameters in libvirtd, then starts it so that its installation can proceed."
msgstr "それでは仮想マシンのインストール作業を開始し、<command>virt-install</command> の最も重要なオプションを詳細に見て行きましょう。<command>virt-install</command> は仮想マシンとそのパラメータを libvirtd に登録し、インストールを進めるために仮想マシンを開始します。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id=\"virtinst.connect\"></co>\n"
"               --virt-type kvm           <co id=\"virtinst.type\"></co>\n"
"               --name testkvm            <co id=\"virtinst.name\"></co>\n"
"               --ram 1024                <co id=\"virtinst.ram\"></co>\n"
"               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <co id=\"virtinst.disk\"></co>\n"
"               --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso  <co id=\"virtinst.cdrom\"></co>\n"
"               --network bridge=br0      <co id=\"virtinst.network\"></co>\n"
"               --vnc                     <co id=\"virtinst.vnc\"></co>\n"
"               --os-type linux           <co id=\"virtinst.os\"></co>\n"
"               --os-variant debianwheezy\n"
"</userinput><computeroutput>\n"
"Starting install...\n"
"Allocating 'testkvm.qcow'             |  10 GB     00:00\n"
"Creating domain...                    |    0 B     00:00\n"
"Guest installation complete... restarting guest.\n"
"</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virt-install --connect qemu:///system  <co id=\"virtinst.connect\"></co>\n"
"               --virt-type kvm           <co id=\"virtinst.type\"></co>\n"
"               --name testkvm            <co id=\"virtinst.name\"></co>\n"
"               --ram 1024                <co id=\"virtinst.ram\"></co>\n"
"               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <co id=\"virtinst.disk\"></co>\n"
"               --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso  <co id=\"virtinst.cdrom\"></co>\n"
"               --network bridge=br0      <co id=\"virtinst.network\"></co>\n"
"               --vnc                     <co id=\"virtinst.vnc\"></co>\n"
"               --os-type linux           <co id=\"virtinst.os\"></co>\n"
"               --os-variant debianwheezy\n"
"</userinput><computeroutput>\n"
"Starting install...\n"
"Allocating 'testkvm.qcow'             |  10 GB     00:00\n"
"Creating domain...                    |    0 B     00:00\n"
"Guest installation complete... restarting guest.\n"
"</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "The <literal>--connect</literal> option specifies the “hypervisor” to use. Its form is that of an URL containing a virtualization system (<literal>xen://</literal>, <literal>qemu://</literal>, <literal>lxc://</literal>, <literal>openvz://</literal>, <literal>vbox://</literal>, and so on) and the machine that should host the VM (this can be left empty in the case of the local host). In addition to that, and in the QEMU/KVM case, each user can manage virtual machines working with restricted permissions, and the URL path allows differentiating “system” machines (<literal>/system</literal>) from others (<literal>/session</literal>)."
msgstr "<literal>--connect</literal> オプションは使用する「ハイパーバイザ」を指定します。これは仮想システムを表す URL (<literal>xen://</literal>、<literal>qemu://</literal>、<literal>lxc://</literal>、<literal>openvz://</literal>、<literal>vbox://</literal> など) と VM をホストするマシン (ローカルホストの場合、空でも構いません) の形をしています。QEMU/KVM の場合、これに加えて各ユーザは制限されたパーミッションで稼働する仮想マシンを管理できます。この場合 URL パスは「システム」マシン (<literal>/system</literal>) かその他 (<literal>/session</literal>) かで識別されます。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "Since KVM is managed the same way as QEMU, the <literal>--virt-type kvm</literal> allows specifying the use of KVM even though the URL looks like QEMU."
msgstr "<literal>--virt-type kvm</literal> を指定することで KVM を使うことが可能です。<literal>--connect</literal> で指定した URL を一見すると QEMU が使われるように見えますが、これは KVM は QEMU と同じ方法で管理されているためです。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "The <literal>--name</literal> option defines a (unique) name for the virtual machine."
msgstr "<literal>--name</literal> オプションは仮想マシンの (一意的な) 名前を定義します。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "The <literal>--ram</literal> option allows specifying the amount of RAM (in MB) to allocate for the virtual machine."
msgstr "<literal>--ram</literal> オプションは仮想マシンに割り当てる RAM の量 (MB 単位) を指定します。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "The <literal>--disk</literal> specifies the location of the image file that is to represent our virtual machine's hard disk; that file is created, unless present, with a size (in GB) specified by the <literal>size</literal> parameter. The <literal>format</literal> parameter allows choosing among several ways of storing the image file. The default format (<literal>raw</literal>) is a single file exactly matching the disk's size and contents. We picked a more advanced format here, that is specific to QEMU and allows starting with a small file that only grows when the virtual machine starts actually using space."
msgstr "<literal>--disk</literal> オプションは仮想マシンのハードディスクとして利用するイメージファイルの場所を指定します。このファイルが存在しなければ、<literal>size</literal> パラメータで指定されたサイズ (GB 単位) のイメージファイルが作成されます。<literal>format</literal> パラメータはイメージファイルを保存するさまざまな方法を選択します。デフォルトフォーマット (<literal>raw</literal>) はディスクサイズと内容が全く同じ単一ファイルです。ここではより先進的なフォーマット qcow2 を選びました。qcow2 は QEMU 専用のフォーマットです。qcow2 フォーマットのファイルは作成時のサイズは小さいのですが、仮想マシンが領域を実際に利用することになった時にサイズが増加します。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "The <literal>--cdrom</literal> option is used to indicate where to find the optical disk to use for installation. The path can be either a local path for an ISO file, an URL where the file can be obtained, or the device file of a physical CD-ROM drive (i.e. <literal>/dev/cdrom</literal>)."
msgstr "<literal>--cdrom</literal> オプションはインストール時に利用する光学ディスクの場所を指定するために使われます。場所には ISO ファイルのローカルパス、ファイル取得先の URL、物理 CD-ROM ドライブのデバイスファイル (例 <literal>/dev/cdrom</literal>) のどれか 1 つを使うことが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "The <literal>--network</literal> specifies how the virtual network card integrates in the host's network configuration. The default behavior (which we explicitly forced in our example) is to integrate it into any pre-existing network bridge. If no such bridge exists, the virtual machine will only reach the physical network through NAT, so it gets an address in a private subnet range (192.168.122.0/24)."
msgstr "<literal>--network</literal> オプションはホストネットワーク設定の中に仮想ネットワークを統合する方法を指定します。デフォルトは既存のネットワークブリッジに仮想ネットワークを統合する方法です (例では明示的にこの挙動を指定しています)。指定したブリッジが存在しない場合、仮想マシンが到達できるネットワークは NAT を介した物理ネットワークだけに限定されるので、仮想マシンはプライベートサブネット範囲 (192.168.122.0/24) に含まれるアドレスを割り当てられます。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "<literal>--vnc</literal> states that the graphical console should be made available using VNC. The default behavior for the associated VNC server is to only listen on the local interface; if the VNC client is to be run on a different host, establishing the connection will require setting up an SSH tunnel (see <xref linkend=\"sect.ssh-port-forwarding\" />). Alternatively, the <literal>--vnclisten=0.0.0.0</literal> can be used so that the VNC server is accessible from all interfaces; note that if you do that, you really should design your firewall accordingly."
msgstr "<literal>--vnc</literal> は VNC を使ってグラフィカルコンソールを利用できるようにすることを意味します。VNC サーバに対するデフォルトの挙動を使った場合、ローカルインターフェースだけがリッスンされます。さらに仮想マシンを操作する VNC クライアントを別のホスト上で実行する場合、VNC 接続を確立するには SSH トンネルを設定する必要があります (<xref linkend=\"sect.ssh-port-forwarding\" />を参照してください)。別の方法として、VNC サーバをすべてのインターフェースを介して利用できるようにするために、<literal>--vnclisten=0.0.0.0</literal> を使うことも可能です。しかしこの方針を取る場合、ファイアウォールを適切に設計するべきという点に注意してください。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "The <literal>--os-type</literal> and <literal>--os-variant</literal> options allow optimizing a few parameters of the virtual machine, based on some of the known features of the operating system mentioned there."
msgstr "<literal>--os-type</literal> と <literal>--os-variant</literal> オプションは、指定されたオペレーティングシステムの備える既知の機能に基づいて、仮想マシンのいくつかのパラメータを最適化するためのものです。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "At this point, the virtual machine is running, and we need to connect to the graphical console to proceed with the installation process. If the previous operation was run from a graphical desktop environment, this connection should be automatically started. If not, or if we operate remotely, <command>virt-viewer</command> can be run from any graphical environment to open the graphical console (note that the root password of the remote host is asked twice because the operation requires 2 SSH connections):"
msgstr "<command>virt-install</command> を実行した時点で仮想マシンが実行されます。インストール作業に進むためには、グラフィカルコンソールに接続する必要があります。上の操作をグラフィカルデスクトップ環境から行った場合、自動的に接続が開始されます。そうでない場合、グラフィカルコンソールを開くために <command>virt-viewer</command> を任意のグラフィカル環境から実行します (この時にリモートホストの root パスワードが 2 回尋ねられる点に注意してください。なぜなら、この操作には 2 つの SSH 接続を必要とするからです)。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput>$ </computeroutput><userinput>virt-viewer --connect qemu+ssh://root@<replaceable>server</replaceable>/system testkvm\n"
"</userinput><computeroutput>root@server's password: \n"
"root@server's password: </computeroutput>"
msgstr ""
"<computeroutput>$ </computeroutput><userinput>virt-viewer --connect qemu+ssh://root@<replaceable>server</replaceable>/system testkvm\n"
"</userinput><computeroutput>root@server's password: \n"
"root@server's password: </computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "When the installation process ends, the virtual machine is restarted, now ready for use."
msgstr "インストール作業が終了したら、仮想マシンが再起動されます。これで仮想マシンを利用する準備が整いました。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "Managing Machines with <command>virsh</command>"
msgstr "<command>virsh</command> を使ったマシンの管理"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "<primary><command>virsh</command></primary>"
msgstr "<primary><command>virsh</command></primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "Now that the installation is done, let us see how to handle the available virtual machines. The first thing to try is to ask <command>libvirtd</command> for the list of the virtual machines it manages:"
msgstr "これでインストールが終了しました。利用できる仮想マシンを取り扱う方法に移りましょう。最初に <command>virsh</command> を使って <command>libvirtd</command> が管理している仮想マシンのリストを確認します。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all\n"
" Id Name                 State\n"
"----------------------------------\n"
"  - testkvm              shut off\n"
"</userinput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system list --all\n"
"</userinput><computeroutput> Id Name                 State\n"
"----------------------------------\n"
"  - testkvm              shut off\n"
"</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Let's start our test virtual machine:"
msgstr "それではテスト用仮想マシンを起動しましょう。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system start testkvm\n"
"</userinput><computeroutput>Domain testkvm started</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system start testkvm\n"
"</userinput><computeroutput>Domain testkvm started</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "We can now get the connection instructions for the graphical console (the returned VNC display can be given as parameter to <command>vncviewer</command>):"
msgstr "そして、グラフィカルコンソールへの接続命令を出します (接続する VNC 画面を <command>vncviewer</command> へのパラメータの形で指定することが可能です)。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm\n"
"</userinput><computeroutput>:0</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>virsh -c qemu:///system vncdisplay testkvm\n"
"</userinput><computeroutput>:0</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "Other available <command>virsh</command> subcommands include:"
msgstr "その他の利用できる <command>virsh</command> サブコマンドには以下のものがあります。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "<literal>reboot</literal> to restart a virtual machine;"
msgstr "<literal>reboot</literal>。仮想マシンを再起動します。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "<literal>shutdown</literal> to trigger a clean shutdown;"
msgstr "<literal>shutdown</literal>。仮想マシンを正常にシャットダウンします。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "<literal>destroy</literal>, to stop it brutally;"
msgstr "<literal>destroy</literal>。仮想マシンを無理やり停止します。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "<literal>suspend</literal> to pause it;"
msgstr "<literal>suspend</literal>。仮想マシンを一時停止します。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "<literal>resume</literal> to unpause it;"
msgstr "<literal>resume</literal>。一時停止された仮想マシンを再開します。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "<literal>autostart</literal> to enable (or disable, with the <literal>--disable</literal> option) starting the virtual machine automatically when the host starts;"
msgstr "<literal>autostart</literal>。ホスト起動時にこの仮想マシンを自動的に起動することを有効化します (または <literal>--disable</literal> オプションを付けて無効化します)。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "<literal>undefine</literal> to remove all traces of the virtual machine from <command>libvirtd</command>."
msgstr "<literal>undefine</literal>。仮想マシンのすべての痕跡を <command>libvirtd</command> から削除します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "All these subcommands take a virtual machine identifier as a parameter."
msgstr "ここに挙げたすべてのサブコマンドは仮想マシン識別子をパラメータとして受け取ります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Installing an RPM based system in Debian with yum"
msgstr "yum を使い RPM に基づくシステムを Debian の中にインストールする"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "If the virtual machine is meant to run a Debian (or one of its derivatives), the system can be initialized with <command>debootstrap</command>, as described above. But if the virtual machine is to be installed with an RPM-based system (such as Fedora, CentOS or Scientific Linux), the setup will need to be done using the <command>yum</command> utility (available in the package of the same name)."
msgstr "仮想マシンが Debian (または Debian 派生物) を実行することを意図している場合、上で述べた通り <command>debootstrap</command> を使ってシステムを初期化することが可能です。しかし、仮想マシンに RPM に基づくシステム (Fedora、CentOS、Scientific Linux など) をインストールする場合、<command>yum</command> ユーティリティ (同名のパッケージに含まれます) を使ってシステムをセットアップする必要があります。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "The procedure requires using <command>rpm</command> to extract an initial set of files, including notably <command>yum</command> configuration files, and then calling <command>yum</command> to extract the remaining set of packages. But since we call <command>yum</command> from outside the chroot, we need to make some temporary changes. In the sample below, the target chroot is <filename>/srv/centos</filename>."
msgstr "RPM に基づくシステムをインストールする際には、特に <command>yum</command> 設定ファイルなどのファイルの初期セットを展開するために <command>rpm</command> を使い、その後パッケージの残りのセットを展開するために <command>yum</command> を呼び出す必要があります。しかし、chroot の外から <command>yum</command> を呼び出しているため、一時的な修正が必要です。以下に載せた例では、対象の chroot 先は <filename>/srv/centos</filename> です。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid ""
"<computeroutput># </computeroutput><userinput>rootdir=\"/srv/centos\"\n"
"</userinput><computeroutput># </computeroutput><userinput>mkdir -p \"$rootdir\" /etc/rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>echo \"%_dbpath /var/lib/rpm\" &gt; /etc/rpm/macros.dbpath\n"
"</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root \"$rootdir\" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm\n"
"</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!\n"
"rpm: However assuming you know what you are doing...\n"
"warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput><computeroutput># </computeroutput><userinput>yum --assumeyes --installroot $rootdir groupinstall core\n"
"</userinput><computeroutput>[...]\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>rootdir=\"/srv/centos\"\n"
"</userinput><computeroutput># </computeroutput><userinput>mkdir -p \"$rootdir\" /etc/rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>echo \"%_dbpath /var/lib/rpm\" &gt; /etc/rpm/macros.dbpath\n"
"</userinput><computeroutput># </computeroutput><userinput>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm\n"
"</userinput><computeroutput># </computeroutput><userinput>rpm --nodeps --root \"$rootdir\" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm\n"
"</userinput><computeroutput>rpm: RPM should not be used directly install RPM packages, use Alien instead!\n"
"rpm: However assuming you know what you are doing...\n"
"warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput><computeroutput># </computeroutput><userinput>yum --assumeyes --installroot $rootdir groupinstall core\n"
"</userinput><computeroutput>[...]\n"
"# </computeroutput><userinput>sed -i -e \"s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g\" $rootdir/etc/yum.repos.d/*.repo\n"
"</userinput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Automated Installation"
msgstr "自動インストール"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary>deployment</primary>"
msgstr "<primary>配備</primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary>installation</primary><secondary>automated installation</secondary>"
msgstr "<primary>インストール</primary><secondary>自動インストール</secondary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The Falcot Corp administrators, like many administrators of large IT services, need tools to install (or reinstall) quickly, and automatically if possible, their new machines."
msgstr "巨大な IT サービスの管理者と同様に Falcot Corp の管理者もまた、新しいマシンへシステムを素早く (可能であれば自動的に) インストール (または再インストール) するツールを必要としています。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "These requirements can be met by a wide range of solutions. On the one hand, generic tools such as SystemImager handle this by creating an image based on a template machine, then deploy that image to the target systems; at the other end of the spectrum, the standard Debian installer can be preseeded with a configuration file giving the answers to the questions asked during the installation process. As a sort of middle ground, a hybrid tool such as FAI (<emphasis>Fully Automatic Installer</emphasis>) installs machines using the packaging system, but it also uses its own infrastructure for tasks that are more specific to massive deployments (such as starting, partitioning, configuration and so on)."
msgstr "自動インストールの要求に応えるためのさまざまな解決策が存在します。一方では、SystemImager などの一般的なツールが存在します。こちらの解決策ではテンプレートマシンに基づくイメージを事前に準備し、そのイメージを目標のシステムで展開します。他方では、標準的な Debian インストーラが存在します。こちらの解決策ではインストール作業中に尋ねられる質問の回答を含めた設定ファイルを事前に準備し、この設定ファイルに基づいて目標のシステムを設定します。両者の折衷案として、FAI (<emphasis>Fully Automatic Installer</emphasis>) などのハイブリッドツールが存在します。こちらの解決策ではパッケージングシステムでシステムをインストールし、自分自身の機能を使って大規模な配備に特有のタスク (起動、パーティショニング、設定など) をこなします。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Each of these solutions has its pros and cons: SystemImager works independently from any particular packaging system, which allows it to manage large sets of machines using several distinct Linux distributions. It also includes an update system that doesn't require a reinstallation, but this update system can only be reliable if the machines are not modified independently; in other words, the user must not update any software on their own, or install any other software. Similarly, security updates must not be automated, because they have to go through the centralized reference image maintained by SystemImager. This solution also requires the target machines to be homogeneous, otherwise many different images would have to be kept and managed (an i386 image won't fit on a powerpc machine, and so on)."
msgstr "これらの解決策には、利点と欠点があります。たとえば SystemImager は特定のパッケージングシステムに依存しません。このことにより、複数の Linux ディストリビューションを使う数多くのマシン群を管理することが可能です。SystemImager には再インストール不要の更新システムが含まれていますが、この更新システムを信頼できるのは各マシンは個別に変更されないという仮定が満足される場合だけです。さらに言い換えれば、ユーザは自分のマシンにインストールされたソフトウェアを更新してはいけませんし、他のソフトウェアをインストールしてもいけません。同様に、セキュリティ更新も自動的に行ってはいけません。なぜなら、セキュリティ更新は SystemImager がメンテナンスする中央集権化された基準イメージによって提供されるからです。また SystemImager による解決策では、管理される側のマシンの種類が同じである必要があります。異種マシンを管理する場合、多くの異なるイメージをメンテナンスおよび管理する必要があります (i386 用のイメージを powerpc マシンに使うことはできません)。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "On the other hand, an automated installation using debian-installer can adapt to the specifics of each machine: the installer will fetch the appropriate kernel and software packages from the relevant repositories, detect available hardware, partition the whole hard disk to take advantage of all the available space, install the corresponding Debian system, and set up an appropriate bootloader. However, the standard installer will only install standard Debian versions, with the base system and a set of pre-selected “tasks”; this precludes installing a particular system with non-packaged applications. Fulfilling this particular need requires customizing the installer… Fortunately, the installer is very modular, and there are tools to automate most of the work required for this customization, most importantly simple-CDD (CDD being an acronym for <emphasis>Custom Debian Derivative</emphasis>). Even the simple-CDD solution, however, only handles initial installations; this is usually not a problem since the APT tools allow efficient deployment of updates later on."
msgstr "逆に、debian-installer を使ってインストールを自動化する場合、マシンごとの違いに適合したシステムをインストールすることが可能です。具体的に言えば、インストーラは対応するリポジトリから適切なカーネルとソフトウェアパッケージを取得し、利用できるハードウェアを検出し、利用できる領域全体を活かしてハードディスク全体をパーティショニングし、対応する Debian システムをインストールし、適切なブートローダを設定します。しかしながら、標準的なインストーラは基本システムと事前に選択された「tasks」を含む標準的な Debian バージョンをインストールするだけです。さらに、パッケージングされていないアプリケーションを備えた特製のシステムをインストールできません。この特別な必要性を満足させるにはインストーラのカスタマイズが必要です。幸いなことに、インストーラはとてもモジュール化されており、カスタマイズに必要なほとんどの作業を自動化するツールが存在します。最も重要なツールは simple-CDD です (CDD は <emphasis>Custom Debian Derivative</emphasis> の頭字語です)。しかしながら simple-CDD が取り扱うことが可能なのは最初のインストールだけです。しかしこれは通常問題になりません。なぜなら、APT ツールのおかげでインストール後の更新作業は効果的に行うことが可能だからです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "We will only give a rough overview of FAI, and skip SystemImager altogether (which is no longer in Debian), in order to focus more intently on debian-installer and simple-CDD, which are more interesting in a Debian-only context."
msgstr "ここでは FAI については大ざっぱな概要を説明し、(もはや Debian に含まれない) SystemImager については完全に省略し、Debian だけのシステムではより興味深い debian-installer と simple-CDD に特に注目します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Fully Automatic Installer (FAI)"
msgstr "Fully Automatic Installer (FAI)"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary>Fully Automatic Installer (FAI)</primary>"
msgstr "<primary>Fully Automatic Installer (FAI)</primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<foreignphrase>Fully Automatic Installer</foreignphrase> is probably the oldest automated deployment system for Debian, which explains its status as a reference; but its very flexible nature only just compensates for the complexity it involves."
msgstr "<foreignphrase>Fully Automatic Installer</foreignphrase> はおそらく最も古い Debian 用の自動配備システムで、基準としての地位を確立しています。しかしその一方で FAI の高い柔軟性は FAI の複雑性によって成し遂げられています。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "FAI requires a server system to store deployment information and allow target machines to boot from the network. This server requires the <emphasis role=\"pkg\">fai-server</emphasis> package (or <emphasis role=\"pkg\">fai-quickstart</emphasis>, which also brings the required elements for a standard configuration)."
msgstr "FAI には、配備情報を保存してネットワークから目標のマシンを起動することを可能にするために、サーバシステムが必要です。サーバシステムには、<emphasis role=\"pkg\">fai-server</emphasis> パッケージ (または <emphasis role=\"pkg\">fai-quickstart</emphasis>、これには、標準的な設定に必要な要素が含まれています) が必要です。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "FAI uses a specific approach for defining the various installable profiles. Instead of simply duplicating a reference installation, FAI is a full-fledged installer, fully configurable via a set of files and scripts stored on the server; the default location <filename>/srv/fai/config/</filename> is not automatically created, so the administrator needs to create it along with the relevant files. Most of the times, these files will be customized from the example files available in the documentation for the <emphasis role=\"pkg\">fai-doc</emphasis> package, more particularly the <filename>/usr/share/doc/fai-doc/examples/simple/</filename> directory."
msgstr "FAI は特殊なやり方で、インストールできるさまざまなプロファイルを定義します。FAI は基準となるインストール状態を単純に複製するのではありません。FAI は本格的なインストーラで、サーバに保存されているファイルとスクリプトを通じて完全に設定することが可能です。しかし、これらのファイルを保存するデフォルトの場所 <filename>/srv/fai/config/</filename> は自動的に作成されません。このため管理者は対応するファイルと一緒にこれも作成する必要があります。ほとんどの場合、これらのファイルは例ファイルからカスタマイズされます。このファイルは <emphasis role=\"pkg\">fai-doc</emphasis> パッケージの文書の中 (より具体的に言えば <filename>/usr/share/doc/fai-doc/examples/simple/</filename> ディレクトリの中) に含まれています。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "Once the profiles are defined, the <command>fai-setup</command> command generates the elements required to start an FAI installation; this mostly means preparing or updating a minimal system (NFS-root) used during installation. An alternative is to generate a dedicated boot CD with <command>fai-cd</command>."
msgstr "プロファイルを定義したら、<command>fai-setup</command> コマンドを使って FAI のインストールを開始するために必要な要素を生成します。そしてこれはインストール中に使われる最小限のシステム (NFS-root) の準備と更新を行うことを意味します。別の方法として、<command>fai-cd</command> を使って専用の CD を生成することも可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Creating all these configuration files requires some understanding of the way FAI works. A typical installation process is made of the following steps:"
msgstr "これらすべての設定ファイルを作成するには、FAI の動作方法を理解する必要があります。典型的なインストール作業は以下の順番で進みます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "fetching a kernel from the network, and booting it;"
msgstr "ネットワークからカーネルを取得して起動します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "mounting the root filesystem from NFS;"
msgstr "NFS でルートファイルシステムをマウントします。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "executing <command>/usr/sbin/fai</command>, which controls the rest of the process (the next steps are therefore initiated by this script);"
msgstr "インストール作業を制御する <command>/usr/sbin/fai</command> を実行します (<command>/usr/sbin/fai</command> が以降の各段階を初期化します)。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "copying the configuration space from the server into <filename>/fai/</filename>;"
msgstr "サーバから <filename>/fai/</filename> に設定領域をコピーします。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "running <command>fai-class</command>. The <filename>/fai/class/[0-9][0-9]*</filename> scripts are executed in turn, and return names of “classes” that apply to the machine being installed; this information will serve as a base for the following steps. This allows for some flexibility in defining the services to be installed and configured."
msgstr "<command>fai-class</command> を実行します。<filename>/fai/class/[0-9][0-9]*</filename> スクリプトが順番に実行され、インストールされるマシンに適用する「クラス」の名前が返されます。この情報は以降の各段階の基礎としての機能を果たします。これを使うことで、インストールおよび設定されるサービスを定義する際に幾らかの柔軟性を持たせることが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "fetching a number of configuration variables, depending on the relevant classes;"
msgstr "対応するクラスに基づき、設定変数を取得します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "partitioning the disks and formatting the partitions, based on information provided in <filename>/fai/disk_config/<replaceable>class</replaceable></filename>;"
msgstr "<filename>/fai/disk_config/<replaceable>class</replaceable></filename> で定義された情報に基づいて、ディスクのパーティショニングとパーティションのフォーマットを行います。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "mounting said partitions;"
msgstr "指定されたパーティションをマウントします。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "installing the base system;"
msgstr "基本システムをインストールします。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "preseeding the Debconf database with <command>fai-debconf</command>;"
msgstr "<command>fai-debconf</command> を使って Debconf データベースを事前配布します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "fetching the list of available packages for APT;"
msgstr "APT で利用できるパッケージのリストを取得します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "installing the packages listed in <filename>/fai/package_config/<replaceable>class</replaceable></filename>;"
msgstr "<filename>/fai/package_config/<replaceable>class</replaceable></filename> にリストされたパッケージをインストールします。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "executing the post-configuration scripts, <filename>/fai/scripts/<replaceable>class</replaceable>/[0-9][0-9]*</filename>;"
msgstr "設定後にスクリプト <filename>/fai/scripts/<replaceable>class</replaceable>/[0-9][0-9]*</filename> を実行します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "recording the installation logs, unmounting the partitions, and rebooting."
msgstr "インストールログを記録し、パーティションをアンマウントし、再起動します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Preseeding Debian-Installer"
msgstr "Debian-Installer の事前設定"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "<primary>preseed</primary>"
msgstr "<primary>preseed</primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary>preconfiguration</primary>"
msgstr "<primary>事前設定</primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "At the end of the day, the best tool to install Debian systems should logically be the official Debian installer. This is why, right from its inception, debian-installer has been designed for automated use, taking advantage of the infrastructure provided by <emphasis role=\"pkg\">debconf</emphasis>. The latter allows, on the one hand, to reduce the number of questions asked (hidden questions will use the provided default answer), and on the other hand, to provide the default answers separately, so that installation can be non-interactive. This last feature is known as <emphasis>preseeding</emphasis>."
msgstr "結局のところ、Debian システムをインストールする最良のツールは必然的に公式の Debian インストーラということになるでしょう。このため、当初から debian-installer は <emphasis role=\"pkg\">debconf</emphasis> の提供するインフラを活用して自動的に使えるように設計されています。<emphasis role=\"pkg\">debconf</emphasis> を使うことで、尋ねられる質問の数を減らしたり (隠された質問に対する回答はデフォルトが使われます)、質問に対する回答を個別に事前設定したりすることが可能です。このことにより、インストールを非対話的に進めることが可能です。質問に対する回答の事前設定機能は <emphasis>preseed</emphasis> として知られています。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>GOING FURTHER</emphasis> Debconf with a centralized database"
msgstr "<emphasis>GOING FURTHER</emphasis> 中央集権型データベースを用いる Debconf"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "<primary><command>debconf</command></primary>"
msgstr "<primary><command>debconf</command></primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
msgid "Preseeding allows to provide a set of answers to Debconf questions at installation time, but these answers are static and do not evolve as time passes. Since already-installed machines may need upgrading, and new answers may become required, the <filename>/etc/debconf.conf</filename> configuration file can be set up so that Debconf uses external data sources (such as an LDAP directory server, or a remote file accessed via NFS or Samba). Several external data sources can be defined at the same time, and they complement one another. The local database is still used (for read-write access), but the remote databases are usually restricted to reading. The <citerefentry><refentrytitle>debconf.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> manual page describes all the possibilities in detail (you need the <emphasis role=\"pkg\">debconf-doc</emphasis> package)."
msgstr "preseed を使うことで、インストール時に尋ねられる Debconf 質問に対する回答を事前に提供することが可能です。しかし、これらの回答は静的なもので、時間経過に従い変化しません。既にインストールされたマシンは更新が必要かもしれませんし、新しい回答が必要かもしれません。このため <filename>/etc/debconf.conf</filename> 設定ファイルから、Debconf が外部のデータソース (LDAP ディレクトリサーバ、NFS や Samba 経由でアクセスされるリモートファイル) を使うようにセットアップして、お互いに補完し合う複数の外部データソースを同時に定義することが可能です。とは言うものの、ローカルデータベースも使えます (読み書きアクセスできます)。これに対してリモートデータベースは読み込みのみに制限されていることが多いです。<citerefentry><refentrytitle>debconf.conf</refentrytitle> <manvolnum>5</manvolnum></citerefentry> マニュアルページではすべての可能性が詳細に説明されています (このマニュアルは <emphasis role=\"pkg\">debconf-doc</emphasis> パッケージに含まれます)。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Using a Preseed File"
msgstr "preseed ファイルの利用"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "There are several places where the installer can get a preseeding file:"
msgstr "インストーラが preseed ファイルを取得することが可能な場所にはいくつかあります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "in the initrd used to start the machine; in this case, preseeding happens at the very beginning of the installation, and all questions can be avoided. The file just needs to be called <filename>preseed.cfg</filename> and stored in the initrd root."
msgstr "マシンを開始するために使われる initrd の中。この場合、インストールの最初から preseed を行い、すべての質問を回避することが可能です。preseed ファイルの名前は必ず <filename>preseed.cfg</filename> にして initrd のルートに保存しなければいけません。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "on the boot media (CD or USB key); preseeding then happens as soon as the media is mounted, which means right after the questions about language and keyboard layout. The <literal>preseed/file</literal> boot parameter can be used to indicate the location of the preseeding file (for instance, <filename>/cdrom/preseed.cfg</filename> when the installation is done off a CD-ROM, or <filename>/hd-media/preseed.cfg</filename> in the USB-key case)."
msgstr "起動メディア (CD や USB メモリ) の中。メディアがマウントされた直後から preseed が始まります。これは言語とキーボードレイアウトに関する質問の直後を意味します。<literal>preseed/file</literal> 起動パラメータを使って preseed ファイルの場所を指定することが可能です (たとえば、インストールが CD-ROM から開始された場合は <filename>/cdrom/preseed.cfg</filename>、USB メモリから開始された場合は <filename>/hd-media/preseed.cfg</filename> などです)。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "from the network; preseeding then only happens after the network is (automatically) configured; the relevant boot parameter is then <literal>preseed/url=http://<replaceable>server</replaceable>/preseed.cfg</literal>."
msgstr "ネットワーク上。ネットワークが (自動的に) 設定された直後から preseed が始まります。対応する起動パラメータは <literal>preseed/url=http://<replaceable>server</replaceable>/preseed.cfg</literal> です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "At a glance, including the preseeding file in the initrd looks like the most interesting solution; however, it is rarely used in practice, because generating an installer initrd is rather complex. The other two solutions are much more common, especially since boot parameters provide another way to preseed the answers to the first questions of the installation process. The usual way to save the bother of typing these boot parameters by hand at each installation is to save them into the configuration for <command>isolinux</command> (in the CD-ROM case) or <command>syslinux</command> (USB key)."
msgstr "一見すると、preseed ファイルを initrd の中に含めることが最もうまい解決策のように見えます。しかし、実際のところこれはほとんど行われません。なぜなら、インストーラの initrd を生成することはかなり複雑だからです。その他の 2 種類の解決策はよく使われます。なぜなら起動パラメータを使うことで、インストール作業の最初の質問の回答を他のやり方で事前指定することが可能だからです。インストールごとに起動パラメータを手作業で打ち込む手間を省くためによく使われる方法は <command>isolinux</command> (CD-ROM の場合) や <command>syslinux</command> (USB メモリの場合) の設定ファイルに起動パラメータを保存することです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Creating a Preseed File"
msgstr "preseed ファイルの作成"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "A preseed file is a plain text file, where each line contains the answer to one Debconf question. A line is split across four fields separated by whitespace (spaces or tabs), as in, for instance, <literal>d-i mirror/suite string stable</literal>:"
msgstr "preseed ファイルはプレーンテキストファイルで、各行に 1 つの Debconf 質問に対する回答が含まれます。行は空白 (スペースかタブ) で区切られた 4 種類のフィールドに分割されます。たとえば <literal>d-i mirror/suite string stable</literal> のようになります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "the first field is the “owner” of the question; “d-i” is used for questions relevant to the installer, but it can also be a package name for questions coming from Debian packages;"
msgstr "最初のフィールドはこの質問の「所有者」です。インストーラに関する質問の場合「d-i」を使い、Debian パッケージからの質問の場合パッケージ名を使います。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "the second field is an identifier for the question;"
msgstr "2 番目のフィールドは質問の識別子です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "third, the type of question;"
msgstr "3 番目のフィールドは質問の種類です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "the fourth and last field contains the value for the answer. Note that it must be separated from the third field with a single space; if there are more than one, the following space characters are considered part of the value."
msgstr "4 番目以降のフィールドは質問に対する回答です。3 番目のフィールドの後ろに必ず 1 つの空白を含めなければいけない点に注意してください。さらに 1 つ以上の回答がある場合、続く空白文字は回答の一部として取り扱われます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The simplest way to write a preseed file is to install a system by hand. Then <command>debconf-get-selections --installer</command> will provide the answers concerning the installer. Answers about other packages can be obtained with <command>debconf-get-selections</command>. However, a cleaner solution is to write the preseed file by hand, starting from an example and the reference documentation: with such an approach, only questions where the default answer needs to be overridden can be preseeded; using the <literal>priority=critical</literal> boot parameter will instruct Debconf to only ask critical questions, and use the default answer for others."
msgstr "preseed ファイルを書く最も簡単な方法はシステムを手作業でインストールする方法です。インストール完了後に <command>debconf-get-selections --installer</command> を実行してインストーラに関連する回答を取得します。<command>debconf-get-selections</command> を使えば、他のパッケージに関連する回答を取得することが可能です。しかしながら、最も明確な解決策は記載例と基準文書を参考にしながら手作業で preseed ファイルを書くことです。なぜなら、このような取り扱い方をすることで、デフォルト回答に対して上書きが必要な質問だけに回答を事前指定することが可能だからです。さらに <literal>priority=critical</literal> 起動パラメータ使うことで、Debconf が重要な質問だけを尋ね、他の質問にはデフォルトの回答を使うようにすることが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>DOCUMENTATION</emphasis> Installation guide appendix"
msgstr "<emphasis>DOCUMENTATION</emphasis> インストールガイドの付録"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The installation guide, available online, includes detailed documentation on the use of a preseed file in an appendix. It also includes a detailed and commented sample file, which can serve as a base for local customizations. <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/amd64/apb.html\" /> <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/example-preseed.txt\" />"
msgstr "オンライン上で利用できるインストールガイドの付録には、preseed ファイルの使い方に関する詳細な文書が含まれます。また、コメントの形で詳細を説明された見本ファイルが含まれます。これは自分用カスタマイズのひな形として使えるように用意されています。<ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/amd64/apb.html\" /><ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/example-preseed.txt\" />"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Creating a Customized Boot Media"
msgstr "カスタマイズされた起動メディアの作成"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Knowing where to store the preseed file is all very well, but the location isn't everything: one must, one way or another, alter the installation boot media to change the boot parameters and add the preseed file."
msgstr "preseed ファイルを保存する場所を知ることは誠に結構なことですが、場所がすべてではありません。なぜなら、起動パラメータを変更し preseed ファイルを追加するためには、いずれにせよインストール用の起動メディアを改造しなければいけないからです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Booting From the Network"
msgstr "ネットワークからの起動"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "When a computer is booted from the network, the server sending the initialization elements also defines the boot parameters. Thus, the change needs to be made in the PXE configuration for the boot server; more specifically, in its <filename>/tftpboot/pxelinux.cfg/default</filename> configuration file. Setting up network boot is a prerequisite; see the Installation Guide for details. <ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/amd64/ch04s05.html\" />"
msgstr "コンピュータをネットワークから起動する場合、初期化要素を送信するサーバを使って起動パラメータを定義することも可能です。この場合、初期化要素の定義は起動サーバの PXE 設定の中で行う必要があります。より具体的に言えば <filename>/tftpboot/pxelinux.cfg/default</filename> 設定ファイルの中で設定します。そして、事前にネットワーク起動をセットアップすることが必要です。詳しくはインストールガイドをご覧ください。<ulink type=\"block\" url=\"https://www.debian.org/releases/jessie/amd64/ch04s05.html\" />"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Preparing a Bootable USB Key"
msgstr "起動可能な USB メモリの準備"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Once a bootable key has been prepared (see <xref linkend=\"sect.install-usb\" />), a few extra operations are needed. Assuming the key contents are available under <filename>/media/usbdisk/</filename>:"
msgstr "起動可能な USB メモリを用意した場合 (<xref linkend=\"sect.install-usb\" />を参照してください)、以下に挙げるいくつかの追加的な操作が必要です。USB メモリの内容は <filename>/media/usbdisk/</filename> で利用できるとします。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "copy the preseed file to <filename>/media/usbdisk/preseed.cfg</filename>"
msgstr "<filename>/media/usbdisk/preseed.cfg</filename> に preseed ファイルをコピーします"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "edit <filename>/media/usbdisk/syslinux.cfg</filename> and add required boot parameters (see example below)."
msgstr "<filename>/media/usbdisk/syslinux.cfg</filename> を編集して、必要な起動パラメータを追加します (以下の例をご覧ください)。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "syslinux.cfg file and preseeding parameters"
msgstr "syslinux.cfg ファイルと preseed パラメータ"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid ""
"default vmlinuz\n"
"append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --"
msgstr ""
"default vmlinuz\n"
"append preseed/file=/hd-media/preseed.cfg locale=en_US.UTF-8 keymap=us language=us country=US vga=788 initrd=initrd.gz  --"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Creating a CD-ROM Image"
msgstr "CD-ROM イメージの作成"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary>debian-cd</primary>"
msgstr "<primary>debian-cd</primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "A USB key is a read-write media, so it was easy for us to add a file there and change a few parameters. In the CD-ROM case, the operation is more complex, since we need to regenerate a full ISO image. This task is handled by <emphasis role=\"pkg\">debian-cd</emphasis>, but this tool is rather awkward to use: it needs a local mirror, and it requires an understanding of all the options provided by <filename>/usr/share/debian-cd/CONF.sh</filename>; even then, <command>make</command> must be invoked several times. <filename>/usr/share/debian-cd/README</filename> is therefore a very recommended read."
msgstr "USB メモリは読み書きメディアなので、ファイルを追加したりいくつかのパラメータを変更することが簡単に可能です。CD-ROM の場合、この操作はさらに複雑になります。なぜなら、完全な ISO イメージを再生成する必要があるからです。<emphasis role=\"pkg\">debian-cd</emphasis> がこの作業を担当しますが、<emphasis role=\"pkg\">debian-cd</emphasis> ツールは少し使いにくいです。すなわち、<emphasis role=\"pkg\">debian-cd</emphasis> ツールを使うには、ローカルミラーと <filename>/usr/share/debian-cd/CONF.sh</filename> によって提供されるすべてのオプションについて理解する必要があります。そしてさらに、<command>make</command> を複数回実行する必要があります。このため、<filename>/usr/share/debian-cd/README</filename> を一読することを強く推奨します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Having said that, debian-cd always operates in a similar way: an “image” directory with the exact contents of the CD-ROM is generated, then converted to an ISO file with a tool such as <command>genisoimage</command>, <command>mkisofs</command> or <command>xorriso</command>. The image directory is finalized after debian-cd's <command>make image-trees</command> step. At that point, we insert the preseed file into the appropriate directory (usually <filename>$TDIR/$CODENAME/CD1/</filename>, $TDIR and $CODENAME being parameters defined by the <filename>CONF.sh</filename> configuration file). The CD-ROM uses <command>isolinux</command> as its bootloader, and its configuration file must be adapted from what debian-cd generated, in order to insert the required boot parameters (the specific file is <filename>$TDIR/$CODENAME/boot1/isolinux/isolinux.cfg</filename>). Then the “normal” process can be resumed, and we can go on to generating the ISO image with <command>make image CD=1</command> (or <command>make images</command> if several CD-ROMs are generated)."
msgstr "そうは言っても、debian-cd の挙動は大きく変わるものではありません。具体的に言えば CD-ROM の内容を展開した「image」ディレクトリが生成され、その後 <command>genisoimage</command>、<command>mkisofs</command>、<command>xorriso</command> などのツールを使って「image」ディレクトリを ISO ファイルに変換します。image ディレクトリは debian-cd の <command>make image-trees</command> 段階の後に仕上げられます。この時点で、preseed ファイルを適切なディレクトリ (通常 <filename>$TDIR/$CODENAME/CD1/</filename> です、ここで $TDIR と $CODENAME は <filename>CONF.sh</filename> 設定ファイルによって定義されるパラメータです) に追加します。CD-ROM は <command>isolinux</command> をブートローダとして使います。必要な起動パラメータを追加するためには、<command>isolinux</command> の設定ファイル (ファイルは <filename>$TDIR/$CODENAME/boot1/isolinux/isolinux.cfg</filename> にあります) を debian-cd が生成した内容に適合させなければいけません。この後、「通常の」プロセスを再開し、<command>make image CD=1</command> (または <command>make images</command> 複数の CD-ROM を生成する場合) を実行して ISO イメージを生成します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Simple-CDD: The All-In-One Solution"
msgstr "Simple-CDD、一体型の解決策"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary>simple-cdd</primary>"
msgstr "<primary>simple-cdd</primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Simply using a preseed file is not enough to fulfill all the requirements that may appear for large deployments. Even though it is possible to execute a few scripts at the end of the normal installation process, the selection of the set of packages to install is still not quite flexible (basically, only “tasks” can be selected); more important, this only allows installing official Debian packages, and precludes locally-generated ones."
msgstr "単純に preseed ファイルを使うだけでは、大規模な配備に要求されるすべてを満足させることはできません。preseed ファイルを使うことで、通常のインストール作業の最後にいくつかのスクリプトを実行することが可能とは言うものの、インストールするパッケージ群の選択にはまだ大きな制限があります (基本的に「tasks」を選択できるだけです)。それどころかより重要なこととして、preseed ファイルを使えば公式の Debian パッケージをインストールすることが可能ですが、自前で作成したパッケージをインストールすることは不可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "On the other hand, debian-cd is able to integrate external packages, and debian-installer can be extended by inserting new steps in the installation process. By combining these capabilities, it should be possible to create a customized installer that fulfills our needs; it should even be able to configure some services after unpacking the required packages. Fortunately, this is not a mere hypothesis, since this is exactly what Simple-CDD (in the <emphasis role=\"pkg\">simple-cdd</emphasis> package) does."
msgstr "逆に、debian-cd を使えば外部パッケージを組み込むことが可能で、debian-installer を使えばインストール作業に新しい段階を挿入するように拡張することが可能です。これらの機能を組み合わせることで、自分の要求を完全に満足するカスタマイズされたインストーラを作成することが可能です。さらにこの方針を取ることで、必要なパッケージを展開した後、いくつかのサービスを設定することが可能です。幸いなことに、この方針は単なる仮説というわけではありません。なぜなら、これこそが Simple-CDD (<emphasis role=\"pkg\">simple-cdd</emphasis> パッケージに含まれます) のやっていることだからです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The purpose of Simple-CDD is to allow anyone to easily create a distribution derived from Debian, by selecting a subset of the available packages, preconfiguring them with Debconf, adding specific software, and executing custom scripts at the end of the installation process. This matches the “universal operating system” philosophy, since anyone can adapt it to their own needs."
msgstr "Simple-CDD の目的とは、利用できるパッケージの一部を選択したり、Debconf を使ってパッケージを事前設定したり、特定のソフトウェアを追加したり、インストール作業の最後にカスタムスクリプトを実行することで、誰でも簡単に Debian の派生ディストリビューションを作成することです。これは「ユニバーサルオペレーティングシステム」の原理に一致します。なぜなら、このことにより誰でも自分自身の要求にオペレーティングシステムを適合させることが可能だからです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Creating Profiles"
msgstr "プロファイルの作成"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Simple-CDD defines “profiles” that match the FAI “classes” concept, and a machine can have several profiles (determined at installation time). A profile is defined by a set of <filename>profiles/<replaceable>profile</replaceable>.*</filename> files:"
msgstr "Simple-CDD は FAI における「クラス」の概念に対応する「プロファイル」を定義し、マシンは (インストール時に定義される) 複数のプロファイルを持つことが可能です。プロファイルは以下に挙げる <filename>profiles/<replaceable>profile</replaceable>.*</filename> ファイルを使って定義されます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "the <filename>.description</filename> file contains a one-line description for the profile;"
msgstr "<filename>.description</filename> ファイル。プロファイルに関する 1 行の説明が含まれます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "the <filename>.packages</filename> file lists packages that will automatically be installed if the profile is selected;"
msgstr "<filename>.packages</filename> ファイル。プロファイルが選択された場合に自動的にインストールするパッケージがリストされています。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "the <filename>.downloads</filename> file lists packages that will be stored onto the installation media, but not necessarily installed;"
msgstr "<filename>.downloads</filename> ファイル。インストールメディアに保存するがシステムにインストールしないパッケージがリストされています。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "the <filename>.preseed</filename> file contains preseeding information for Debconf questions (for the installer and/or for packages);"
msgstr "<filename>.preseed</filename> ファイル。(インストーラおよびパッケージの) Debconf 質問に関する preseed 情報が含まれます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "the <filename>.postinst</filename> file contains a script that will be run at the end of the installation process;"
msgstr "<filename>.postinst</filename> ファイル。インストール作業の最後に実行されるスクリプトが含まれます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "lastly, the <filename>.conf</filename> file allows changing some Simple-CDD parameters based on the profiles to be included in an image."
msgstr "<filename>.conf</filename> ファイル。イメージに保存されるプロファイルに基づいていくつかの Simple-CDD パラメータを修正することが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The <literal>default</literal> profile has a particular role, since it is always selected; it contains the bare minimum required for Simple-CDD to work. The only thing that is usually customized in this profile is the <literal>simple-cdd/profiles</literal> preseed parameter: this allows avoiding the question, introduced by Simple-CDD, about what profiles to install."
msgstr "<literal>default</literal> プロファイルは特別な役割を担います。なぜなら <literal>default</literal> プロファイルは常に選択されるからです。そして <literal>default</literal> プロファイルには、Simple-CDD を動作させるために最低限必要な要素が含まれています。通常 <literal>default</literal> プロファイルの中でカスタマイズが必要なのは <literal>simple-cdd/profiles</literal> preseed パラメータだけです。なぜなら、これを使うことで Simple-CDD によって追加されたインストールするプロファイルの選択に関する質問を避けることが可能だからです。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: PTAL;
msgid "Note also that the commands will need to be invoked from the parent directory of the <filename>profiles</filename> directory."
msgstr "コマンドは <filename>profiles</filename> ディレクトリの親ディレクトリから実行する必要がある点に注意してください。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Configuring and Using <command>build-simple-cdd</command>"
msgstr "<command>build-simple-cdd</command> の設定と利用"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary><command>build-simple-cdd</command></primary>"
msgstr "<primary><command>build-simple-cdd</command></primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>QUICK LOOK</emphasis> Detailed configuration file"
msgstr "<emphasis>QUICK LOOK</emphasis> 詳細説明を含む設定ファイル"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "An example of a Simple-CDD configuration file, with all possible parameters, is included in the package (<filename>/usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz</filename>). This can be used as a starting point when creating a custom configuration file."
msgstr "すべての利用できるパラメータを含む Simple-CDD 設定ファイルの例 (<filename>/usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz</filename>) がパッケージに含まれています。カスタム設定ファイルを作成する際に、この例を足掛かりとして使うことが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
# Ref: $ zless /usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz;
# Ref: $ man 1 simple-cdd;
msgid "Simple-CDD requires many parameters to operate fully. They will most often be gathered in a configuration file, which <command>build-simple-cdd</command> can be pointed at with the <literal>--conf</literal> option, but they can also be specified via dedicated parameters given to <command>build-simple-cdd</command>. Here is an overview of how this command behaves, and how its parameters are used:"
msgstr "Simple-CDD を完全に動作させるには多くのパラメータが必要です。通常、パラメータは設定ファイルの中にまとめられ、この設定ファイルを <command>build-simple-cdd</command> の <literal>--conf</literal> オプションに指定します。しかし、パラメータは専用パラメータを <command>build-simple-cdd</command> に渡すことでも指定することも可能です。以下では、パラメータの使い方と <command>build-simple-cdd</command> の挙動を大ざっぱに説明しています。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
# Ref: $ zless /usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz;
msgid "the <literal>profiles</literal> parameter lists the profiles that will be included on the generated CD-ROM image;"
msgstr "<literal>profiles</literal> パラメータは生成する CD-ROM イメージに含めるプロファイルをリストします。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
# Ref: $ zless /usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz;
msgid "based on the list of required packages, Simple-CDD downloads the appropriate files from the server mentioned in <literal>server</literal>, and gathers them into a partial mirror (which will later be given to debian-cd);"
msgstr "Simple-CDD は要求されるパッケージのリストに基づいて、<literal>server</literal> で指定されているサーバから適切なファイルをダウンロードし、(後に debian-cd に渡される) ローカルミラーにまとめます。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
# Ref: $ zless /usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz;
msgid "the custom packages mentioned in <literal>local_packages</literal> are also integrated into this local mirror;"
msgstr "また <literal>local_packages</literal> で指定されたカスタムパッケージがこのローカルミラーの中に統合されます。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
# Ref: $ zless /usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz;
msgid "debian-cd is then executed (within a default location that can be configured with the <literal>debian_cd_dir</literal> variable), with the list of packages to integrate;"
msgstr "この後、組み込むパッケージのリストを使って debian-cd が実行されます (実行場所は <literal>debian_cd_dir</literal> 変数を使って設定されたデフォルト位置です)。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
# Ref: $ zless /usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz;
msgid "once debian-cd has prepared its directory, Simple-CDD applies some changes to this directory:"
msgstr "debian-cd が <literal>debian_cd_dir</literal> で指定したディレクトリを用意した後、Simple-CDD はいくつかの変更をこのディレクトリに加えます。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
# Ref: $ zless /usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz;
msgid "files containing the profiles are added in a <filename>simple-cdd</filename> subdirectory (that will end up on the CD-ROM);"
msgstr "プロファイルを含むファイルが <filename>simple-cdd</filename> サブディレクトリに追加されます (CD-ROM に追加されます)。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
# Ref: $ zless /usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz;
msgid "other files listed in the <literal>all_extras</literal> parameter are also added;"
msgstr "<literal>all_extras</literal> パラメータで指定された他のファイルがディレクトリに追加されます。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Tag: L-CFMD;
# Ref: $ zless /usr/share/doc/simple-cdd/examples/simple-cdd.conf.detailed.gz;
msgid "the boot parameters are adjusted so as to enable the preseeding. Questions concerning language and country can be avoided if the required information is stored in the <literal>language</literal> and <literal>country</literal> variables."
msgstr "起動パラメータが調整され、preseed が有効化されます。言語と国に関する質問を避けるには、これらの情報を <literal>language</literal> と <literal>country</literal> 変数に保存します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "debian-cd then generates the final ISO image."
msgstr "この後、debian-cd が最終的な ISO イメージを生成します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Generating an ISO Image"
msgstr "ISO イメージの生成"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Once we have written a configuration file and defined our profiles, the remaining step is to invoke <command>build-simple-cdd --conf simple-cdd.conf</command>. After a few minutes, we get the required image in <filename>images/debian-8.0-amd64-CD-1.iso</filename>."
msgstr "設定ファイルの記述と自分のプロファイルの定義が完了したら、<command>build-simple-cdd --conf simple-cdd.conf</command> を実行します。数分後、要求されたイメージが <filename>images/debian-8.0-amd64-CD-1.iso</filename> に完成します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Monitoring is a generic term, and the various involved activities have several goals: on the one hand, following usage of the resources provided by a machine allows anticipating saturation and the subsequent required upgrades; on the other hand, alerting the administrator as soon as a service is unavailable or not working properly means that the problems that do happen can be fixed sooner."
msgstr "監視は一般的な用語で、さまざまな目的で行われるさまざまな活動を意味します。すなわち一方では、マシンの提供するリソースが使い切られ、更新が必要になることを予測することが可能です。さらに他方では、サービスが利用できなくなったり適切に動作していないことを可能な限り早く管理者に警告することにより、発生した問題の早急な修正を可能にすることを意味します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>Munin</emphasis> covers the first area, by displaying graphical charts for historical values of a number of parameters (used RAM, occupied disk space, processor load, network traffic, Apache/MySQL load, and so on). <emphasis>Nagios</emphasis> covers the second area, by regularly checking that the services are working and available, and sending alerts through the appropriate channels (e-mails, text messages, and so on). Both have a modular design, which makes it easy to create new plug-ins to monitor specific parameters or services."
msgstr "<emphasis>Munin</emphasis> は最初の範囲をカバーします。すなわち <emphasis>Munin</emphasis> はいくつかのパラメータの経時変化をグラフィカルに図示します (使用された RAM、専有されたディスク領域、プロセッサの負荷、ネットワークトラフィック、Apache/MySQL の負荷などを図示します)。<emphasis>Nagios</emphasis> は 2 番目の範囲をカバーします。すなわち <emphasis>Nagios</emphasis> はサービスの稼働状態と利用可能状態を定期的に確認し、適切な経路 (電子メール、テキストメッセージなど) を通じて警告を送信します。<emphasis>Munin</emphasis> と <emphasis>Nagios</emphasis> はモジュール式に設計されているので、特定のパラメータやサービスを監視する新しいプラグインを簡単に作成できます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>ALTERNATIVE</emphasis> Zabbix, an integrated monitoring tool"
msgstr "<emphasis>ALTERNATIVE</emphasis> Zabbix、統合監視ツール"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary>Zabbix</primary>"
msgstr "<primary>Zabbix</primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Although Munin and Nagios are in very common use, they are not the only players in the monitoring field, and each of them only handles half of the task (graphing on one side, alerting on the other). Zabbix, on the other hand, integrates both parts of monitoring; it also has a web interface for configuring the most common aspects. It has grown by leaps and bounds during the last few years, and can now be considered a viable contender. On the monitoring server, you would install <emphasis role=\"pkg\">zabbix-server-pgsql</emphasis> (or <emphasis role=\"pkg\">zabbix-server-mysql</emphasis>), possibly together with <emphasis role=\"pkg\">zabbix-frontend-php</emphasis> to have a web interface. On the hosts to monitor you would install <emphasis role=\"pkg\">zabbix-agent</emphasis> feeding data back to the server. <ulink type=\"block\" url=\"http://www.zabbix.com/\" />"
msgstr "Munin と Nagios はとてもよく使われていますが、監視分野における唯一の選択肢というわけではありませんし、両者が担当している範囲は監視タスクの半分 (片方がグラフ化、もう一方が警告) に留まっています。これに対して、Zabbix は監視タスクの両方を統合しています。さらに Zabbix は最もよく使われる側面を設定するためのウェブインターフェースを備えています。Zabbix は最近の数年間で急速に成長し続けており、今や Munin と Nagios の対抗馬と考えられています。監視サーバには <emphasis role=\"pkg\">zabbix-server-pgsql</emphasis> (または <emphasis role=\"pkg\">zabbix-server-mysql</emphasis>) をインストールし、ウェブインターフェースを使うには <emphasis role=\"pkg\">zabbix-frontend-php</emphasis> もインストールします。監視対象ホストには <emphasis role=\"pkg\">zabbix-agent</emphasis> をインストールして、監視サーバからのデータ要求に応答します。<ulink type=\"block\" url=\"http://www.zabbix.com/\" />"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>ALTERNATIVE</emphasis> Icinga, a Nagios fork"
msgstr "<emphasis>ALTERNATIVE</emphasis> Icinga、Nagios のフォーク"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary>Icinga</primary>"
msgstr "<primary>Icinga</primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Spurred by divergences in opinions concerning the development model for Nagios (which is controlled by a company), a number of developers forked Nagios and use Icinga as their new name. Icinga is still compatible — so far — with Nagios configurations and plugins, but it also adds extra features. <ulink type=\"block\" url=\"http://www.icinga.org/\" />"
msgstr "Nagios の開発モデル (開発は企業によって管理されています) に関する意見の食い違いが引き金となり、多数の開発者が Nagios をフォークし、新しい名前として Icinga を使っています。現時点で Icinga はまだ Nagios の設定およびプラグインと互換性を持っています。しかし、Icinga にはいくつかの機能が追加されています。<ulink type=\"block\" url=\"http://www.icinga.org/\" />"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Setting Up Munin"
msgstr "Munin のセットアップ"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary>Munin</primary>"
msgstr "<primary>Munin</primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The purpose of Munin is to monitor many machines; therefore, it quite naturally uses a client/server architecture. The central host — the grapher — collects data from all the monitored hosts, and generates historical graphs."
msgstr "Munin の目的は多くのマシンを監視することです。そしてこのため、Munin は当然クライアント/サーバアーキテクチャを採用しています。グラフ化を担当している中央ホストがすべての監視されているホストからデータを収集し、データの履歴グラフを生成します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Configuring Hosts To Monitor"
msgstr "監視対象ホストの設定"

# Checked-By: Ryuunosuke Ayanokouzi;
# Ref: https://packages.debian.org/jessie/all/munin-plugins-core/filelist;
# Ref: $ man 8 munin-node
msgid "The first step is to install the <emphasis role=\"pkg\">munin-node</emphasis> package. The daemon installed by this package listens on port 4949 and sends back the data collected by all the active plugins. Each plugin is a simple program returning a description of the collected data as well as the latest measured value. Plugins are stored in <filename>/usr/share/munin/plugins/</filename>, but only those with a symbolic link in <filename>/etc/munin/plugins/</filename> are really used."
msgstr "最初に <emphasis role=\"pkg\">munin-node</emphasis> パッケージをインストールします。<emphasis role=\"pkg\">munin-node</emphasis> パッケージによってインストールされるデーモンはポート 4949 番をリッスンし、すべての動作しているプラグインによって収集されたデータを送り返します。それぞれのプラグインは収集されたデータの説明および最新の計測値を返す簡単なプログラムです。プラグインは <filename>/usr/share/munin/plugins/</filename> に保存されますが、実際に使われるのは <filename>/etc/munin/plugins/</filename> 内からシンボリックリンクを張られたプラグインだけです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "When the package is installed, a set of active plugins is determined based on the available software and the current configuration of the host. However, this autoconfiguration depends on a feature that each plugin must provide, and it is usually a good idea to review and tweak the results by hand. Browsing the <ulink url=\"http://gallery.munin-monitoring.org\">Plugin Gallery</ulink> can be interesting even though not all plugins have comprehensive documentation. However, all plugins are scripts and most are rather simple and well-commented. Browsing <filename>/etc/munin/plugins/</filename> is therefore a good way of getting an idea of what each plugin is about and determining which should be removed. Similarly, enabling an interesting plugin found in <filename>/usr/share/munin/plugins/</filename> is a simple matter of setting up a symbolic link with <command>ln -sf /usr/share/munin/plugins/<replaceable>plugin</replaceable> /etc/munin/plugins/</command>. Note that when a plugin name ends with an underscore “_”, the plugin requires a parameter. This parameter must be stored in the name of the symbolic link; for instance, the “if_” plugin must be enabled with a <filename>if_eth0</filename> symbolic link, and it will monitor network traffic on the eth0 interface."
msgstr "<emphasis role=\"pkg\">munin-node</emphasis> パッケージのインストールが完了したら、利用できるソフトウェアと現在のホストの設定に基づいて有効なプラグイン群が決定されます。しかしながら、有効プラグインの自動決定は各プラグインの提供する機能に依存します。通常、手作業で結果を確認して微調整することを推奨します。すべてのプラグインに対して包括的な文書が用意されているわけではありませんが、<ulink url=\"http://gallery.munin-monitoring.org\">プラグインギャラリー</ulink>を閲覧すると面白いかもしれません。さらに、すべてのプラグインはスクリプトで、その多くは単純かつ詳細に説明されています。このため、各プラグインの機能を理解して無効化するべきプラグインを決定するには <filename>/etc/munin/plugins/</filename> を閲覧すると良いでしょう。同様に、<filename>/usr/share/munin/plugins/</filename> の中にある興味深いプラグインを有効化するには、<command>ln -sf /usr/share/munin/plugins/<replaceable>plugin</replaceable> /etc/munin/plugins/</command> を使ってシンボリックリンクを作成するだけです。プラグイン名がアンダースコア「_」で終わる場合、そのプラグインはパラメータが必要という点に注意してください。シンボリックリンクの名前を使って、このパラメータを指定します。従って、たとえば「if_」プラグインは必ず <filename>if_eth0</filename> シンボリックリンクを使って有効化しなければいけません。こうすることで、eth0 インターフェースのネットワークトラフィックを監視します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Once all plugins are correctly set up, the daemon configuration must be updated to describe access control for the collected data. This involves <literal>allow</literal> directives in the <filename>/etc/munin/munin-node.conf</filename> file. The default configuration is <literal>allow ^127\\.0\\.0\\.1$</literal>, and only allows access to the local host. An administrator will usually add a similar line containing the IP address of the grapher host, then restart the daemon with <command>service munin-node restart</command>."
msgstr "すべてのプラグインを正常に設定したら、収集されたデータへのアクセス制御に関するデーモン設定を更新します。これを行うには、<filename>/etc/munin/munin-node.conf</filename> ファイルの中で <literal>allow</literal> 指示文を使います。デフォルト設定は <literal>allow ^127\\.0\\.0\\.1$</literal> で、ローカルホストへのアクセスのみを許可します。通常、管理者はグラフ化を担当しているホストの IP アドレスを含めた同様の行を追加します。その後、<command>service munin-node restart</command> を使ってデーモンを再起動します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>GOING FURTHER</emphasis> Creating local plugins"
msgstr "<emphasis>GOING FURTHER</emphasis> ローカルプラグインの作成"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Munin does include detailed documentation on how plugins should behave, and how to develop new plugins. <ulink type=\"block\" url=\"http://munin-monitoring.org/wiki/plugins\" />"
msgstr "Munin にはプラグインの動作様式と新規プラグインの開発方法に関する詳細な文書が用意されています。<ulink type=\"block\" url=\"http://munin-monitoring.org/wiki/plugins\" />"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "A plugin is best tested when run in the same conditions as it would be when triggered by munin-node; this can be simulated by running <command>munin-run <replaceable>plugin</replaceable></command> as root. A potential second parameter given to this command (such as <literal>config</literal>) is passed to the plugin as a parameter."
msgstr "プラグインを検査するには、プラグインが munin-node から実行されたのと同じ状況下で実行するのが最良の方法です。この状況を模倣するには root で <command>munin-run <replaceable>plugin</replaceable></command> を実行します。このコマンドに与えられた 2 番目のパラメータ (<literal>config</literal> など) はパラメータとしてプラグインに渡されます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "When a plugin is invoked with the <literal>config</literal> parameter, it must describe itself by returning a set of fields:"
msgstr "プラグインに <literal>config</literal> パラメータを付けて実行した場合、プラグイン自身を説明する一連のフィールドが返されます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid ""
"<computeroutput>$ </computeroutput><userinput>sudo munin-run load config\n"
"</userinput><computeroutput>graph_title Load average\n"
"graph_args --base 1000 -l 0\n"
"graph_vlabel load\n"
"graph_scale no\n"
"graph_category system\n"
"load.label load\n"
"graph_info The load average of the machine describes how many processes are in the run-queue (scheduled to run \"immediately\").\n"
"load.info 5 minute load average\n"
"</computeroutput>"
msgstr ""
"<computeroutput>$ </computeroutput><userinput>sudo munin-run load config\n"
"</userinput><computeroutput>graph_title Load average\n"
"graph_args --base 1000 -l 0\n"
"graph_vlabel load\n"
"graph_scale no\n"
"graph_category system\n"
"load.label load\n"
"graph_info The load average of the machine describes how many processes are in the run-queue (scheduled to run \"immediately\").\n"
"load.info 5 minute load average\n"
"</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The various available fields are described by the “Plugin reference” available as part of the “Munin guide”. <ulink type=\"block\" url=\"http://munin.readthedocs.org/en/latest/reference/plugin.html\" />"
msgstr "利用できるさまざまなフィールドは「Munin ガイド」の一部として提供されている「プラグインリファレンス」で説明されています。<ulink type=\"block\" url=\"http://munin.readthedocs.org/en/latest/reference/plugin.html\" />"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "When invoked without a parameter, the plugin simply returns the last measured values; for instance, executing <command>sudo munin-run load</command> could return <literal>load.value 0.12</literal>."
msgstr "パラメータを付けずにプラグインを実行した場合、プラグインは最新の計測値を返却します。従って、たとえば <command>sudo munin-run load</command> を実行すると <literal>load.value 0.12</literal> が返されます。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Finally, when a plugin is invoked with the <literal>autoconf</literal> parameter, it should return “yes” (and a 0 exit status) or “no” (with a 1 exit status) according to whether the plugin should be enabled on this host."
msgstr "最後に、プラグインに <literal>autoconf</literal> パラメータを付けて実行した場合、プラグインは自分が対象のホストで有効化されているか否かに基づいて「yes」(終了ステータス 0) または「no」(終了ステータス 1) を返すべきです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Configuring the Grapher"
msgstr "グラフ化担当マシンの設定"

# Checked-By: Ryuunosuke Ayanokouzi;
# Checked-By: Osamu Aoki;
msgid "The “grapher” is simply the computer that aggregates the data and generates the corresponding graphs. The required software is in the <emphasis role=\"pkg\">munin</emphasis> package. The standard configuration runs <command>munin-cron</command> (once every 5 minutes), which gathers data from all the hosts listed in <filename>/etc/munin/munin.conf</filename> (only the local host is listed by default), saves the historical data in RRD files (<emphasis>Round Robin Database</emphasis>, a file format designed to store data varying in time) stored under <filename>/var/lib/munin/</filename> and generates an HTML page with the graphs in <filename>/var/cache/munin/www/</filename>."
msgstr "「グラフ化担当マシン」はデータを集計し対応するグラフを生成するだけのコンピュータです。「グラフ化担当マシン」に必要なソフトウェアは <emphasis role=\"pkg\">munin</emphasis> パッケージに含まれます。標準的な設定は <command>munin-cron</command> を (5 分ごとに) 実行します。このコマンドは <filename>/etc/munin/munin.conf</filename> にリストされているすべてのホスト (デフォルトではローカルホストのみがリストされています) からデータを収集し、時系列データを <filename>/var/lib/munin/</filename> にある RRD ファイル (<emphasis>Round Robin Database</emphasis>、経時変化するデータを保存するために設計されたファイルフォーマット) に保存し、<filename>/var/cache/munin/www/</filename> に含まれるグラフを使って HTML ページを生成します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "All monitored machines must therefore be listed in the <filename>/etc/munin/munin.conf</filename> configuration file. Each machine is listed as a full section with a name matching the machine and at least an <literal>address</literal> entry giving the corresponding IP address."
msgstr "すべての監視対象のマシンは <filename>/etc/munin/munin.conf</filename> 設定ファイルにリストされていなければいけません。各マシンは完全なセクションの形でリストされています。セクションはマシンと同じ名前で、少なくとも対応する IP アドレスを指定する <literal>address</literal> エントリを持っていなければいけません。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid ""
"[ftp.falcot.com]\n"
"    address 192.168.0.12\n"
"    use_node_name yes"
msgstr ""
"[ftp.falcot.com]\n"
"    address 192.168.0.12\n"
"    use_node_name yes"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Sections can be more complex, and describe extra graphs that could be created by combining data coming from several machines. The samples provided in the configuration file are good starting points for customization."
msgstr "セクションをさらに複雑にして、複数のマシンからのデータをまとめて作成されるグラフを追加することも可能です。設定ファイルの中で提供されている見本がカスタマイズの良い足掛かりとなります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The last step is to publish the generated pages; this involves configuring a web server so that the contents of <filename>/var/cache/munin/www/</filename> are made available on a website. Access to this website will often be restricted, using either an authentication mechanism or IP-based access control. See <xref linkend=\"sect.http-web-server\" /> for the relevant details."
msgstr "最後の段階は生成されたページを公開することです。そしてこれは、ウェブサイトから <filename>/var/cache/munin/www/</filename> の内容を利用できるようにするようウェブサーバを設定することを意味しています。通常このウェブサイトへのアクセスは認証メカニズムか IP に基づくアクセス制御を使って制限されています。アクセス制御の詳細は<xref linkend=\"sect.http-web-server\" />をご覧ください。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Setting Up Nagios"
msgstr "Nagios のセットアップ"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<primary>Nagios</primary>"
msgstr "<primary>Nagios</primary>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Unlike Munin, Nagios does not necessarily require installing anything on the monitored hosts; most of the time, Nagios is used to check the availability of network services. For instance, Nagios can connect to a web server and check that a given web page can be obtained within a given time."
msgstr "Munin と異なり、Nagios の場合、必ずしも監視対象のホストに何かをインストールする必要はありません。そしてほとんどの場合、Nagios はネットワークサービスの可用性を確認するために使われます。たとえば Nagios を使うことで、ウェブサーバに接続して特定のウェブページがある時間内に取得できるかを確認することが可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Installing"
msgstr "インストール"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The first step in setting up Nagios is to install the <emphasis role=\"pkg\">nagios3</emphasis>, <emphasis role=\"pkg\">nagios-plugins</emphasis> and <emphasis role=\"pkg\">nagios3-doc</emphasis> packages. Installing the packages configures the web interface and creates a first <literal>nagiosadmin</literal> user (for which it asks for a password). Adding other users is a simple matter of inserting them in the <filename>/etc/nagios3/htpasswd.users</filename> file with Apache's <command>htpasswd</command> command. If no Debconf question was displayed during installation, <command>dpkg-reconfigure nagios3-cgi</command> can be used to define the <literal>nagiosadmin</literal> password."
msgstr "Nagios をセットアップするには、最初に <emphasis role=\"pkg\">nagios3</emphasis>、<emphasis role=\"pkg\">nagios-plugins</emphasis>、<emphasis role=\"pkg\">nagios3-doc</emphasis> パッケージをインストールします。これらのパッケージをインストールするとウェブインターフェースが設定され、最初の <literal>nagiosadmin</literal> ユーザが作成されます (このユーザのパスワードが尋ねられます)。他のユーザを追加するには、Apache の <command>htpasswd</command> コマンドを使ってユーザを <filename>/etc/nagios3/htpasswd.users</filename> ファイルに追加するだけです。Debconf 質問がインストール中に表示されない場合、<command>dpkg-reconfigure nagios3-cgi</command> を使って <literal>nagiosadmin</literal> のパスワードを定義することも可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Pointing a browser at <literal>http://<replaceable>server</replaceable>/nagios3/</literal> displays the web interface; in particular, note that Nagios already monitors some parameters of the machine where it runs. However, some interactive features such as adding comments to a host do not work. These features are disabled in the default configuration for Nagios, which is very restrictive for security reasons."
msgstr "ブラウザで <literal>http://<replaceable>server</replaceable>/nagios3/</literal> にアクセスすると、ウェブインターフェースが表示されます。特に、Nagios は自分が実行されているマシンのいくつかのパラメータを既に監視している点に注意してください。しかしながら、たとえばホストに対するコメントを追加するなどの対話型機能は動作しません。Nagios のデフォルト設定はこれらの機能を無効化し、セキュリティの理由からとても厳しい制限を設けています。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Ref: https://packages.debian.org/jessie/all/nagios3-common/filelist
msgid "As documented in <filename>/usr/share/doc/nagios3/README.Debian</filename>, enabling some features involves editing <filename>/etc/nagios3/nagios.cfg</filename> and setting its <literal>check_external_commands</literal> parameter to “1”. We also need to set up write permissions for the directory used by Nagios, with commands such as the following:"
msgstr "<filename>/usr/share/doc/nagios3/README.Debian</filename> で説明されている通り、いくつかの機能を有効化するには <filename>/etc/nagios3/nagios.cfg</filename> を編集し、<literal>check_external_commands</literal> パラメータを「1」に設定します。また、以下のようにして、Nagios が使うディレクトリに書き込みパーミッションを設定する必要があります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid ""
"<computeroutput># </computeroutput><userinput>service nagios3 stop</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>dpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios3/rw\n"
"</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios3\n"
"</userinput><computeroutput># </computeroutput><userinput>service nagios3 start</userinput>\n"
"<computeroutput>[...]</computeroutput>"
msgstr ""
"<computeroutput># </computeroutput><userinput>service nagios3 stop</userinput>\n"
"<computeroutput>[...]\n"
"# </computeroutput><userinput>dpkg-statoverride --update --add nagios www-data 2710 /var/lib/nagios3/rw\n"
"</userinput><computeroutput># </computeroutput><userinput>dpkg-statoverride --update --add nagios nagios 751 /var/lib/nagios3\n"
"</userinput><computeroutput># </computeroutput><userinput>service nagios3 start</userinput>\n"
"<computeroutput>[...]</computeroutput>"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Configuring"
msgstr "設定"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The Nagios web interface is rather nice, but it does not allow configuration, nor can it be used to add monitored hosts and services. The whole configuration is managed via files referenced in the central configuration file, <filename>/etc/nagios3/nagios.cfg</filename>."
msgstr "Nagios のウェブインターフェースはかなり良くできていますが、設定もできませんし、監視対象のホストやサービスの追加もできません。全体の設定は中央設定ファイル <filename>/etc/nagios3/nagios.cfg</filename> から参照されているオブジェクト設定ファイルを使って管理されます。"

# Checked-By: Ryuunosuke Ayanokouzi;
# Ref: $ less /etc/nagios3/nagios.cfg;
# Ref: https://assets.nagios.com/downloads/nagioscore/docs/nagioscore/3/en/configobject.html;
# Ref: https://assets.nagios.com/downloads/nagioscore/docs/nagioscore/3/en/objectdefinitions.html;
msgid "These files should not be dived into without some understanding of the Nagios concepts. The configuration lists objects of the following types:"
msgstr "Nagios の概念を理解していない場合、オブジェクト設定ファイルの内容に立ち入るべきではありません。オブジェクト設定ファイルから設定するオブジェクトには以下の種類があります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "a <emphasis>host</emphasis> is a machine to be monitored;"
msgstr "<emphasis>host</emphasis> は監視対象のマシンです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "a <emphasis>hostgroup</emphasis> is a set of hosts that should be grouped together for display, or to factor some common configuration elements;"
msgstr "<emphasis>hostgroup</emphasis> はグループ化して表示されたり、同じ設定要素を持つホスト群です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "a <emphasis>service</emphasis> is a testable element related to a host or a host group. It will most often be a check for a network service, but it can also involve checking that some parameters are within an acceptable range (for instance, free disk space or processor load);"
msgstr "<emphasis>service</emphasis> はホストやホストグループへの検査項目を定義します。これは多くの場合、あるネットワークサービスに対する検査を定義するものですが、いくつかのパラメータ (たとえば空きディスク領域やプロセッサ負荷) が条件を満たす範囲内にあるかに対する検査を定義することも可能です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "a <emphasis>servicegroup</emphasis> is a set of services that should be grouped together for display;"
msgstr "<emphasis>servicegroup</emphasis> はグループ化して表示されるサービス群です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "a <emphasis>contact</emphasis> is a person who can receive alerts;"
msgstr "<emphasis>contact</emphasis> は警告を受け取る人です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "a <emphasis>contactgroup</emphasis> is a set of such contacts;"
msgstr "<emphasis>contactgroup</emphasis> は警告を受け取る人のグループです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "a <emphasis>timeperiod</emphasis> is a range of time during which some services have to be checked;"
msgstr "<emphasis>timeperiod</emphasis> は時間範囲で、この範囲内にいくつかのサービスを確認します。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "a <emphasis>command</emphasis> is the command line invoked to check a given service."
msgstr "<emphasis>command</emphasis> はサービスを確認するために実行するコマンドラインです。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "According to its type, each object has a number of properties that can be customized. A full list would be too long to include, but the most important properties are the relations between the objects."
msgstr "オブジェクトの種類に応じて、各オブジェクトにはカスタマイズが可能な複数の属性が含まれます。完全なリストはここに挙げるには長すぎますが、最重要の属性はオブジェクト間の関係性を示す属性です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "A <emphasis>service</emphasis> uses a <emphasis>command</emphasis> to check the state of a feature on a <emphasis>host</emphasis> (or a <emphasis>hostgroup</emphasis>) within a <emphasis>timeperiod</emphasis>. In case of a problem, Nagios sends an alert to all members of the <emphasis>contactgroup</emphasis> linked to the service. Each member is sent the alert according to the channel described in the matching <emphasis>contact</emphasis> object."
msgstr "<emphasis>service</emphasis> は <emphasis>command</emphasis> を使い、<emphasis>timeperiod</emphasis> で定めた時間内に <emphasis>host</emphasis> (または <emphasis>hostgroup</emphasis>) で稼働する特定の機能を確認します。問題が起きた場合、Nagios はそのサービスに関連付けられた <emphasis>contactgroup</emphasis> のメンバーに警告を送信します。各メンバーは対応する <emphasis>contact</emphasis> オブジェクトに書かれたチャンネルを介して警告を受け取ります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "An inheritance system allows easy sharing of a set of properties across many objects without duplicating information. Moreover, the initial configuration includes a number of standard objects; in many cases, defining new hosts, services and contacts is a simple matter of deriving from the provided generic objects. The files in <filename>/etc/nagios3/conf.d/</filename> are a good source of information on how they work."
msgstr "継承システムにより、情報を複製せずに多くのオブジェクト間の属性群を簡単に共有することが可能です。加えて、初期設定には数多くの標準的なオブジェクトが定義されています。このため多くの場合、初期設定の標準的なオブジェクトに加えて新たな <emphasis>host</emphasis>、<emphasis>service</emphasis>、<emphasis>contact</emphasis> を定義するだけで簡単に設定を完了させることが可能です。<filename>/etc/nagios3/conf.d/</filename> に含まれるファイルはオブジェクトの動作に関する良い情報源です。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "The Falcot Corp administrators use the following configuration:"
msgstr "Falcot Corp の管理者は以下の設定を使います。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<filename>/etc/nagios3/conf.d/falcot.cfg</filename> file"
msgstr "<filename>/etc/nagios3/conf.d/falcot.cfg</filename> ファイル"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid ""
"define contact{\n"
"    name                            generic-contact\n"
"    service_notification_period     24x7\n"
"    host_notification_period        24x7\n"
"    service_notification_options    w,u,c,r\n"
"    host_notification_options       d,u,r\n"
"    service_notification_commands   notify-service-by-email\n"
"    host_notification_commands      notify-host-by-email\n"
"    register                        0 ; Template only\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rhertzog\n"
"    alias           Raphael Hertzog\n"
"    email           hertzog@debian.org\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rmas\n"
"    alias           Roland Mas\n"
"    email           lolando@debian.org\n"
"}\n"
"\n"
"define contactgroup{\n"
"    contactgroup_name     falcot-admins\n"
"    alias                 Falcot Administrators\n"
"    members               rhertzog,rmas\n"
"}\n"
"\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             www-host\n"
"    alias                 www.falcot.com\n"
"    address               192.168.0.5\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             ftp-host\n"
"    alias                 ftp.falcot.com\n"
"    address               192.168.0.6\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"\n"
"# 'check_ftp' command with custom parameters\n"
"define command{\n"
"    command_name          check_ftp2\n"
"    command_line          /usr/lib/nagios/plugins/check_ftp -H $HOSTADDRESS$ -w 20 -c 30 -t 35\n"
"}\n"
"\n"
"# Generic Falcot service\n"
"define service{\n"
"    name                  falcot-service\n"
"    use                   generic-service\n"
"    contact_groups        falcot-admins\n"
"    register              0\n"
"}\n"
"\n"
"# Services to check on www-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTP\n"
"    check_command         check_http\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTPS\n"
"    check_command         check_https\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   SMTP\n"
"    check_command         check_smtp\n"
"}\n"
"\n"
"# Services to check on ftp-host\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             ftp-host\n"
"    service_description   FTP\n"
"    check_command         check_ftp2\n"
"}"
msgstr ""
"define contact{\n"
"    name                            generic-contact\n"
"    service_notification_period     24x7\n"
"    host_notification_period        24x7\n"
"    service_notification_options    w,u,c,r\n"
"    host_notification_options       d,u,r\n"
"    service_notification_commands   notify-service-by-email\n"
"    host_notification_commands      notify-host-by-email\n"
"    register                        0 ; Template only\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rhertzog\n"
"    alias           Raphael Hertzog\n"
"    email           hertzog@debian.org\n"
"}\n"
"define contact{\n"
"    use             generic-contact\n"
"    contact_name    rmas\n"
"    alias           Roland Mas\n"
"    email           lolando@debian.org\n"
"}\n"
"\n"
"define contactgroup{\n"
"    contactgroup_name     falcot-admins\n"
"    alias                 Falcot Administrators\n"
"    members               rhertzog,rmas\n"
"}\n"
"\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             www-host\n"
"    alias                 www.falcot.com\n"
"    address               192.168.0.5\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"define host{\n"
"    use                   generic-host ; Name of host template to use\n"
"    host_name             ftp-host\n"
"    alias                 ftp.falcot.com\n"
"    address               192.168.0.6\n"
"    contact_groups        falcot-admins\n"
"    hostgroups            debian-servers,ssh-servers\n"
"}\n"
"\n"
"# 'check_ftp' コマンドにカスタムパラメータを渡します\n"
"define command{\n"
"    command_name          check_ftp2\n"
"    command_line          /usr/lib/nagios/plugins/check_ftp -H $HOSTADDRESS$ -w 20 -c 30 -t 35\n"
"}\n"
"\n"
"# Falcot の運用する一般サービスを定義します\n"
"define service{\n"
"    name                  falcot-service\n"
"    use                   generic-service\n"
"    contact_groups        falcot-admins\n"
"    register              0\n"
"}\n"
"\n"
"# www-host 上の監視対象サービスを定義します\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTP\n"
"    check_command         check_http\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   HTTPS\n"
"    check_command         check_https\n"
"}\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             www-host\n"
"    service_description   SMTP\n"
"    check_command         check_smtp\n"
"}\n"
"\n"
"# ftp-host 上の監視対象サービスを定義します\n"
"define service{\n"
"    use                   falcot-service\n"
"    host_name             ftp-host\n"
"    service_description   FTP\n"
"    check_command         check_ftp2\n"
"}"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "This configuration file describes two monitored hosts. The first one is the web server, and the checks are made on the HTTP (80) and secure-HTTP (443) ports. Nagios also checks that an SMTP server runs on port 25. The second host is the FTP server, and the check includes making sure that a reply comes within 20 seconds. Beyond this delay, a <emphasis>warning</emphasis> is emitted; beyond 30 seconds, the alert is deemed critical. The Nagios web interface also shows that the SSH service is monitored: this comes from the hosts belonging to the <literal>ssh-servers</literal> hostgroup. The matching standard service is defined in <filename>/etc/nagios3/conf.d/services_nagios2.cfg</filename>."
msgstr "この設定ファイルでは、2 種類の監視対象ホストが定義されています。1 番目のホストはウェブサーバです。Nagios はこのホストに対してウェブサーバが HTTP (80) とセキュア HTTP (443) ポートで稼働していること、SMTP サーバがポート 25 番で稼働していることを確認します。2 番目のホストは FTP サーバです。Nagios はこのホストに対して応答が 20 秒以内に返されることが保証されることを確認します。Nagios は FTP サーバからの応答にかかる時間が 20 秒より長い場合に<emphasis>警告</emphasis>を、30 秒より長い場合に危機的な警告を発します。Nagios のウェブインターフェースは SSH サービスが監視されていることを示しています。すなわちこれは <literal>ssh-servers</literal> ホストグループに所属するホストの情報です。標準的なサービスの稼動状態確認は <filename>/etc/nagios3/conf.d/services_nagios2.cfg</filename> で定義されています。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Note the use of inheritance: an object is made to inherit from another object with the “use <replaceable>parent-name</replaceable>”. The parent object must be identifiable, which requires giving it a “name <replaceable>identifier</replaceable>” property. If the parent object is not meant to be a real object, but only to serve as a parent, giving it a “register 0” property tells Nagios not to consider it, and therefore to ignore the lack of some parameters that would otherwise be required."
msgstr "継承の使い方に注意してください。具体的に言えば、オブジェクトを継承するには「use <replaceable>parent-name</replaceable>」の形で親オブジェクトの名前を指定します。親オブジェクトは識別可能でなければいけません。つまり、親オブジェクトに「name <replaceable>identifier</replaceable>」属性を与える必要があります。親オブジェクトが真のオブジェクトでなく、属性継承の機能を担うだけの場合、このオブジェクトに「register 0」属性を与えます。こうすることで Nagios はこのオブジェクトを考慮しなくなり、真のオブジェクトならば必須とされるいくつかのパラメータが欠けていてもその問題を無視するようになります。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>DOCUMENTATION</emphasis> List of object properties"
msgstr "<emphasis>DOCUMENTATION</emphasis> オブジェクト属性のリスト"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "A more in-depth understanding of the various ways in which Nagios can be configured can be obtained from the documentation provided by the <emphasis role=\"pkg\">nagios3-doc</emphasis> package. This documentation is directly accessible from the web interface, with the “Documentation” link in the top left corner. It includes a list of all object types, with all the properties they can have. It also explains how to create new plugins."
msgstr "Nagios を設定するさまざまな方法に関してより深い理解を得るには、<emphasis role=\"pkg\">nagios3-doc</emphasis> パッケージに含まれる文書を読むと良いでしょう。この文書はウェブインターフェースの左上にある「Documentation」リンクから直接的に利用できます。この文書には、すべてのオブジェクト型のリストと各オブジェクトの取りうるすべての属性が説明されています。さらに、新しいプラグインの作り方も説明されています。"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "<emphasis>GOING FURTHER</emphasis> Remote tests with NRPE"
msgstr "<emphasis>GOING FURTHER</emphasis> NRPE を使ったリモートマシンの状態検査"

# Checked-By: Ryuunosuke Ayanokouzi;
msgid "Many Nagios plugins allow checking some parameters local to a host; if many machines need these checks while a central installation gathers them, the NRPE (<emphasis>Nagios Remote Plugin Executor</emphasis>) plugin needs to be deployed. The <emphasis role=\"pkg\">nagios-nrpe-plugin</emphasis> package needs to be installed on the Nagios server, and <emphasis role=\"pkg\">nagios-nrpe-server</emphasis> on the hosts where local tests need to run. The latter gets its configuration from <filename>/etc/nagios/nrpe.cfg</filename>. This file should list the tests that can be started remotely, and the IP addresses of the machines allowed to trigger them. On the Nagios side, enabling these remote tests is a simple matter of adding matching services using the new <emphasis>check_nrpe</emphasis> command."
msgstr "多くの Nagios プラグインでは、あるホストに固有のいくつかのパラメータを確認することが可能です。多くのマシンに対するパラメータの確認が必要にも関わらず Nagios サーバでパラメータを収集する場合、NRPE (<emphasis>Nagios Remote Plugin Executor</emphasis>) プラグインを配備する必要があります。Nagios サーバには <emphasis role=\"pkg\">nagios-nrpe-plugin</emphasis> パッケージをインストールし、ローカルでテストを実行するホストに <emphasis role=\"pkg\">nagios-nrpe-server</emphasis> をインストールする必要があります。<emphasis role=\"pkg\">nagios-nrpe-server</emphasis> は <filename>/etc/nagios/nrpe.cfg</filename> から設定を取得します。このファイルには、リモートからの命令に従って実行されるテストとリモートからのテスト開始命令を受け入れるマシンの IP アドレスをリストするべきです。Nagios サーバ側でリモートテストを有効化するには、<emphasis>check_nrpe</emphasis> コマンドを使って一致するサービスを追加するだけで済みます。"

#~ msgid "the <emphasis role=\"distribution\">Jessie</emphasis> standard kernel does not allow limiting the amount of memory available to a container; the feature exists, and is built in the kernel, but it is disabled by default because it has a (slight) cost on overall system performance; however, enabling it is a simple matter of setting the <command>cgroup_enable=memory</command> kernel command-line option at boot time;"
#~ msgstr "<emphasis role=\"distribution\">Jessie</emphasis> の標準的なカーネルはコンテナに対して利用を許可するメモリサイズを制限できません。この機能は存在し、カーネルに組み込まれていますが、デフォルトでは無効化されています。なぜなら、システム全体の性能に (わずかな) 影響をおよぼすからです。しかしながら、この機能を有効化することは簡単で、<command>cgroup_enable=memory</command> カーネルコマンドラインオプションを起動時に設定するだけです。"

#~ msgid "The procedure requires setting up a <filename>yum.conf</filename> file containing the necessary parameters, including the path to the source RPM repositories, the path to the plugin configuration, and the destination folder. For this example, we will assume that the environment will be stored in <filename>/var/tmp/yum-bootstrap</filename>. The file <filename>/var/tmp/yum-bootstrap/yum.conf</filename> file should look like this:"
#~ msgstr "この手順の中で、必要なパラメータを含む <filename>yum.conf</filename> ファイルを設定する必要があります。<filename>yum.conf</filename> ファイルには、ソース RPM リポジトリへのパス、プラグイン設定のパス、宛先フォルダが含まれます。以下の例では、環境を <filename>/var/tmp/yum-bootstrap</filename> に保存することを仮定しています。<filename>/var/tmp/yum-bootstrap/yum.conf</filename> ファイルは以下のようになります。"

#~ msgid "The <filename>/var/tmp/yum-bootstrap/repos.d</filename> directory should contain the descriptions of the RPM source repositories in <filename>*.repo</filename> files, just as in <filename>/etc/yum.repos.d</filename> in an already installed RPM-based system. Here is an example for a CentOS 6 installation:"
#~ msgstr "<filename>/var/tmp/yum-bootstrap/repos.d</filename> ディレクトリには、RPM ソースリポジトリを示した <filename>*.repo</filename> ファイルを含めるべきです。このファイルはインストール済みの RPM に基づくシステムの <filename>/etc/yum.repos.d</filename> にあるものとよく似ています。以下は CentOS 6 のインストールに使うファイルの例です。"

#~ msgid "Finally, <filename>pluginconf.d/installonlyn.conf</filename> file should contain the following:"
#~ msgstr "最後に、<filename>pluginconf.d/installonlyn.conf</filename> ファイルは以下の内容を含みます。"

#~ msgid "Once all this is setup, make sure the <command>rpm</command> databases are correctly initialized, with a command such as <command>rpm --rebuilddb</command>. An installation of CentOS 6 is then a matter of the following:"
#~ msgstr "これらの設定が済んだら、<command>rpm --rebuilddb</command> などのコマンドを使って、<command>rpm</command> データベースが正しく初期化されることを確認してください。CentOS 6 をインストールするには以下のコマンドを使うだけです。"

#~ msgid "<userinput>yum -c /var/tmp/yum-bootstrap/yum.conf -y install coreutils basesystem centos-release yum-basearchonly initscripts</userinput>"
#~ msgstr "<userinput>yum -c /var/tmp/yum-bootstrap/yum.conf -y install coreutils basesystem centos-release yum-basearchonly initscripts</userinput>"

#~ msgid "Xen is currently only available for the i386 and amd64 architectures. Moreover, it uses processor instructions that haven't always been provided in all i386-class computers. Note that most of the Pentium-class (or better) processors made after 2001 will work, so this restriction won't apply to very many situations."
#~ msgstr "現在のところ、Xen を利用できるのは i386 と amd64 アーキテクチャだけです。加えて、Xen はすべての i386 型コンピュータで提供されていないプロセッサ命令を使います。2001 年以降に作られたほとんどの Pentium 級 (またはそれよりも良い) プロセッサは Xen を動作させることが可能です。このため、ほとんどの場合この制限はないものと思って差し支えありません。"

#~ msgid "<filename>/sys/fs/cgroup</filename> will then be mounted automatically at boot time; if no immediate reboot is planned, the filesystem should be manually mounted with <command>mount /sys/fs/cgroup</command>."
#~ msgstr "これで <filename>/sys/fs/cgroup</filename> は起動時に自動的にマウントされます。一方で、すぐに再起動できない場合、<command>mount /sys/fs/cgroup</command> を使ってファイルシステムを手作業でマウントします。"

#~ msgid "The <emphasis role=\"pkg\">lxc</emphasis> package contains an initialization script that can automatically start one or several containers when the host boots; its configuration file, <filename>/etc/default/lxc</filename>, is relatively straightforward; note that the container configuration files need to be stored in <filename>/etc/lxc/auto/</filename>; many users may prefer symbolic links, such as can be created with <command>ln -s /var/lib/lxc/testlxc/config /etc/lxc/auto/testlxc.config</command>."
#~ msgstr "<emphasis role=\"pkg\">lxc</emphasis> パッケージには、ホストの起動時に自動的に 1 つまたは複数のコンテナを開始するための初期化スクリプトが含まれます。そして、初期化スクリプトの設定ファイルは <filename>/etc/default/lxc</filename> で、比較的分かりやすいものです。さらに、コンテナの設定ファイルは <filename>/etc/lxc/auto/</filename> に保存しなければいけない点に注意してください。また、多くのユーザは実際の設定ファイルへのシンボリックリンクを <filename>/etc/lxc/auto/</filename> に保存する方法を選びます。シンボリックリンクを作成するには <command>ln -s /var/lib/lxc/testlxc/config /etc/lxc/auto/testlxc.config</command> を使います。"

#~ msgid "<emphasis>BEWARE</emphasis> Bugs in default <literal>debian</literal> template"
#~ msgstr "<emphasis>BEWARE</emphasis> デフォルトの <literal>debian</literal> テンプレートに含まれるバグ"

#~ msgid "The <command>/usr/share/lxc/templates/lxc-debian</command> template creation script provided in the initial <emphasis role=\"distribution\">Wheezy</emphasis> package (aka <emphasis role=\"pkg\">lxc</emphasis> 0.8.0~rc1-8+deb7u1) suffers from numerous problems. The most important one is that it relies on the <command>live-debconfig</command> program which is not available in <emphasis role=\"distribution\">Wheezy</emphasis> but only in newer versions of Debian. <ulink type=\"block\" url=\"http://bugs.debian.org/680469\" /> <ulink type=\"block\" url=\"http://bugs.debian.org/686747\" />"
#~ msgstr "最初の <emphasis role=\"distribution\">Wheezy</emphasis> パッケージ (別名 <emphasis role=\"pkg\">lxc</emphasis> 0.8.0~rc1-8+deb7u1) に含まれていた <command>/usr/share/lxc/templates/lxc-debian</command> テンプレートを作成するスクリプトは数多くの問題を抱えていました。最も重要な問題は <command>live-debconfig</command> プログラムに頼っていたにも関わらず <emphasis role=\"distribution\">Wheezy</emphasis> ではこのプログラムが利用できなかった点です。<command>live-debconfig</command> プログラムは Debian のより新しいバージョンで利用できるようになりました。<ulink type=\"block\" url=\"http://bugs.debian.org/680469\" /><ulink type=\"block\" url=\"http://bugs.debian.org/686747\" />"

#~ msgid "At the time of writing, there was no good solution and no usable work-around, except to use an alternate template creation script. Further updates of lxc might fix this though. This section assumes that <command>/usr/share/lxc/templates/lxc-debian</command> matches the upstream provided script: <ulink type=\"block\" url=\"https://github.com/lxc/lxc/raw/master/templates/lxc-debian.in\" />"
#~ msgstr "執筆時点で、別のテンプレート作成スクリプトを使う以外に良い解決策はありませんし、利用できる次善策もありません。lxc の今後の更新によりこの問題が修正されるかもしれません。この節では <command>/usr/share/lxc/templates/lxc-debian</command> と上流開発から提供されるスクリプトとが一致することを仮定します。<ulink type=\"block\" url=\"https://github.com/lxc/lxc/raw/master/templates/lxc-debian.in\" />"
